{"cells":[{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import time"],"metadata":{"id":"GXQTc_W7YFdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using {device}.')"],"metadata":{"id":"P972v29oYIl0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implementing ResNet-18\n","\n","* ResNets (residual networks) won the ImageNet Large Scale Visual Recognition Challenge in 2015.\n","\n","* In this lab session, you will use high-level PyTorch functionalities to implement an adapted version of the smallest ResNet: ResNet-18.\n","\n","* You will use the [fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which is composed of labeled images of clothes and accessories.\n","\n","* You will resize the images in the dataset in order to preserve the ResNet-18 architecture, which was developed to classify colored images with $224 \\times 224$ pixels.\n","\n","* The following function is used to load the dataset and resize the images. You don't need to understand it for now.\n","\n"],"metadata":{"id":"WcK5OqnLYkpK"}},{"cell_type":"code","source":["import torchvision\n","\n","# You don't need to understand this function for now.\n","def load_data_fashion_mnist(batch_size, resize=None):\n","    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n","    trans = [torchvision.transforms.ToTensor()]\n","    if resize:\n","        trans.insert(0, torchvision.transforms.Resize(resize))\n","    trans = torchvision.transforms.Compose(trans)\n","\n","    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n","    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n","\n","    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=2, pin_memory=True), # Using pinned memory\n","            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=2, pin_memory=True)) # Using pinned memory"],"metadata":{"id":"0qNQhcPoZE7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Storing data in [pinned memory](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/) speeds up training in a GPU."],"metadata":{"id":"XymaaSGLZImj"}},{"cell_type":"code","source":["batch_size = 128 # Defines the batch size\n","train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=(224, 224)) # Loads the fashion MNIST dataset. `train_iter` and `test_iter` are `DataLoader` objects."],"metadata":{"id":"Rp8Zoo3DZLUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = next(iter(train_iter)) # Requests the first training batch\n","print(X.size()) # 128 images per batch. Each image is represented by a 1 x 224 x 224 tensor (number of channels x height x width). The images are grayscale, so there is a single channel.\n","print(y.size()) # 128 targets. Each target is a number between 0 and 9. The classification problem has 10 clases."],"metadata":{"id":"L6aHHH8KZOy1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* The following code displays some images from the first training batch.\n"],"metadata":{"id":"-LUj6olTZQfq"}},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","\n","class_labels = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'boot'] # Pre-defined class labels\n","\n","for i in range(3):\n","    print(f'\\nImage {i} ({class_labels[int(y[i])]}):\\n') # Prints the index `i` and the label associated to the `i`-th image.\n","    cv2_imshow(X[i].numpy().transpose(1, 2, 0) * 255) # Converts and displays the `i`-th image in the batch."],"metadata":{"id":"CK2Ozj5bZSM4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ResNet block\n","\n","* Your first task is to implement the ResNet block as a `torch.nn.Module` called `ResNetBlock`.\n","\n","* The constructor for `ResNetBlock` should receive the number of input channels $c_i$, the number of output channels $c_o$, and the stride $s$.\n","\n","* If $c_i = c_o$ and $s = 1$, the input image and the output image have the same shape $c_i \\times h_i \\times w_i$. In that case:\n","    1. The input image goes through a convolutional layer with $c_o$ convolutional filters, each with a $3 \\times 3$ window, padding $1$, and stride $s$.\n","    2. The resulting image goes through a batch normalization layer for images and a rectified linear activation function.\n","    3. The resulting image goes through another convolutional layer with $c_o$ convolutional filters, each with a $3 \\times 3$ window, padding $1$, and stride $1$.\n","    4. The resulting image goes through another batch normalization layer for images.\n","    5. The resulting image is added to the input image (skip connection).\n","    6. The resulting image goes through a rectified linear activation function.\n","\n","* If $c_i \\neq c_o$ or $s > 1$, the input image has shape $c_i \\times h_i \\times w_i$ and the output image has shape $c_o \\times \\lceil h_i/s \\rceil \\times \\lceil w_i/s \\rceil$. In that case:\n","    1. The input image goes through the first 4 steps described for the previous case.\n","    2. The input image also goes through a convolutional layer with $c_o$ convolutional filters, each with a $1 \\times 1$ window and stride $s$.\n","    3. The image that results from step 1 is added to the image that results from step 2 (skip connection).\n","    4. The resulting image goes through a rectified linear activation function.\n","\n","![ResNet block.](https://drive.google.com/uc?export=view&id=1yNH83-ndmKY0MAa94M9GNKdQU4OUdYAI)\n","\n","\n"],"metadata":{"id":"amcUkyGhZf3g"}},{"cell_type":"code","source":["# TODO: Implement the class `ResNetBlock`\n","class ResNetBlock(torch.nn.Module):\n","    def __init__(self, input_channels, output_channels, stride=1):\n","        super(ResNetBlock, self).__init__()\n","\n","    def forward(self, x):\n","        return None"],"metadata":{"id":"UJ9YUgsRbzjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Architecture\n","\n","* Your next task is to implement the ResNet macroblock as a `torch.nn.Module` called `ResNetMacroblock`.\n","\n","* The constructor of the `ResNetMacroblock` should receive the number of input channels and a boolean variable that indicates whether it is the first macroblock in the ResNet18 architecture (whose output shape matches the input shape).\n","\n","* The following image summarizes the ResNet-18 architecture.\n","\n","![ResNet-18 architecture.](https://drive.google.com/uc?export=view&id=1QTqgZHKRYV_engdzG7ZpsJdRPEzri_ab)\n","\n","* The input image goes through a convolutional layer with $64$ convolutional filters, each with a $7 \\times 7$ window, padding $3$, and stride $2$.\n","\n","* The resulting image goes through a batch normalization layer for images and a rectified linear activation function.\n","\n","* The resulting image goes through a max-pooling layer with $3 \\times 3$ windows, padding $1$, and stride $2$.\n","\n","* The resulting image goes through a sequence of four **ResNet macroblocks**, each of which is composed of $2$ ResNet blocks. In total, there are 8 ResNet blocks.\n","\n","* The first ResNet block in each ResNet macroblock employs stride $2$, which requires the input image to go through a convolutional layer with $1 \\times 1$ windows and stride $2$. Because the input image and the output image will necessarily have different shapes, this block also uses the opportunity to double the number of channels.\n","\n","* Exceptionally, the first ResNet block in the first ResNet macroblock does not employ a convolutional layer with $1 \\times 1$ windows. Therefore, its input image and output image will necessarily have the same shape.\n","\n","* The image resulting from the sequence of four ResNet macroblocks has each of its $512$ channels averaged, resulting in a $512$-dimensional vector that goes through a single fully connected layer to produce a $q$-dimensional vector of logits, where $q$ is the number of classes in the classification problem.\n"],"metadata":{"id":"-cIb4NaCiAdg"}},{"cell_type":"code","source":["# TODO: Implement the class `ResNetMacroblock`\n","class ResNetMacroblock(torch.nn.Module):\n","    def __init__(self, input_channels, first_macroblock=False):\n","        super(ResNetMacroblock, self).__init__()\n","\n","    def forward(self, x):\n","        return None"],"metadata":{"id":"EDW3Jd_mdxdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResNet18(torch.nn.Module):\n","    def __init__(self, num_outputs):\n","        super(ResNet18, self).__init__()\n","\n","        self.num_outputs = num_outputs\n","\n","        self.conv = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3) # Assuming 1 x 224 x 224 input images\n","        self.bn = torch.nn.BatchNorm2d(64)\n","        self.relu = torch.nn.ReLU()\n","        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.rmb1 = ResNetMacroblock(64, first_macroblock=True)\n","        self.rmb2 = ResNetMacroblock(64)\n","        self.rmb3 = ResNetMacroblock(128)\n","        self.rmb4 = ResNetMacroblock(256)\n","\n","        self.channel_avg = torch.nn.AdaptiveAvgPool2d((1, 1))\n","        self.flatten = torch.nn.Flatten()\n","        self.linear = torch.nn.Linear(512, self.num_outputs)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        out = self.maxpool(out)\n","\n","        out = self.rmb1(out)\n","        out = self.rmb2(out)\n","        out = self.rmb3(out)\n","        out = self.rmb4(out)\n","\n","        out = self.channel_avg(out)\n","        out = self.flatten(out)\n","        out = self.linear(out)\n","\n","        return out\n"],"metadata":{"id":"mb6ZCd5Ji5jR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applies Xavier initialization if the `torch.nn.Module` is `torch.nn.Linear` or `torch.nn.Conv2d`\n","def init_weights(m):\n","    if type(m) == torch.nn.Linear or type(m) == torch.nn.Conv2d:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","\n","num_outputs = 10\n","\n","model = ResNet18(10).to(device) # Creates and moves the model to `device`\n","model.apply(init_weights) # Applies `init_weights` to every `torch.nn.Module` inside `model`"],"metadata":{"id":"mVm8CigRpT5z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loss function\n"],"metadata":{"id":"I1bWZuTTpr0-"}},{"cell_type":"code","source":["loss = torch.nn.CrossEntropyLoss()"],"metadata":{"id":"N60NFd7Qpwzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimization Algorithm\n"],"metadata":{"id":"EFGyPE1LpynB"}},{"cell_type":"code","source":["lr = 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)"],"metadata":{"id":"qXHtHHaCpz0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation\n"],"metadata":{"id":"3b2yKjBup7tB"}},{"cell_type":"code","source":["def correct(logits, y):\n","    y_hat = logits.argmax(axis=1) # Finds the column with the highest value for each row of `logits`.\n","    return (y_hat == y).float().sum() # Computes the number of times that `y_hat` and `y` match.\n","\n","# Example: 1 correct classification,\n","y = torch.tensor([2, 1])\n","logits = torch.tensor([[0.1, 0.3, 0.6], [0.5, 0.2, 0.3]])\n","print(correct(logits, y))"],"metadata":{"id":"rwXMQRjtp-fJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_metric(model, data_iter, metric):\n","    \"\"\"Compute the average `metric` of the model on a dataset.\"\"\"\n","    c = 0.\n","    n = 0.\n","    for X, y in data_iter:\n","        X, y = X.to(device), y.to(device) # Moves data to `device`\n","        logits = model(X)\n","        c += metric(logits, y)\n","        n += len(y)\n","\n","    return float(c / n)"],"metadata":{"id":"3FPKGCJZqAln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval() # This is necessary because batch normalization behaves differently between training and evaluation\n","print(f'Training accuracy: {evaluate_metric(model, train_iter, correct)}. Testing accuracy: {evaluate_metric(model, test_iter, correct)}.')"],"metadata":{"id":"yqAfGhAlqCmp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Executing the cell above takes several minutes in a CPU.\n"],"metadata":{"id":"W-UQzaWlqHDh"}},{"cell_type":"markdown","source":["## Training\n","\n","* Training your ResNet18 is optional. If you want to train your ResNet18 using Google Colab, make sure to select a GPU runtime (Runtime > Change Runtime type).\n","\n","* **If you plan on using Google Colab for the programming project without a subscription, you should be very mindful about your GPU usage. For example, only change to a GPU runtime once you are ready to train your model.**\n"],"metadata":{"id":"qT5e1eiFqTTL"}},{"cell_type":"code","source":["losses = [] # Stores the loss for each training batch\n","train_accs = [] # Stores the training accuracy after each epoch\n","test_accs = [] # Stores the testing accuracy after each epoch\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    print(f'\\nEpoch {epoch + 1}/{num_epochs}.')\n","    start_time = time.perf_counter()\n","\n","    model.train() # This is necessary because batch normalization behaves differently between training and evaluation\n","\n","    for X, y in train_iter:\n","        X, y = X.to(device), y.to(device) # Moves data to `device`\n","        logits = model(X) # Computes the logits for the batch of images `X`\n","\n","        l = loss(logits, y) # Computes the loss given the `logits` and the class vector `y`\n","        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n","        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n","\n","        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n","\n","        losses.append(float(l)) # Stores the loss for this batch\n","\n","    with torch.no_grad(): # Computing performance metrics does not require gradients\n","        model.eval() # This is necessary because batch normalization behaves differently between training and evaluation\n","        train_accs.append(evaluate_metric(model, train_iter, correct))\n","        test_accs.append(evaluate_metric(model, test_iter, correct))\n","\n","        end_time = time.perf_counter()\n","\n","        print(f'Training accuracy: {train_accs[-1]}. Testing accuracy: {test_accs[-1]}. Duration: {end_time - start_time:.3f}s.') # Computes and displays training/testing dataset accuracy.\n","\n","plt.plot(losses) # Plots the loss for each training batch\n","plt.xlabel('Training batch')\n","plt.ylabel('Cross entropy loss')\n","plt.show()\n","\n","plt.plot(train_accs, label='Training accuracy')\n","plt.plot(test_accs, label='Testing accuracy')\n","plt.legend(loc='best')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"oSL5i_VSqUg4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","celltoolbar":"Slideshow","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}