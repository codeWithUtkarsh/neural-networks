{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4304,"status":"ok","timestamp":1711199593527,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"rBRbT6Vhepca"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"eBHr_XSahn-6"},"source":["# Long Short-Term Memory Networks: Implementation\n","\n","* This notebook shows how an autoregressive language model based on a long short-term memory network can be implemented in PyTorch.\n","\n","* This model will be trained using the book [Frankenstein; or, The Modern Prometheus](https://www.gutenberg.org/cache/epub/84/pg84.txt) by Mary Wollstonecraft Shelley.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7nfvtNSvT8go"},"source":["# Organizing the Dataset\n","\n","\n","* The following code downloads the book and stores it into a Python string."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2015,"status":"ok","timestamp":1711199595535,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"WPOKMzjMRkj8"},"outputs":[],"source":["import requests\n","\n","# Project Gutenberg has many books stored as text files (https://www.gutenberg.org/)\n","raw_text = requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt').text # Downloads and stores the text into a string\n","raw_text = raw_text.partition('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[2] # Removes foreword\n","raw_text = raw_text.partition('*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[0] # Removes afterword"]},{"cell_type":"markdown","metadata":{"id":"kT2LAMnrUqQw"},"source":["* In the following code:\n","    * The function `preprocess` converts a string so that any sequence of non-letters is substituted by a whitespace and uppercase letters are substituted by corresponding lowercase letters.\n","    * The function `tokenize` converts a string into a list of tokens (in this case, characters).\n","    * The class `Vocabulary` implements a vocabulary."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1711199595538,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"SRW97hHQRQFR"},"outputs":[],"source":["import re\n","import collections\n","\n","\n","def preprocess(text):\n","    text = re.sub('[^A-Za-z]+', ' ', text) # Substitutes any sequence of non-letters by a whitespace\n","    text = text.lower() # Converts uppercase letters to lowercase letters\n","\n","    return text\n","\n","\n","def tokenize(text, use_chars):\n","    if use_chars: # One token for each character\n","        return list(text)\n","    else: # One token for each sequence of letters\n","        return text.split()\n","\n","\n","class Vocabulary:\n","    def __init__(self, tokens):\n","        counter = collections.Counter(tokens)\n","        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # List of pairs where each pair is composed of a token and its frequency. Sorted according to decreasing frequency.\n","\n","        self.unknown = '?' # Represents an unknown token\n","\n","        self.id_to_token = sorted([token for token, freq in self.token_freqs]) + [self.unknown] # Maps an index to a token\n","        self.token_to_id = {token: id for id, token in enumerate(self.id_to_token)} # Maps a token to an index\n","\n","    def __len__(self):\n","        return len(self.id_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)): # If called with a single token\n","            return self.token_to_id.get(tokens, self.token_to_id[self.unknown])\n","        else: # If called with a list of tokens\n","            return [self.token_to_id.get(token, self.token_to_id[self.unknown]) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)): # If called with a single index\n","            return self.id_to_token[indices]\n","        else: # If called with a list of indices\n","            return [self.id_to_token[index] for index in indices]"]},{"cell_type":"markdown","metadata":{"id":"M56cn7z_UzWp"},"source":["* The following code converts the raw text into a sequence of numbers, each of which corresponds to a token (in this case, a character)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":487,"status":"ok","timestamp":1711199595998,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"6z7ngAInSzst","outputId":"1c66e550-6bbe-4e04-dd4c-bea07c0048bd"},"outputs":[{"output_type":"stream","name":"stdout","text":[" frankenstein or the modern prometheus\n","[' ', 'f', 'r', 'a', 'n', 'k', 'e', 'n', 's', 't', 'e', 'i', 'n', ' ', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'r', 'n', ' ', 'p', 'r', 'o', 'm', 'e', 't', 'h', 'e', 'u', 's']\n","[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '?']\n","[0, 6, 18, 1, 14, 11, 5, 14, 19, 20, 5, 9, 14, 0, 15, 18, 0, 20, 8, 5, 0, 13, 15, 4, 5, 18, 14, 0, 16, 18, 15, 13, 5, 20, 8, 5, 21, 19]\n","[' ', 'f', 'r', 'a', 'n', 'k', 'e', 'n', 's', 't', 'e', 'i', 'n', ' ', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'r', 'n', ' ', 'p', 'r', 'o', 'm', 'e', 't', 'h', 'e', 'u', 's']\n"]}],"source":["text = preprocess(raw_text)\n","print(text[:38]) # Prints the first 38 characters\n","\n","tokens = tokenize(text, use_chars=True)\n","print(tokens[:38])\n","\n","vocab = Vocabulary(tokens)\n","print(vocab.id_to_token)\n","\n","indices = vocab[tokens]\n","print(indices[:38])\n","\n","tokens_from_indices = vocab.to_tokens(indices)\n","print(tokens_from_indices[:38])"]},{"cell_type":"markdown","metadata":{"id":"Ivi4IF9TVRCL"},"source":["* For a pre-defined batch size $n$ and a pre-defined number of steps $T$, the sequence of indices will be partitioned into subsequences of length $T$, which will be grouped into batches of size $n$.\n","\n","* The following code implements sequential partitioning using a Python generator.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1711199595999,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"rrpYHDGXTA9c"},"outputs":[],"source":["def sequential_partitioning(sequence, batch_size, num_steps, offset=None, batch_first=False):\n","    if offset is None:\n","        offset = int(torch.randint(num_steps, (1,))) # A random number between 0 and `num_steps` (excluding `num_steps`)\n","\n","    sequence = sequence[offset:] # Discards the first `offset` elements of the sequence\n","\n","    len_macro_subseq = (len(sequence) - 1) // batch_size # The length of each macro-subsequence. There is one macro-subsequence for each unit of batch size\n","\n","    num_elements = len_macro_subseq * batch_size # The number of elements that fit into the macro-subsequences\n","\n","    Xs = torch.tensor(sequence[: num_elements]) # Selects the elements that fit into the macro-subsequences\n","    Ys = torch.tensor(sequence[1: num_elements + 1]) # Selects the elements that fit into the macro-subsequences (shifted by one index)\n","\n","    Xs = Xs.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence\n","    Ys = Ys.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence (shifted by one index)\n","\n","    num_subseqs = Xs.shape[1] // num_steps # The number of subsequences that fit into each macro-subsequence\n","\n","    for i in range(0, num_subseqs * num_steps, num_steps):\n","        X = Xs[:, i: i + num_steps] # Each row of `X` contains a subsequence from a distinct macro-subsequence\n","        Y = Ys[:, i: i + num_steps] # Each row of `Y` contains a subsequence from a distinct macro-subsequence (shifted by one index)\n","\n","        if batch_first:\n","            yield X, Y # Yields a pair of `batch_size` x `num_steps` matrices\n","        else:\n","            yield X.T, Y.T # Yields a pair of `num_steps` x `batch_size` matrices"]},{"cell_type":"markdown","metadata":{"id":"RkfQRcF8XqBx"},"source":["* The following code displays the first three batches of input subsequences and target subsequences (converted into batches of tokens)."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":697,"status":"ok","timestamp":1711199596680,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"gxmmvDOrTgqh","outputId":"e03bdb5c-b20a-4469-e689-55bb14872187"},"outputs":[{"output_type":"stream","name":"stdout","text":["Partitioning and batching the sequence of token indices (first three batches, converted into batches of tokens):\n","Inputs: \n","[[' ' 'i' ' ' ' ']\n"," ['f' 's' 'a' 'o']\n"," ['r' ' ' 't' 'n']\n"," ['a' 'm' ' ' 'l']\n"," ['n' 'a' 'p' 'y']\n"," ['k' 'n' 'r' ' ']\n"," ['e' 'y' 'e' 't']\n"," ['n' ' ' 's' 'h']\n"," ['s' 'l' 'e' 'e']\n"," ['t' 'e' 'n' ' ']\n"," ['e' 't' 't' 's']\n"," ['i' 't' ' ' 'o']\n"," ['n' 'e' 'e' 'u']\n"," [' ' 'r' 'x' 'n']\n"," ['o' 's' 'i' 'd']\n"," ['r' ' ' 's' ' ']].\n","Targets: \n","[['f' 's' 'a' 'o']\n"," ['r' ' ' 't' 'n']\n"," ['a' 'm' ' ' 'l']\n"," ['n' 'a' 'p' 'y']\n"," ['k' 'n' 'r' ' ']\n"," ['e' 'y' 'e' 't']\n"," ['n' ' ' 's' 'h']\n"," ['s' 'l' 'e' 'e']\n"," ['t' 'e' 'n' ' ']\n"," ['e' 't' 't' 's']\n"," ['i' 't' ' ' 'o']\n"," ['n' 'e' 'e' 'u']\n"," [' ' 'r' 'x' 'n']\n"," ['o' 's' 'i' 'd']\n"," ['r' ' ' 's' ' ']\n"," [' ' 'w' 't' 'o']].\n","\n","\n","Inputs: \n","[[' ' 'w' 't' 'o']\n"," ['t' 'e' 'i' 'f']\n"," ['h' ' ' 'n' ' ']\n"," ['e' 'a' 'g' 't']\n"," [' ' 'r' ' ' 'h']\n"," ['m' 'e' 'i' 'e']\n"," ['o' ' ' 'n' ' ']\n"," ['d' 's' ' ' 'b']\n"," ['e' 'i' 't' 'o']\n"," ['r' 'n' 'h' 'a']\n"," ['n' 'c' 'e' 't']\n"," [' ' 'e' ' ' ' ']\n"," ['p' 'r' 'w' 'a']\n"," ['r' 'e' 'o' 's']\n"," ['o' 'l' 'r' ' ']\n"," ['m' 'y' 'l' 'i']].\n","Targets: \n","[['t' 'e' 'i' 'f']\n"," ['h' ' ' 'n' ' ']\n"," ['e' 'a' 'g' 't']\n"," [' ' 'r' ' ' 'h']\n"," ['m' 'e' 'i' 'e']\n"," ['o' ' ' 'n' ' ']\n"," ['d' 's' ' ' 'b']\n"," ['e' 'i' 't' 'o']\n"," ['r' 'n' 'h' 'a']\n"," ['n' 'c' 'e' 't']\n"," [' ' 'e' ' ' ' ']\n"," ['p' 'r' 'w' 'a']\n"," ['r' 'e' 'o' 's']\n"," ['o' 'l' 'r' ' ']\n"," ['m' 'y' 'l' 'i']\n"," ['e' ' ' 'd' 't']].\n","\n","\n","Inputs: \n","[['e' ' ' 'd' 't']\n"," ['t' 'g' ' ' 's']\n"," ['h' 'r' 'i' ' ']\n"," ['e' 'a' 't' 'k']\n"," ['u' 't' ' ' 'e']\n"," ['s' 'e' 'g' 'e']\n"," [' ' 'f' 'a' 'l']\n"," ['b' 'u' 'v' ' ']\n"," ['y' 'l' 'e' 'c']\n"," [' ' ' ' ' ' 'u']\n"," ['m' 'a' 'm' 't']\n"," ['a' 'd' 'e' ' ']\n"," ['r' 'i' ' ' 't']\n"," ['y' 'e' 'a' 'h']\n"," [' ' 'u' 'n' 'r']\n"," ['w' ' ' ' ' 'o']].\n","Targets: \n","[['t' 'g' ' ' 's']\n"," ['h' 'r' 'i' ' ']\n"," ['e' 'a' 't' 'k']\n"," ['u' 't' ' ' 'e']\n"," ['s' 'e' 'g' 'e']\n"," [' ' 'f' 'a' 'l']\n"," ['b' 'u' 'v' ' ']\n"," ['y' 'l' 'e' 'c']\n"," [' ' ' ' ' ' 'u']\n"," ['m' 'a' 'm' 't']\n"," ['a' 'd' 'e' ' ']\n"," ['r' 'i' ' ' 't']\n"," ['y' 'e' 'a' 'h']\n"," [' ' 'u' 'n' 'r']\n"," ['w' ' ' ' ' 'o']\n"," ['o' 'm' 'i' 'u']].\n","\n","\n"]}],"source":["import numpy as np\n","\n","print('Partitioning and batching the sequence of token indices (first three batches, converted into batches of tokens):')\n","for i, (X, Y) in enumerate(sequential_partitioning(indices, batch_size=4, num_steps=16, offset=0)):\n","    X_tokens = np.array([vocab.to_tokens(list(X[j])) for j in range(X.shape[0])])\n","    Y_tokens = np.array([vocab.to_tokens(list(Y[j])) for j in range(Y.shape[0])])\n","    print(f'Inputs: \\n{X_tokens}.')\n","    print(f'Targets: \\n{Y_tokens}.\\n')\n","    print()\n","\n","    if i >= 2:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"u3wB7_-QzCZq"},"source":["# Long Short-Term Memory Layer\n","\n","* The following code implements a long short-term memory layer as a PyTorch module called `LSTM`. PyTorch implements an analogous module called [`torch.nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n","\n","* The method `forward` will receive a $T \\times n \\times d$ tensor called `input`, where $T$ is the number of time steps, $n$ is the batch size, and $d$ is the size of the vocabulary (number of distinct tokens). It will also receive a pair of $1 \\times n \\times h$ tensors called `h_0_c0`, where $h$ is the number of units in the recurrent layer.\n","    * If the tensor `input` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each matrix has one row for each unit of batch size, so that each row contains a (transposed) input vector. These input vectors are the result of one-hot encoding a token into a vector with $d$ elements.\n","    * Let `h_0` and `c_0` denote the elements of the pair `h_0_c_0`. If the tensor `h_0` is interpreted as a list with a single matrix, this matrix corresponds to the initial hidden state matrix. If the tensor `c_0` is interpreted as a list with a single matrix, this matrix corresponds to the initial cell state matrix.\n","\n","* The method `forward` will output a $T \\times n \\times h$ tensor called `outputs`. It will also output pair of $1 \\times n \\times h$ tensors called `h_c`.\n","    * If the tensor `outputs` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each matrix has one row for each unit of batch size, so that each row contains a (transposed) hidden state vector.\n","\n","    * Let `h` and `c` denote the elements of the pair `h_c`. If the tensor `h` is interpreted as a list with a single matrix, this matrix corresponds to the last hiddden state matrix. If the tensor `c` is interpreted as a list with a single matrix, this matrix corresponds to the last cell state matrix."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1711199596682,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"ognM67PJYG1r"},"outputs":[],"source":["class LSTM(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, sigma=0.01):\n","        super(LSTM, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # Input gate parameters\n","        self.WII = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRI = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix that transforms the previous hidden state vectors\n","        self.bI = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","        # Forget gate parameters\n","        self.WIF = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRF = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix that transforms the previous hidden state vectors\n","        self.bF = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","        # Output gate parameters\n","        self.WIO = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRO = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix that transforms the previous hidden state vectors\n","        self.bO = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","        # Candidate cell parameters\n","        self.WIC = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRC = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix that transforms the previous hidden state vectors\n","        self.bC = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","    def forward(self, input, h_0_c0):\n","        \"\"\" Keyword arguments:\n","        `input` -- a `num_steps` x `batch_size` x `input_size` tensor\n","        `h_0_c0`   -- a pair of 1 x `batch_size` x `hidden_size` tensors (PyTorch convention)\n","        \"\"\"\n","        h_0, c_0 = h_0_c0\n","\n","        hidden_state = h_0.reshape(h_0.shape[1], h_0.shape[2]) # A `batch_size` x `hidden_size` initial hidden state matrix\n","        cell_state = c_0.reshape(c_0.shape[1], c_0.shape[2]) # A `batch_size` x `hidden_size` initial cell state matrix\n","\n","        outputs = [] # A list that contains the hidden state matrix for each time step\n","        for X in input: # `X` is a `batch_size` x `input_size` matrix that contains the input vectors for a given time step\n","            I = torch.sigmoid(torch.matmul(X, self.WII) + torch.matmul(hidden_state, self.WRI) + self.bI) # A `batch_size` x `hidden_size` input gate matrix for a given time step\n","            F = torch.sigmoid(torch.matmul(X, self.WIF) + torch.matmul(hidden_state, self.WRF) + self.bF) # A `batch_size` x `hidden_size` forget gate matrix for a given time step\n","            O = torch.sigmoid(torch.matmul(X, self.WIO) + torch.matmul(hidden_state, self.WRO) + self.bO) # A `batch_size` x `hidden_size` output gate matrix for a given time step\n","\n","            C_tilde = torch.tanh(torch.matmul(X, self.WIC) + torch.matmul(hidden_state, self.WRC) + self.bC) # A `batch_size` x `hidden_size` candidate cell state matrix for a given time step\n","            cell_state = F * cell_state + I * C_tilde # A `batch_size` x `hidden_size` cell state matrix for a given time step\n","            hidden_state = O * torch.tanh(cell_state) # A `batch_size` x `hidden_size` hidden state matrix for a given time step\n","\n","            outputs.append(hidden_state)\n","\n","        outputs = torch.stack(outputs) # A `num_steps` x `batch_size` x `hidden_size` tensor\n","\n","        hidden_state = hidden_state.reshape(1, hidden_state.shape[0], hidden_state.shape[1]) # The last hidden state matrix, reshaped into a 1 x `batch_size` x `hidden_size` tensor (PyTorch convention)\n","        cell_state = cell_state.reshape(1, cell_state.shape[0], cell_state.shape[1]) # The last cell state matrix, reshaped into a 1 x `batch_size` x `hidden_size` tensor (PyTorch convention)\n","\n","        h_c = (hidden_state, cell_state)\n","\n","        return outputs, h_c"]},{"cell_type":"markdown","metadata":{"id":"xnR0BOABqQ0b"},"source":["# Long Short-Term Memory Language Model\n","\n","* The following code implements an autoregressive language model based on a long short-term memory network in a PyTorch module called `LSTMLanguageModel`.\n","\n","* The method `forward` receives a $T \\times n$ tensor called `X` and a pair of $1 \\times n \\times h$ tensors called `state`, where $T$ is the number of time steps, $n$ is the batch size, and $h$ is the number of long short-term memory units.\n","    * The elements of the tensor `X` are one-hot encoded, resulting in a $T \\times n \\times d$ tensor called `input` that is provided to the recurrent layer together with the `state`, where $d$ is the length of the vocabulary (number of distinct tokens).\n","\n","* The method `forward` outputs a $ T \\times n \\times d$ tensor called `outputs` and a pair of $1 \\times n \\times h$ tensors called `state`.\n","    * If the tensor `outputs` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each (logits) matrix has one row for each unit of batch size, so that each row contains a (transposed) logits vector.\n","\n","    * Let `h` and `c` denote the elements of the pair `state`. If the tensor `h` is interpreted as a list with a single matrix, this matrix corresponds to the last hiddden state matrix. If the tensor `c` is interpreted as a list with a single matrix, this matrix corresponds to the last cell state matrix.\n","\n","* The method `generate` receives a string `start_text` and an integer `n_tokens` and generates a string with `len(start_text) + n_tokens` tokens by using the long short-term memory network as an autoregressive language model.\n","\n","* The `temperature` allows control over the sampling process. Lowering the temperature causes less probable tokens to be sampled even less frequently, and raising the temperature causes less probable tokens to be sampled more frequently.\n","\n","* Note that you can choose between our implementation of a long short-term memory layer (`LSTM`) and the PyTorch implementation of a long short-term memory layer (`torch.nn.LSTM`)."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":438,"status":"ok","timestamp":1711199597106,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"},"user_tz":0},"id":"WLYD2TK5ovPE"},"outputs":[],"source":["class LSTMLanguageModel(torch.nn.Module):\n","    def __init__(self, vocab, num_hidden):\n","        super(LSTMLanguageModel, self).__init__()\n","\n","        self.vocab = vocab\n","        self.num_hidden = num_hidden\n","\n","        # Uncomment one of the following lines to choose between `LSTM` (our implementation) and `torch.nn.LSTM` (PyTorch implementation)\n","        self.lstm = LSTM(len(self.vocab), num_hidden) # A long short-term memory layer with as many input units as there are distinct tokens and `num_hidden` units\n","        # self.lstm = torch.nn.LSTM(len(self.vocab), num_hidden) # A  long short-term memory layer with as many input units as there are distinct tokens and `num_hidden` units\n","\n","        self.linear = torch.nn.Linear(num_hidden, len(self.vocab)) # An output layer with as many output units as there are distinct tokens\n","        self.softmax = torch.nn.Softmax(dim=1) # The softmax activation function, which will be used to sample tokens\n","\n","    def forward(self, X, state=None):\n","        \"\"\" Keyword arguments:\n","        `X`     -- a `num_steps` x `batch_size` tensor (batch of token indices)\n","        `state` -- a pair of 1 x `batch_size` x `hidden_size` tensors\n","        \"\"\"\n","        input = torch.nn.functional.one_hot(X, num_classes=len(self.vocab)).float() # One-hot encodes the elements of `X`, which results in a `num_steps` x `batch_size` x `len(vocab)` tensor\n","\n","        if state is None: # If the initial state is not provided, use a pair of tensors filled with zeros\n","            state = self.initial_state(X.shape[1])\n","\n","        outputs, state = self.lstm(input, state) # `outputs` is a `num_steps` x `batch_size` x `num_hidden` tensor\n","\n","        # Computes the logits tensor (list of logits matrices)\n","        outputs = self.linear(outputs) # `outputs` is a `num_steps` x `batch_size` x `len(vocab)` tensor obtained by transforming each of the hidden state vectors by a linear layer\n","\n","        return outputs, state # Returns a `num_steps` x `batch_size` x `len(vocab)` tensor and a pair of 1 x `batch_size` x `num_hidden` tensors\n","\n","    def initial_state(self, batch_size):\n","        return torch.zeros(1, batch_size, self.num_hidden), torch.zeros(1, batch_size, self.num_hidden) # pair of 1 x `batch_size` x `num_hidden` tensors filled with zeros\n","\n","    def generate(self, start_text, n_tokens, temperature=1.0):\n","        \"\"\" Keyword arguments:\n","        `start_text`  -- a string with the initial text\n","        `n_tokens`    -- the number of tokens to be generated\n","        `temperature` -- lowering this value causes less probable tokens to be sampled even less frequently\n","        \"\"\"\n","        tokens = tokenize(start_text, use_chars=True) # Converts the initial text into a list of characters\n","        indices = self.vocab[tokens] # Obtains the indices corresponding to the characters\n","\n","        state = self.initial_state(1) # Creates an initial state for a batch composed of a single sequence of indices\n","        X = torch.tensor(indices).reshape(-1, 1) # Organizes the indices into a `len(indices)` x 1 tensor\n","\n","        with torch.no_grad(): # Backpropagation will not be required\n","            for _ in range(n_tokens): # For each token to be generated\n","                outputs, state = self(X, state) # Obtains the outputs and state of the long short-term memory network for the batch of indices `X` given the previous `state`\n","                logits = outputs[-1] # The 1 x `len(vocab)` logits matrix for the last time step\n","\n","                p = self.softmax(logits / temperature).reshape(-1).numpy() # Divides the logits by the temperature, applies the softmax activation function, an reshapes the result into a probability vector `p`\n","\n","                index = np.random.choice(len(self.vocab), p=p) # Samples a token index according to the probabilities in `p`\n","                indices.append(index) # Appends the token index to the list of indices\n","\n","                X = torch.tensor([[index]]) # Organizes the last index into a 1 x 1 tensor, which may be given to the long short-term memory network together with the current `state`\n","\n","        tokens = self.vocab.to_tokens(indices) # Converts the list of indices into a list of characters\n","\n","        return ''.join(tokens) # Returns the result of concatenating the list of characters into a string"]},{"cell_type":"markdown","metadata":{"id":"5TGNNvv0ejPp"},"source":["# Training\n","\n","* The following code defines the hyperparameters, model, loss, and optimizer. It also implements the training loop.\n","\n","* Note that a hidden state matrix and a cell state matrix are carried across batches within an epoch. This enables predicting the first token index of a given subsequence based on the corresponding final state of the previous batch, which justifies sequential partitioning.\n","\n","* This effectively implements *truncated backpropagation through time*."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vaELK53DdX78","executionInfo":{"status":"ok","timestamp":1711204987823,"user_tz":0,"elapsed":5390723,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"dc22d6d2-8725-433c-eb25-ac45349d1955"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200. Loss: 2.79987.\n","Epoch 2/200. Loss: 2.33085.\n","Epoch 3/200. Loss: 2.13581.\n","Epoch 4/200. Loss: 2.02974.\n","Epoch 5/200. Loss: 1.95366.\n","Epoch 6/200. Loss: 1.89096.\n","Epoch 7/200. Loss: 1.83455.\n","Epoch 8/200. Loss: 1.78195.\n","Epoch 9/200. Loss: 1.73253.\n","Epoch 10/200. Loss: 1.68585.\n","Epoch 11/200. Loss: 1.64217.\n","Epoch 12/200. Loss: 1.60132.\n","Epoch 13/200. Loss: 1.56199.\n","Epoch 14/200. Loss: 1.52509.\n","Epoch 15/200. Loss: 1.49044.\n","Epoch 16/200. Loss: 1.45840.\n","Epoch 17/200. Loss: 1.42893.\n","Epoch 18/200. Loss: 1.40168.\n","Epoch 19/200. Loss: 1.37668.\n","Epoch 20/200. Loss: 1.35376.\n","Epoch 21/200. Loss: 1.33306.\n","Epoch 22/200. Loss: 1.31315.\n","Epoch 23/200. Loss: 1.29474.\n","Epoch 24/200. Loss: 1.27798.\n","Epoch 25/200. Loss: 1.26196.\n","Epoch 26/200. Loss: 1.24769.\n","Epoch 27/200. Loss: 1.23339.\n","Epoch 28/200. Loss: 1.22036.\n","Epoch 29/200. Loss: 1.20834.\n","Epoch 30/200. Loss: 1.19619.\n","Epoch 31/200. Loss: 1.18471.\n","Epoch 32/200. Loss: 1.17353.\n","Epoch 33/200. Loss: 1.16286.\n","Epoch 34/200. Loss: 1.15300.\n","Epoch 35/200. Loss: 1.14314.\n","Epoch 36/200. Loss: 1.13365.\n","Epoch 37/200. Loss: 1.12468.\n","Epoch 38/200. Loss: 1.11605.\n","Epoch 39/200. Loss: 1.10798.\n","Epoch 40/200. Loss: 1.10036.\n","Epoch 41/200. Loss: 1.09222.\n","Epoch 42/200. Loss: 1.08528.\n","Epoch 43/200. Loss: 1.07864.\n","Epoch 44/200. Loss: 1.07237.\n","Epoch 45/200. Loss: 1.06700.\n","Epoch 46/200. Loss: 1.06123.\n","Epoch 47/200. Loss: 1.05631.\n","Epoch 48/200. Loss: 1.04951.\n","Epoch 49/200. Loss: 1.04249.\n","Epoch 50/200. Loss: 1.03746.\n","Epoch 51/200. Loss: 1.03232.\n","Epoch 52/200. Loss: 1.03148.\n","Epoch 53/200. Loss: 1.02547.\n","Epoch 54/200. Loss: 1.01642.\n","Epoch 55/200. Loss: 1.00695.\n","Epoch 56/200. Loss: 0.99960.\n","Epoch 57/200. Loss: 0.99190.\n","Epoch 58/200. Loss: 0.98712.\n","Epoch 59/200. Loss: 0.98463.\n","Epoch 60/200. Loss: 0.98433.\n","Epoch 61/200. Loss: 0.98385.\n","Epoch 62/200. Loss: 0.97821.\n","Epoch 63/200. Loss: 0.97098.\n","Epoch 64/200. Loss: 0.96400.\n","Epoch 65/200. Loss: 0.95838.\n","Epoch 66/200. Loss: 0.95054.\n","Epoch 67/200. Loss: 0.94601.\n","Epoch 68/200. Loss: 0.94580.\n","Epoch 69/200. Loss: 0.94977.\n","Epoch 70/200. Loss: 0.94652.\n","Epoch 71/200. Loss: 0.93898.\n","Epoch 72/200. Loss: 0.93380.\n","Epoch 73/200. Loss: 0.92902.\n","Epoch 74/200. Loss: 0.92722.\n","Epoch 75/200. Loss: 0.92565.\n","Epoch 76/200. Loss: 0.91956.\n","Epoch 77/200. Loss: 0.92051.\n","Epoch 78/200. Loss: 0.92364.\n","Epoch 79/200. Loss: 0.91676.\n","Epoch 80/200. Loss: 0.90830.\n","Epoch 81/200. Loss: 0.90197.\n","Epoch 82/200. Loss: 0.89984.\n","Epoch 83/200. Loss: 0.89958.\n","Epoch 84/200. Loss: 0.89913.\n","Epoch 85/200. Loss: 0.89669.\n","Epoch 86/200. Loss: 0.89098.\n","Epoch 87/200. Loss: 0.88384.\n","Epoch 88/200. Loss: 0.88006.\n","Epoch 89/200. Loss: 0.87915.\n","Epoch 90/200. Loss: 0.87837.\n","Epoch 91/200. Loss: 0.87707.\n","Epoch 92/200. Loss: 0.87221.\n","Epoch 93/200. Loss: 0.87043.\n","Epoch 94/200. Loss: 0.86237.\n","Epoch 95/200. Loss: 0.85508.\n","Epoch 96/200. Loss: 0.85525.\n","Epoch 97/200. Loss: 0.85568.\n","Epoch 98/200. Loss: 0.85941.\n","Epoch 99/200. Loss: 0.85930.\n","Epoch 100/200. Loss: 0.85396.\n","Epoch 101/200. Loss: 0.85430.\n","Epoch 102/200. Loss: 0.85287.\n","Epoch 103/200. Loss: 0.85314.\n","Epoch 104/200. Loss: 0.85517.\n","Epoch 105/200. Loss: 0.85397.\n","Epoch 106/200. Loss: 0.84442.\n","Epoch 107/200. Loss: 0.84226.\n","Epoch 108/200. Loss: 0.83310.\n","Epoch 109/200. Loss: 0.82896.\n","Epoch 110/200. Loss: 0.82579.\n","Epoch 111/200. Loss: 0.82351.\n","Epoch 112/200. Loss: 0.82111.\n","Epoch 113/200. Loss: 0.82087.\n","Epoch 114/200. Loss: 0.82033.\n","Epoch 115/200. Loss: 0.82081.\n","Epoch 116/200. Loss: 0.81411.\n","Epoch 117/200. Loss: 0.81343.\n","Epoch 118/200. Loss: 0.81768.\n","Epoch 119/200. Loss: 0.81919.\n","Epoch 120/200. Loss: 0.81222.\n","Epoch 121/200. Loss: 0.80945.\n","Epoch 122/200. Loss: 0.80187.\n","Epoch 123/200. Loss: 0.79960.\n","Epoch 124/200. Loss: 0.79878.\n","Epoch 125/200. Loss: 0.79823.\n","Epoch 126/200. Loss: 0.79732.\n","Epoch 127/200. Loss: 0.79110.\n","Epoch 128/200. Loss: 0.78935.\n","Epoch 129/200. Loss: 0.78781.\n","Epoch 130/200. Loss: 0.79084.\n","Epoch 131/200. Loss: 0.79250.\n","Epoch 132/200. Loss: 0.79097.\n","Epoch 133/200. Loss: 0.78775.\n","Epoch 134/200. Loss: 0.79164.\n","Epoch 135/200. Loss: 0.79249.\n","Epoch 136/200. Loss: 0.78706.\n","Epoch 137/200. Loss: 0.78508.\n","Epoch 138/200. Loss: 0.78619.\n","Epoch 139/200. Loss: 0.78325.\n","Epoch 140/200. Loss: 0.78557.\n","Epoch 141/200. Loss: 0.78185.\n","Epoch 142/200. Loss: 0.78355.\n","Epoch 143/200. Loss: 0.77783.\n","Epoch 144/200. Loss: 0.77330.\n","Epoch 145/200. Loss: 0.77457.\n","Epoch 146/200. Loss: 0.77245.\n","Epoch 147/200. Loss: 0.77251.\n","Epoch 148/200. Loss: 0.77095.\n","Epoch 149/200. Loss: 0.77119.\n","Epoch 150/200. Loss: 0.77093.\n","Epoch 151/200. Loss: 0.76285.\n","Epoch 152/200. Loss: 0.76165.\n","Epoch 153/200. Loss: 0.75973.\n","Epoch 154/200. Loss: 0.75780.\n","Epoch 155/200. Loss: 0.75590.\n","Epoch 156/200. Loss: 0.75684.\n","Epoch 157/200. Loss: 0.75058.\n","Epoch 158/200. Loss: 0.75156.\n","Epoch 159/200. Loss: 0.74949.\n","Epoch 160/200. Loss: 0.74410.\n","Epoch 161/200. Loss: 0.74500.\n","Epoch 162/200. Loss: 0.74647.\n","Epoch 163/200. Loss: 0.74396.\n","Epoch 164/200. Loss: 0.74210.\n","Epoch 165/200. Loss: 0.74215.\n","Epoch 166/200. Loss: 0.73715.\n","Epoch 167/200. Loss: 0.74123.\n","Epoch 168/200. Loss: 0.73671.\n","Epoch 169/200. Loss: 0.73782.\n","Epoch 170/200. Loss: 0.73787.\n","Epoch 171/200. Loss: 0.73500.\n","Epoch 172/200. Loss: 0.73911.\n","Epoch 173/200. Loss: 0.73583.\n","Epoch 174/200. Loss: 0.73436.\n","Epoch 175/200. Loss: 0.73502.\n","Epoch 176/200. Loss: 0.73790.\n","Epoch 177/200. Loss: 0.72937.\n","Epoch 178/200. Loss: 0.72442.\n","Epoch 179/200. Loss: 0.72407.\n","Epoch 180/200. Loss: 0.72117.\n","Epoch 181/200. Loss: 0.71918.\n","Epoch 182/200. Loss: 0.71659.\n","Epoch 183/200. Loss: 0.71870.\n","Epoch 184/200. Loss: 0.71338.\n","Epoch 185/200. Loss: 0.71077.\n","Epoch 186/200. Loss: 0.70981.\n","Epoch 187/200. Loss: 0.70777.\n","Epoch 188/200. Loss: 0.70744.\n","Epoch 189/200. Loss: 0.70784.\n","Epoch 190/200. Loss: 0.70999.\n","Epoch 191/200. Loss: 0.70258.\n","Epoch 192/200. Loss: 0.70200.\n","Epoch 193/200. Loss: 0.70193.\n","Epoch 194/200. Loss: 0.70042.\n","Epoch 195/200. Loss: 0.69895.\n","Epoch 196/200. Loss: 0.69758.\n","Epoch 197/200. Loss: 0.70068.\n","Epoch 198/200. Loss: 0.69789.\n","Epoch 199/200. Loss: 0.69637.\n","Epoch 200/200. Loss: 0.69199.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Hyperparameters\n","num_hidden = 256 # The number of units in the long short-term memory network\n","lr = 0.001 # The learning rate\n","num_epochs = 200 # The number of epochs for training\n","num_steps = 64 # The length of the training sequences\n","batch_size = 32 # The number of training sequences per batch\n","\n","# Model\n","lstmlm = LSTMLanguageModel(vocab, num_hidden=num_hidden)\n","\n","# Loss and optimizer\n","loss = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(lstmlm.parameters(), lr=lr)\n","\n","# Training loop\n","epoch_losses = []\n","for epoch in range(num_epochs):\n","    train_iter = sequential_partitioning(indices, batch_size=batch_size, num_steps=num_steps)\n","\n","    state = lstmlm.initial_state(batch_size) # A pair of 1 x `batch_size` x `num_hidden` tensors filled with zeros\n","\n","    losses = [] # Stores the loss for each batch\n","\n","    for X, Y in train_iter:\n","        logits, state = lstmlm(X, state) # Computes the `num_steps` x `batch_size` x `len(vocab)` logits tensor for the `num_steps` x `batch_size` tensor of indices `X`, based on the current `state`\n","\n","        Y_flat = Y.reshape(-1) # Flattens the `num_steps` x `batch_size` tensor of (target) indices `Y` into a vector with `num_steps` * `batch_size` elements\n","        logits_flat = logits.reshape(-1, logits.shape[2]) # Reshapes the `num_steps` x `batch_size` x `len(vocab)` logits tensor into a `num_steps` * `batch_size` x `len(vocab)` logits matrix\n","\n","        l = loss(logits_flat, Y_flat) # Computes the loss given the logits matrix `logits_flat` and the target vector `Y_flat`\n","\n","        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n","        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n","        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n","\n","        losses.append(float(l)) # Stores the loss for this batch\n","\n","        state = state[0].clone().detach(), state[1].clone().detach() # A pair of 1 x `batch_size` x `num_hidden` tensors that carry the current state across batches\n","\n","    epoch_losses.append(np.mean(losses))\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}. Loss: {epoch_losses[-1]:.5f}.')\n","\n","plt.plot(epoch_losses) # Plots the average loss across batches for each epoch\n","plt.xlabel('Epoch')\n","plt.ylabel('Average cross entropy loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C64gC7k1emOY"},"source":["# Text generation\n","\n","* The following code generates text using the trained long short-term memory language model.\n","\n","* You may obtain different results by running the cell multiple times (and changing the temperature)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nKHNx5ha5NFZ","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1711206790715,"user_tz":0,"elapsed":599,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"ff188b86-77fa-4e02-f2c0-41b69d85380f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'therefore and the result of my mind and the sea and was expected upon that could ill my fortune could to undertake the door forcif science which had been the scenery of his child suploms and early around himself with his father spoke of your misery that was decised in my bosom exclame for the most and belove in the mind too would be suppluesiby of my friend i saw frized myself at the trancleste of immediaten shions i have no relaties in the morning who had dir often in the mist carr i had entered into the room in th'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["lstmlm.generate('therefore', n_tokens=512, temperature=0.1)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuddRE2dEVYWmV4su9c0Fp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}