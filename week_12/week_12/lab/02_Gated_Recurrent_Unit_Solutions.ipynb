{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNVHwr2soVuBpeGkFCv3r3z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import torch"],"metadata":{"id":"rBRbT6Vhepca","executionInfo":{"status":"ok","timestamp":1711185920796,"user_tz":0,"elapsed":1720,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Gated Recurrent Units: Implementation\n","\n","* In this lab session, you will implement a autoregressive language model based on a gated recurrent unit network in PyTorch.\n","\n","* This model will be trained using the book [Frankenstein; or, The Modern Prometheus](https://www.gutenberg.org/cache/epub/84/pg84.txt) by Mary Wollstonecraft Shelley.\n","\n"],"metadata":{"id":"eBHr_XSahn-6"}},{"cell_type":"markdown","source":["# Organizing the Dataset\n","\n","\n","* The following code downloads the book and stores it into a Python string."],"metadata":{"id":"7nfvtNSvT8go"}},{"cell_type":"code","source":["import requests\n","\n","# Project Gutenberg has many books stored as text files (https://www.gutenberg.org/)\n","raw_text = requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt').text # Downloads and stores the text into a string\n","raw_text = raw_text.partition('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[2] # Removes foreword\n","raw_text = raw_text.partition('*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[0] # Removes afterword"],"metadata":{"id":"WPOKMzjMRkj8","executionInfo":{"status":"ok","timestamp":1711185921794,"user_tz":0,"elapsed":1002,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["* In the following code:\n","    * The function `preprocess` converts a string so that any sequence of non-letters is substituted by a whitespace and uppercase letters are substituted by corresponding lowercase letters.\n","    * The function `tokenize` converts a string into a list of tokens (in this case, characters).\n","    * The class `Vocabulary` implements a vocabulary."],"metadata":{"id":"kT2LAMnrUqQw"}},{"cell_type":"code","source":["import re\n","import collections\n","\n","\n","def preprocess(text):\n","    text = re.sub('[^A-Za-z]+', ' ', text) # Substitutes any sequence of non-letters by a whitespace\n","    text = text.lower() # Converts uppercase letters to lowercase letters\n","\n","    return text\n","\n","\n","def tokenize(text, use_chars):\n","    if use_chars: # One token for each character\n","        return list(text)\n","    else: # One token for each sequence of letters\n","        return text.split()\n","\n","\n","class Vocabulary:\n","    def __init__(self, tokens):\n","        counter = collections.Counter(tokens)\n","        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # List of pairs where each pair is composed of a token and its frequency. Sorted according to decreasing frequency.\n","\n","        self.unknown = '?' # Represents an unknown token\n","\n","        self.id_to_token = sorted([token for token, freq in self.token_freqs]) + [self.unknown] # Maps an index to a token\n","        self.token_to_id = {token: id for id, token in enumerate(self.id_to_token)} # Maps a token to an index\n","\n","    def __len__(self):\n","        return len(self.id_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)): # If called with a single token\n","            return self.token_to_id.get(tokens, self.token_to_id[self.unknown])\n","        else: # If called with a list of tokens\n","            return [self.token_to_id.get(token, self.token_to_id[self.unknown]) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)): # If called with a single index\n","            return self.id_to_token[indices]\n","        else: # If called with a list of indices\n","            return [self.id_to_token[index] for index in indices]"],"metadata":{"id":"SRW97hHQRQFR","executionInfo":{"status":"ok","timestamp":1711185921795,"user_tz":0,"elapsed":32,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["* The following code converts the raw text into a sequence of numbers, each of which corresponds to a token (in this case, a character)."],"metadata":{"id":"M56cn7z_UzWp"}},{"cell_type":"code","source":["text = preprocess(raw_text)\n","print(text[:38]) # Prints the first 38 characters\n","\n","tokens = tokenize(text, use_chars=True)\n","print(tokens[:38])\n","\n","vocab = Vocabulary(tokens)\n","print(vocab.id_to_token)\n","\n","indices = vocab[tokens]\n","print(indices[:38])\n","\n","tokens_from_indices = vocab.to_tokens(indices)\n","print(tokens_from_indices[:38])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z7ngAInSzst","executionInfo":{"status":"ok","timestamp":1711185922242,"user_tz":0,"elapsed":474,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"ce319010-ef95-48f0-b5cc-f5faabb31ca5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":[" frankenstein or the modern prometheus\n","[' ', 'f', 'r', 'a', 'n', 'k', 'e', 'n', 's', 't', 'e', 'i', 'n', ' ', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'r', 'n', ' ', 'p', 'r', 'o', 'm', 'e', 't', 'h', 'e', 'u', 's']\n","[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '?']\n","[0, 6, 18, 1, 14, 11, 5, 14, 19, 20, 5, 9, 14, 0, 15, 18, 0, 20, 8, 5, 0, 13, 15, 4, 5, 18, 14, 0, 16, 18, 15, 13, 5, 20, 8, 5, 21, 19]\n","[' ', 'f', 'r', 'a', 'n', 'k', 'e', 'n', 's', 't', 'e', 'i', 'n', ' ', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'r', 'n', ' ', 'p', 'r', 'o', 'm', 'e', 't', 'h', 'e', 'u', 's']\n"]}]},{"cell_type":"markdown","source":["* For a pre-defined batch size $n$ and a pre-defined number of steps $T$, the sequence of indices will be partitioned into subsequences of length $T$, which will be grouped into batches of size $n$.\n","\n","* The following code implements sequential partitioning using a Python generator.\n"],"metadata":{"id":"Ivi4IF9TVRCL"}},{"cell_type":"code","source":["def sequential_partitioning(sequence, batch_size, num_steps, offset=None, batch_first=False):\n","    if offset is None:\n","        offset = int(torch.randint(num_steps, (1,))) # A random number between 0 and `num_steps` (excluding `num_steps`)\n","\n","    sequence = sequence[offset:] # Discards the first `offset` elements of the sequence\n","\n","    len_macro_subseq = (len(sequence) - 1) // batch_size # The length of each macro-subsequence. There is one macro-subsequence for each unit of batch size\n","\n","    num_elements = len_macro_subseq * batch_size # The number of elements that fit into the macro-subsequences\n","\n","    Xs = torch.tensor(sequence[: num_elements]) # Selects the elements that fit into the macro-subsequences\n","    Ys = torch.tensor(sequence[1: num_elements + 1]) # Selects the elements that fit into the macro-subsequences (shifted by one index)\n","\n","    Xs = Xs.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence\n","    Ys = Ys.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence (shifted by one index)\n","\n","    num_subseqs = Xs.shape[1] // num_steps # The number of subsequences that fit into each macro-subsequence\n","\n","    for i in range(0, num_subseqs * num_steps, num_steps):\n","        X = Xs[:, i: i + num_steps] # Each row of `X` contains a subsequence from a distinct macro-subsequence\n","        Y = Ys[:, i: i + num_steps] # Each row of `Y` contains a subsequence from a distinct macro-subsequence (shifted by one index)\n","\n","        if batch_first:\n","            yield X, Y # Yields a pair of `batch_size` x `num_steps` matrices\n","        else:\n","            yield X.T, Y.T # Yields a pair of `num_steps` x `batch_size` matrices"],"metadata":{"id":"rrpYHDGXTA9c","executionInfo":{"status":"ok","timestamp":1711185922244,"user_tz":0,"elapsed":24,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["* The following code displays the first three batches of input subsequences and target subsequences (converted into batches of tokens)."],"metadata":{"id":"RkfQRcF8XqBx"}},{"cell_type":"code","source":["import numpy as np\n","\n","print('Partitioning and batching the sequence of token indices (first three batches, converted into batches of tokens):')\n","for i, (X, Y) in enumerate(sequential_partitioning(indices, batch_size=4, num_steps=16, offset=0)):\n","    X_tokens = np.array([vocab.to_tokens(list(X[j])) for j in range(X.shape[0])])\n","    Y_tokens = np.array([vocab.to_tokens(list(Y[j])) for j in range(Y.shape[0])])\n","    print(f'Inputs: \\n{X_tokens}.')\n","    print(f'Targets: \\n{Y_tokens}.\\n')\n","    print()\n","\n","    if i >= 2:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxmmvDOrTgqh","executionInfo":{"status":"ok","timestamp":1711185922245,"user_tz":0,"elapsed":24,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"4ef7271f-1d88-4966-9aff-c56d61432c73"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Partitioning and batching the sequence of token indices (first three batches, converted into batches of tokens):\n","Inputs: \n","[[' ' 'i' ' ' ' ']\n"," ['f' 's' 'a' 'o']\n"," ['r' ' ' 't' 'n']\n"," ['a' 'm' ' ' 'l']\n"," ['n' 'a' 'p' 'y']\n"," ['k' 'n' 'r' ' ']\n"," ['e' 'y' 'e' 't']\n"," ['n' ' ' 's' 'h']\n"," ['s' 'l' 'e' 'e']\n"," ['t' 'e' 'n' ' ']\n"," ['e' 't' 't' 's']\n"," ['i' 't' ' ' 'o']\n"," ['n' 'e' 'e' 'u']\n"," [' ' 'r' 'x' 'n']\n"," ['o' 's' 'i' 'd']\n"," ['r' ' ' 's' ' ']].\n","Targets: \n","[['f' 's' 'a' 'o']\n"," ['r' ' ' 't' 'n']\n"," ['a' 'm' ' ' 'l']\n"," ['n' 'a' 'p' 'y']\n"," ['k' 'n' 'r' ' ']\n"," ['e' 'y' 'e' 't']\n"," ['n' ' ' 's' 'h']\n"," ['s' 'l' 'e' 'e']\n"," ['t' 'e' 'n' ' ']\n"," ['e' 't' 't' 's']\n"," ['i' 't' ' ' 'o']\n"," ['n' 'e' 'e' 'u']\n"," [' ' 'r' 'x' 'n']\n"," ['o' 's' 'i' 'd']\n"," ['r' ' ' 's' ' ']\n"," [' ' 'w' 't' 'o']].\n","\n","\n","Inputs: \n","[[' ' 'w' 't' 'o']\n"," ['t' 'e' 'i' 'f']\n"," ['h' ' ' 'n' ' ']\n"," ['e' 'a' 'g' 't']\n"," [' ' 'r' ' ' 'h']\n"," ['m' 'e' 'i' 'e']\n"," ['o' ' ' 'n' ' ']\n"," ['d' 's' ' ' 'b']\n"," ['e' 'i' 't' 'o']\n"," ['r' 'n' 'h' 'a']\n"," ['n' 'c' 'e' 't']\n"," [' ' 'e' ' ' ' ']\n"," ['p' 'r' 'w' 'a']\n"," ['r' 'e' 'o' 's']\n"," ['o' 'l' 'r' ' ']\n"," ['m' 'y' 'l' 'i']].\n","Targets: \n","[['t' 'e' 'i' 'f']\n"," ['h' ' ' 'n' ' ']\n"," ['e' 'a' 'g' 't']\n"," [' ' 'r' ' ' 'h']\n"," ['m' 'e' 'i' 'e']\n"," ['o' ' ' 'n' ' ']\n"," ['d' 's' ' ' 'b']\n"," ['e' 'i' 't' 'o']\n"," ['r' 'n' 'h' 'a']\n"," ['n' 'c' 'e' 't']\n"," [' ' 'e' ' ' ' ']\n"," ['p' 'r' 'w' 'a']\n"," ['r' 'e' 'o' 's']\n"," ['o' 'l' 'r' ' ']\n"," ['m' 'y' 'l' 'i']\n"," ['e' ' ' 'd' 't']].\n","\n","\n","Inputs: \n","[['e' ' ' 'd' 't']\n"," ['t' 'g' ' ' 's']\n"," ['h' 'r' 'i' ' ']\n"," ['e' 'a' 't' 'k']\n"," ['u' 't' ' ' 'e']\n"," ['s' 'e' 'g' 'e']\n"," [' ' 'f' 'a' 'l']\n"," ['b' 'u' 'v' ' ']\n"," ['y' 'l' 'e' 'c']\n"," [' ' ' ' ' ' 'u']\n"," ['m' 'a' 'm' 't']\n"," ['a' 'd' 'e' ' ']\n"," ['r' 'i' ' ' 't']\n"," ['y' 'e' 'a' 'h']\n"," [' ' 'u' 'n' 'r']\n"," ['w' ' ' ' ' 'o']].\n","Targets: \n","[['t' 'g' ' ' 's']\n"," ['h' 'r' 'i' ' ']\n"," ['e' 'a' 't' 'k']\n"," ['u' 't' ' ' 'e']\n"," ['s' 'e' 'g' 'e']\n"," [' ' 'f' 'a' 'l']\n"," ['b' 'u' 'v' ' ']\n"," ['y' 'l' 'e' 'c']\n"," [' ' ' ' ' ' 'u']\n"," ['m' 'a' 'm' 't']\n"," ['a' 'd' 'e' ' ']\n"," ['r' 'i' ' ' 't']\n"," ['y' 'e' 'a' 'h']\n"," [' ' 'u' 'n' 'r']\n"," ['w' ' ' ' ' 'o']\n"," ['o' 'm' 'i' 'u']].\n","\n","\n"]}]},{"cell_type":"markdown","source":["# Task 1: Gated Recurrent Unit Layer"],"metadata":{"id":"tyg8ijRSYHZY"}},{"cell_type":"markdown","source":["* **Your task is to implement a gated recurrent unit as a PyTorch module called `GRU`**. PyTorch implements an analogous module called [`torch.nn.GRU`](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n","\n","* The method `forward` should receive a $T \\times n \\times d$ tensor called `input`, where $T$ is the number of time steps, $n$ is the batch size, and $d$ is the size of the vocabulary (number of distinct tokens). It should also receive a $1 \\times n \\times h$ tensor called `h_0`, where $h$ is the number of units in the gated recurrent unit layer.\n","    * If the tensor `input` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each matrix has one row for each unit of batch size, so that each row contains a (transposed) input vector. These input vectors are the result of one-hot encoding a token into a vector with $d$ elements.\n","    * If the tensor `h_0` is interpreted as a list with a single matrix, this matrix corresponds to the initial hidden state matrix.\n","\n","* The method `forward` should output a $T \\times n \\times h$ tensor called `outputs`. It should also output a $1 \\times n \\times h$ tensor called `state`.\n","    * If the tensor `outputs` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each matrix has one row for each unit of batch size, so that each row contains a (transposed) hidden state vector.\n","\n","    * If the tensor `state` is interpreted as a list with a single matrix, this matrix corresponds to the last hiddden state matrix."],"metadata":{"id":"u3wB7_-QzCZq"}},{"cell_type":"code","source":["class GRU(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, sigma=0.01):\n","        super(GRU, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # Reset gate parameters\n","        self.WIR = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRR = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix that transforms the previous hidden state vectors\n","        self.bR = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","        # Update gate parameters\n","        self.WIZ = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRZ = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix that transforms the previous hidden state vectors\n","        self.bZ = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","        # Candidate hidden state parameters\n","        self.WIH = torch.nn.Parameter(torch.randn(input_size, hidden_size) * sigma) # An `input_size` x `hidden_size` weight matrix that transforms the current input vectors\n","        self.WRH = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * sigma) # A `hidden_size` x `hidden_size` weight matrix\n","        self.bH = torch.nn.Parameter(torch.zeros(hidden_size)) # A bias vector with `hidden_size` elements\n","\n","    def forward(self, input, h_0):\n","        \"\"\" Keyword arguments:\n","        `input` -- a `num_steps` x `batch_size` x `input_size` tensor\n","        `h_0`   -- a 1 x `batch_size` x `hidden_size` tensor (PyTorch convention)\n","        \"\"\"\n","        state = h_0.reshape(h_0.shape[1], h_0.shape[2]) # A `batch_size` x `hidden_size` initial hidden state matrix\n","\n","        outputs = [] # A list that contains the hidden state matrix for each time step\n","        for X in input: # `X` is a `batch_size` x `input_size` matrix that contains the input vectors for a given time step\n","            R = torch.sigmoid(torch.matmul(X, self.WIR) + torch.matmul(state, self.WRR) + self.bR) # The `batch_size` x `hidden_size` reset gate matrix for a given time step\n","            Z = torch.sigmoid(torch.matmul(X, self.WIZ) + torch.matmul(state, self.WRZ) + self.bZ) # The `batch_size` x `hidden_size` update gate matrix for a given time step\n","\n","            H_tilde = torch.tanh(torch.matmul(X, self.WIH) + torch.matmul(R * state, self.WRH) + self.bH) # The `batch_size` x `hidden_size` candidate hidden state matrix for a given time step\n","            state = Z * state + (1 - Z) * H_tilde # The `batch_size` x `hidden_size` hidden state matrix for a given time step\n","\n","            outputs.append(state)\n","\n","        outputs = torch.stack(outputs) # A `num_steps` x `batch_size` x `hidden_size` tensor\n","        state = state.reshape(1, state.shape[0], state.shape[1]) # The last hidden state matrix, reshaped into a 1 x `batch_size` x `hidden_size` tensor (PyTorch convention)\n","\n","        return outputs, state"],"metadata":{"id":"ognM67PJYG1r","executionInfo":{"status":"ok","timestamp":1711185922246,"user_tz":0,"elapsed":18,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Gated Recurrent Unit Language Model\n","\n","* The following code implements an autoregressive language model based on a gated recurrent unit network in a PyTorch module called `GRULanguageModel`.\n","\n","* The method `forward` receives a $T \\times n$ tensor called `X` and a $1 \\times n \\times h$ tensor called `state`, where $T$ is the number of time steps, $n$ is the batch size, and $h$ is the number of gated recurrent units.\n","    * The elements of the tensor `X` are one-hot encoded, resulting in a $T \\times n \\times d$ tensor called `input` that is provided to the gated recurrent unit layer together with the `state`, where $d$ is the length of the vocabulary (number of distinct tokens).\n","\n","* The method `forward` outputs a $ T \\times n \\times d$ tensor called `outputs` and a $1 \\times n \\times h$ tensor called `state`.\n","    * If the tensor `outputs` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each (logits) matrix has one row for each unit of batch size, so that each row contains a (transposed) logits vector.\n","\n","    * If the tensor `state` is interpreted as a list with a single matrix, this matrix corresponds to the last hiddden state matrix.\n","\n","* The method `generate` receives a string `start_text` and an integer `n_tokens` and generates a string with `len(start_text) + n_tokens` tokens by using the gated recurrent unit network as an autoregressive language model.\n","\n","* The `temperature` allows control over the sampling process. Lowering the temperature causes less probable tokens to be sampled even less frequently, and raising the temperature causes less probable tokens to be sampled more frequently.\n","\n","* Note that you can choose between our implementation of a gated recurrent unit layer (`GRU`) and the PyTorch implementation of a gated recurrent unit layer (`torch.nn.GRU`)."],"metadata":{"id":"xnR0BOABqQ0b"}},{"cell_type":"code","source":["class GRULanguageModel(torch.nn.Module):\n","    def __init__(self, vocab, num_hidden):\n","        super(GRULanguageModel, self).__init__()\n","\n","        self.vocab = vocab\n","        self.num_hidden = num_hidden\n","\n","        # Uncomment one of the following lines to choose between `GRU` (our implementation) and `torch.nn.GRU` (PyTorch implementation)\n","        self.gru = GRU(len(self.vocab), num_hidden) # A gated recurrent layer with as many input units as there are distinct tokens and `num_hidden` units\n","        # self.gru = torch.nn.GRU(len(self.vocab), num_hidden) # A gated recurrent layer with as many input units as there are distinct tokens and `num_hidden` units\n","\n","        self.linear = torch.nn.Linear(num_hidden, len(self.vocab)) # An output layer with as many output units as there are distinct tokens\n","        self.softmax = torch.nn.Softmax(dim=1) # The softmax activation function, which will be used to sample tokens\n","\n","    def forward(self, X, state=None):\n","        \"\"\" Keyword arguments:\n","        `X`     -- a `num_steps` x `batch_size` tensor (batch of token indices)\n","        `state` -- a 1 x `batch_size` x `num_hidden` tensor\n","        \"\"\"\n","        input = torch.nn.functional.one_hot(X, num_classes=len(self.vocab)).float() # One-hot encodes the elements of `X`, which results in a `num_steps` x `batch_size` x `len(vocab)` tensor\n","\n","        if state is None: # If the initial state is not provided, use a tensor filled with zeros\n","            state = self.initial_state(X.shape[1])\n","\n","        outputs, state = self.gru(input, state) # `outputs` is a `num_steps` x `batch_size` x `num_hidden` tensor\n","\n","        # Computes the logits tensor (list of logits matrices)\n","        outputs = self.linear(outputs) # `outputs` is a `num_steps` x `batch_size` x `len(vocab)` tensor obtained by transforming each of the hidden state vectors by a linear layer\n","\n","        return outputs, state # Returns a `num_steps` x `batch_size` x `len(vocab)` tensor and a 1 x `batch_size` x `num_hidden` tensor\n","\n","    def initial_state(self, batch_size):\n","        return torch.zeros(1, batch_size, self.num_hidden) # A 1 x `batch_size` x `num_hidden` tensor filled with zeros\n","\n","    def generate(self, start_text, n_tokens, temperature=1.0):\n","        \"\"\" Keyword arguments:\n","        `start_text`  -- a string with the initial text\n","        `n_tokens`    -- the number of tokens to be generated\n","        `temperature` -- lowering this value causes less probable tokens to be sampled even less frequently\n","        \"\"\"\n","        tokens = tokenize(start_text, use_chars=True) # Converts the initial text into a list of characters\n","        indices = self.vocab[tokens] # Obtains the indices corresponding to the characters\n","\n","        state = self.initial_state(1) # Creates an initial state for a batch composed of a single sequence of indices\n","        X = torch.tensor(indices).reshape(-1, 1) # Organizes the indices into a `len(indices)` x 1 tensor\n","\n","        with torch.no_grad(): # Backpropagation will not be required\n","            for _ in range(n_tokens): # For each token to be generated\n","                outputs, state = self(X, state) # Obtains the outputs and state of the gated recurrent unit network for the batch of indices `X` given the previous `state`\n","                logits = outputs[-1] # The 1 x `len(vocab)` logits matrix for the last time step\n","\n","                p = self.softmax(logits / temperature).reshape(-1).numpy() # Divides the logits by the temperature, applies the softmax activation function, an reshapes the result into a probability vector `p`\n","\n","                index = np.random.choice(len(self.vocab), p=p) # Samples a token index according to the probabilities in `p`\n","                indices.append(index) # Appends the token index to the list of indices\n","\n","                X = torch.tensor([[index]]) # Organizes the last index into a 1 x 1 tensor, which may be given to the gated recurrent unit network together with the current `state`\n","\n","        tokens = self.vocab.to_tokens(indices) # Converts the list of indices into a list of characters\n","\n","        return ''.join(tokens) # Returns the result of concatenating the list of characters into a string\n"],"metadata":{"id":"WLYD2TK5ovPE","executionInfo":{"status":"ok","timestamp":1711185922247,"user_tz":0,"elapsed":18,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","\n","* The following code defines the hyperparameters, model, loss, and optimizer. It also implements the training loop.\n","\n","* Note that a hidden state matrix is carried across batches within an epoch. This enables predicting the first token index of a given subsequence based on the corresponding final hidden state vector of the previous batch, which justifies sequential partitioning.\n","\n","* This effectively implements *truncated backpropagation through time*."],"metadata":{"id":"5TGNNvv0ejPp"}},{"cell_type":"code","source":["# Hyperparameters\n","num_hidden = 256 # The number of units in the gated recurrent unit network\n","lr = 0.001 # The learning rate\n","num_epochs = 200 # The number of epochs for training\n","num_steps = 64 # The length of the training sequences\n","batch_size = 32 # The number of training sequences per batch\n","\n","# Model\n","grulm = GRULanguageModel(vocab, num_hidden=num_hidden)\n","\n","# Loss and optimizer\n","loss = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(grulm.parameters(), lr=lr)\n","\n","# Training loop\n","epoch_losses = []\n","for epoch in range(num_epochs):\n","    train_iter = sequential_partitioning(indices, batch_size=batch_size, num_steps=num_steps)\n","\n","    state = grulm.initial_state(batch_size) # A 1 x `batch_size` x `num_hidden` tensor filled with zeros\n","\n","    losses = [] # Stores the loss for each batch\n","\n","    for X, Y in train_iter:\n","        logits, state = grulm(X, state) # Computes the `num_steps` x `batch_size` x `len(vocab)` logits tensor for the `num_steps` x `batch_size` tensor of indices `X`, based on the current `state`\n","\n","        Y_flat = Y.reshape(-1) # Flattens the `num_steps` x `batch_size` tensor of (target) indices `Y` into a vector with `num_steps` * `batch_size` elements\n","        logits_flat = logits.reshape(-1, logits.shape[2]) # Reshapes the `num_steps` x `batch_size` x `len(vocab)` logits tensor into a `num_steps` * `batch_size` x `len(vocab)` logits matrix\n","\n","        l = loss(logits_flat, Y_flat) # Computes the loss given the logits matrix `logits_flat` and the target vector `Y_flat`\n","\n","        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n","        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n","        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n","\n","        losses.append(float(l)) # Stores the loss for this batch\n","\n","        state = state.clone().detach() # A 1 x `batch_size` x `num_hidden` tensor that carries the current state across batches\n","\n","    epoch_losses.append(np.mean(losses))\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}. Loss: {epoch_losses[-1]:.5f}.')\n","\n","plt.plot(epoch_losses) # Plots the average loss across batches for each epoch\n","plt.xlabel('Epoch')\n","plt.ylabel('Average cross entropy loss')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vaELK53DdX78","executionInfo":{"status":"ok","timestamp":1711190303112,"user_tz":0,"elapsed":4380881,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"166487fc-a7cd-4bcb-8c8f-4769be6e4eca"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200. Loss: 2.73758.\n","Epoch 2/200. Loss: 2.21265.\n","Epoch 3/200. Loss: 2.01323.\n","Epoch 4/200. Loss: 1.86719.\n","Epoch 5/200. Loss: 1.75831.\n","Epoch 6/200. Loss: 1.67132.\n","Epoch 7/200. Loss: 1.59804.\n","Epoch 8/200. Loss: 1.53529.\n","Epoch 9/200. Loss: 1.48015.\n","Epoch 10/200. Loss: 1.43218.\n","Epoch 11/200. Loss: 1.39105.\n","Epoch 12/200. Loss: 1.35563.\n","Epoch 13/200. Loss: 1.32401.\n","Epoch 14/200. Loss: 1.29584.\n","Epoch 15/200. Loss: 1.27105.\n","Epoch 16/200. Loss: 1.24871.\n","Epoch 17/200. Loss: 1.22830.\n","Epoch 18/200. Loss: 1.20980.\n","Epoch 19/200. Loss: 1.19274.\n","Epoch 20/200. Loss: 1.17664.\n","Epoch 21/200. Loss: 1.16134.\n","Epoch 22/200. Loss: 1.14666.\n","Epoch 23/200. Loss: 1.13262.\n","Epoch 24/200. Loss: 1.11918.\n","Epoch 25/200. Loss: 1.10605.\n","Epoch 26/200. Loss: 1.09331.\n","Epoch 27/200. Loss: 1.08121.\n","Epoch 28/200. Loss: 1.06933.\n","Epoch 29/200. Loss: 1.05795.\n","Epoch 30/200. Loss: 1.04650.\n","Epoch 31/200. Loss: 1.03619.\n","Epoch 32/200. Loss: 1.02609.\n","Epoch 33/200. Loss: 1.01757.\n","Epoch 34/200. Loss: 1.01246.\n","Epoch 35/200. Loss: 1.00833.\n","Epoch 36/200. Loss: 1.00056.\n","Epoch 37/200. Loss: 0.99049.\n","Epoch 38/200. Loss: 0.97970.\n","Epoch 39/200. Loss: 0.97052.\n","Epoch 40/200. Loss: 0.96364.\n","Epoch 41/200. Loss: 0.95914.\n","Epoch 42/200. Loss: 0.95510.\n","Epoch 43/200. Loss: 0.95264.\n","Epoch 44/200. Loss: 0.94904.\n","Epoch 45/200. Loss: 0.94033.\n","Epoch 46/200. Loss: 0.93412.\n","Epoch 47/200. Loss: 0.92628.\n","Epoch 48/200. Loss: 0.92213.\n","Epoch 49/200. Loss: 0.92088.\n","Epoch 50/200. Loss: 0.91228.\n","Epoch 51/200. Loss: 0.90777.\n","Epoch 52/200. Loss: 0.90537.\n","Epoch 53/200. Loss: 0.89916.\n","Epoch 54/200. Loss: 0.89649.\n","Epoch 55/200. Loss: 0.89060.\n","Epoch 56/200. Loss: 0.88688.\n","Epoch 57/200. Loss: 0.88352.\n","Epoch 58/200. Loss: 0.88108.\n","Epoch 59/200. Loss: 0.87723.\n","Epoch 60/200. Loss: 0.86974.\n","Epoch 61/200. Loss: 0.87092.\n","Epoch 62/200. Loss: 0.86737.\n","Epoch 63/200. Loss: 0.86420.\n","Epoch 64/200. Loss: 0.86340.\n","Epoch 65/200. Loss: 0.85948.\n","Epoch 66/200. Loss: 0.85504.\n","Epoch 67/200. Loss: 0.85214.\n","Epoch 68/200. Loss: 0.85211.\n","Epoch 69/200. Loss: 0.84876.\n","Epoch 70/200. Loss: 0.84571.\n","Epoch 71/200. Loss: 0.84176.\n","Epoch 72/200. Loss: 0.83912.\n","Epoch 73/200. Loss: 0.83617.\n","Epoch 74/200. Loss: 0.83597.\n","Epoch 75/200. Loss: 0.83114.\n","Epoch 76/200. Loss: 0.83110.\n","Epoch 77/200. Loss: 0.82870.\n","Epoch 78/200. Loss: 0.82411.\n","Epoch 79/200. Loss: 0.82541.\n","Epoch 80/200. Loss: 0.82107.\n","Epoch 81/200. Loss: 0.82041.\n","Epoch 82/200. Loss: 0.81681.\n","Epoch 83/200. Loss: 0.81234.\n","Epoch 84/200. Loss: 0.81124.\n","Epoch 85/200. Loss: 0.81101.\n","Epoch 86/200. Loss: 0.80575.\n","Epoch 87/200. Loss: 0.80374.\n","Epoch 88/200. Loss: 0.80366.\n","Epoch 89/200. Loss: 0.80214.\n","Epoch 90/200. Loss: 0.79776.\n","Epoch 91/200. Loss: 0.79596.\n","Epoch 92/200. Loss: 0.79122.\n","Epoch 93/200. Loss: 0.79157.\n","Epoch 94/200. Loss: 0.79171.\n","Epoch 95/200. Loss: 0.78946.\n","Epoch 96/200. Loss: 0.78561.\n","Epoch 97/200. Loss: 0.78386.\n","Epoch 98/200. Loss: 0.78092.\n","Epoch 99/200. Loss: 0.78145.\n","Epoch 100/200. Loss: 0.78309.\n","Epoch 101/200. Loss: 0.77773.\n","Epoch 102/200. Loss: 0.77489.\n","Epoch 103/200. Loss: 0.77473.\n","Epoch 104/200. Loss: 0.77190.\n","Epoch 105/200. Loss: 0.77114.\n","Epoch 106/200. Loss: 0.76739.\n","Epoch 107/200. Loss: 0.76713.\n","Epoch 108/200. Loss: 0.76509.\n","Epoch 109/200. Loss: 0.76806.\n","Epoch 110/200. Loss: 0.76647.\n","Epoch 111/200. Loss: 0.76475.\n","Epoch 112/200. Loss: 0.76164.\n","Epoch 113/200. Loss: 0.75854.\n","Epoch 114/200. Loss: 0.75741.\n","Epoch 115/200. Loss: 0.75826.\n","Epoch 116/200. Loss: 0.75727.\n","Epoch 117/200. Loss: 0.75596.\n","Epoch 118/200. Loss: 0.75341.\n","Epoch 119/200. Loss: 0.75416.\n","Epoch 120/200. Loss: 0.75157.\n","Epoch 121/200. Loss: 0.75020.\n","Epoch 122/200. Loss: 0.74745.\n","Epoch 123/200. Loss: 0.74975.\n","Epoch 124/200. Loss: 0.75188.\n","Epoch 125/200. Loss: 0.74600.\n","Epoch 126/200. Loss: 0.74517.\n","Epoch 127/200. Loss: 0.74246.\n","Epoch 128/200. Loss: 0.74147.\n","Epoch 129/200. Loss: 0.74192.\n","Epoch 130/200. Loss: 0.74093.\n","Epoch 131/200. Loss: 0.73941.\n","Epoch 132/200. Loss: 0.73690.\n","Epoch 133/200. Loss: 0.74177.\n","Epoch 134/200. Loss: 0.73661.\n","Epoch 135/200. Loss: 0.73271.\n","Epoch 136/200. Loss: 0.73239.\n","Epoch 137/200. Loss: 0.73203.\n","Epoch 138/200. Loss: 0.73029.\n","Epoch 139/200. Loss: 0.73209.\n","Epoch 140/200. Loss: 0.73463.\n","Epoch 141/200. Loss: 0.73111.\n","Epoch 142/200. Loss: 0.73197.\n","Epoch 143/200. Loss: 0.72963.\n","Epoch 144/200. Loss: 0.72780.\n","Epoch 145/200. Loss: 0.72621.\n","Epoch 146/200. Loss: 0.72706.\n","Epoch 147/200. Loss: 0.72289.\n","Epoch 148/200. Loss: 0.72310.\n","Epoch 149/200. Loss: 0.72295.\n","Epoch 150/200. Loss: 0.72226.\n","Epoch 151/200. Loss: 0.72053.\n","Epoch 152/200. Loss: 0.72105.\n","Epoch 153/200. Loss: 0.71951.\n","Epoch 154/200. Loss: 0.71901.\n","Epoch 155/200. Loss: 0.72039.\n","Epoch 156/200. Loss: 0.71881.\n","Epoch 157/200. Loss: 0.71716.\n","Epoch 158/200. Loss: 0.71877.\n","Epoch 159/200. Loss: 0.71527.\n","Epoch 160/200. Loss: 0.71580.\n","Epoch 161/200. Loss: 0.71266.\n","Epoch 162/200. Loss: 0.71472.\n","Epoch 163/200. Loss: 0.71375.\n","Epoch 164/200. Loss: 0.71156.\n","Epoch 165/200. Loss: 0.70941.\n","Epoch 166/200. Loss: 0.71163.\n","Epoch 167/200. Loss: 0.71146.\n","Epoch 168/200. Loss: 0.70807.\n","Epoch 169/200. Loss: 0.70438.\n","Epoch 170/200. Loss: 0.70690.\n","Epoch 171/200. Loss: 0.70543.\n","Epoch 172/200. Loss: 0.70386.\n","Epoch 173/200. Loss: 0.70583.\n","Epoch 174/200. Loss: 0.70511.\n","Epoch 175/200. Loss: 0.71040.\n","Epoch 176/200. Loss: 0.70891.\n","Epoch 177/200. Loss: 0.70371.\n","Epoch 178/200. Loss: 0.70469.\n","Epoch 179/200. Loss: 0.70163.\n","Epoch 180/200. Loss: 0.70174.\n","Epoch 181/200. Loss: 0.70087.\n","Epoch 182/200. Loss: 0.69728.\n","Epoch 183/200. Loss: 0.70218.\n","Epoch 184/200. Loss: 0.70122.\n","Epoch 185/200. Loss: 0.69746.\n","Epoch 186/200. Loss: 0.69803.\n","Epoch 187/200. Loss: 0.69608.\n","Epoch 188/200. Loss: 0.69653.\n","Epoch 189/200. Loss: 0.69881.\n","Epoch 190/200. Loss: 0.69680.\n","Epoch 191/200. Loss: 0.69796.\n","Epoch 192/200. Loss: 0.69519.\n","Epoch 193/200. Loss: 0.69226.\n","Epoch 194/200. Loss: 0.69242.\n","Epoch 195/200. Loss: 0.69133.\n","Epoch 196/200. Loss: 0.69211.\n","Epoch 197/200. Loss: 0.69237.\n","Epoch 198/200. Loss: 0.69301.\n","Epoch 199/200. Loss: 0.68948.\n","Epoch 200/200. Loss: 0.69421.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# Text generation\n","\n","* The following code generates text using the trained gated recurrent unit language model.\n","\n","* You may obtain different results by running the cell multiple times (and changing the temperature)"],"metadata":{"id":"C64gC7k1emOY"}},{"cell_type":"code","source":["grulm.generate('therefore', n_tokens=512, temperature=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"nKHNx5ha5NFZ","executionInfo":{"status":"ok","timestamp":1711190613748,"user_tz":0,"elapsed":744,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"e824f473-bbc6-4761-d616-757e1f433f24"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'therefore what could i consider the sun had indeed allow me to expect i shall have the compassion and rain uron least and was strengte son when henry returned to the same signisity and stretched mation i am a resolution to life some of the spirits that grand upon the scene that i felt the fie the attempt and sprengul in an air of every great and feel the remainder of the same of the summits of the words in a moment from the moonstated hatred of the most distinguished discoverers he appeared to me as i believe that i'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]}]}