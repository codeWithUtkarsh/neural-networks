{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMmJmXh1oGhLQeT1+nwJz38"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import torch"],"metadata":{"id":"rBRbT6Vhepca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gated Recurrent Units: Implementation\n","\n","* In this lab session, you will implement a autoregressive language model based on a gated recurrent unit network in PyTorch.\n","\n","* This model will be trained using the book [Frankenstein; or, The Modern Prometheus](https://www.gutenberg.org/cache/epub/84/pg84.txt) by Mary Wollstonecraft Shelley.\n","\n"],"metadata":{"id":"eBHr_XSahn-6"}},{"cell_type":"markdown","source":["# Organizing the Dataset\n","\n","\n","* The following code downloads the book and stores it into a Python string."],"metadata":{"id":"7nfvtNSvT8go"}},{"cell_type":"code","source":["import requests\n","\n","# Project Gutenberg has many books stored as text files (https://www.gutenberg.org/)\n","raw_text = requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt').text # Downloads and stores the text into a string\n","raw_text = raw_text.partition('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[2] # Removes foreword\n","raw_text = raw_text.partition('*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[0] # Removes afterword"],"metadata":{"id":"WPOKMzjMRkj8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* In the following code:\n","    * The function `preprocess` converts a string so that any sequence of non-letters is substituted by a whitespace and uppercase letters are substituted by corresponding lowercase letters.\n","    * The function `tokenize` converts a string into a list of tokens (in this case, characters).\n","    * The class `Vocabulary` implements a vocabulary."],"metadata":{"id":"kT2LAMnrUqQw"}},{"cell_type":"code","source":["import re\n","import collections\n","\n","\n","def preprocess(text):\n","    text = re.sub('[^A-Za-z]+', ' ', text) # Substitutes any sequence of non-letters by a whitespace\n","    text = text.lower() # Converts uppercase letters to lowercase letters\n","\n","    return text\n","\n","\n","def tokenize(text, use_chars):\n","    if use_chars: # One token for each character\n","        return list(text)\n","    else: # One token for each sequence of letters\n","        return text.split()\n","\n","\n","class Vocabulary:\n","    def __init__(self, tokens):\n","        counter = collections.Counter(tokens)\n","        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # List of pairs where each pair is composed of a token and its frequency. Sorted according to decreasing frequency.\n","\n","        self.unknown = '?' # Represents an unknown token\n","\n","        self.id_to_token = sorted([token for token, freq in self.token_freqs]) + [self.unknown] # Maps an index to a token\n","        self.token_to_id = {token: id for id, token in enumerate(self.id_to_token)} # Maps a token to an index\n","\n","    def __len__(self):\n","        return len(self.id_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)): # If called with a single token\n","            return self.token_to_id.get(tokens, self.token_to_id[self.unknown])\n","        else: # If called with a list of tokens\n","            return [self.token_to_id.get(token, self.token_to_id[self.unknown]) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)): # If called with a single index\n","            return self.id_to_token[indices]\n","        else: # If called with a list of indices\n","            return [self.id_to_token[index] for index in indices]"],"metadata":{"id":"SRW97hHQRQFR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* The following code converts the raw text into a sequence of numbers, each of which corresponds to a token (in this case, a character)."],"metadata":{"id":"M56cn7z_UzWp"}},{"cell_type":"code","source":["text = preprocess(raw_text)\n","print(text[:38]) # Prints the first 38 characters\n","\n","tokens = tokenize(text, use_chars=True)\n","print(tokens[:38])\n","\n","vocab = Vocabulary(tokens)\n","print(vocab.id_to_token)\n","\n","indices = vocab[tokens]\n","print(indices[:38])\n","\n","tokens_from_indices = vocab.to_tokens(indices)\n","print(tokens_from_indices[:38])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z7ngAInSzst","executionInfo":{"status":"ok","timestamp":1711043373684,"user_tz":0,"elapsed":412,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"db3d4dd3-c645-4353-bd58-fd39c4fc3962"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" frankenstein or the modern prometheus\n","[' ', 'f', 'r', 'a', 'n', 'k', 'e', 'n', 's', 't', 'e', 'i', 'n', ' ', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'r', 'n', ' ', 'p', 'r', 'o', 'm', 'e', 't', 'h', 'e', 'u', 's']\n","[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '?']\n","[0, 6, 18, 1, 14, 11, 5, 14, 19, 20, 5, 9, 14, 0, 15, 18, 0, 20, 8, 5, 0, 13, 15, 4, 5, 18, 14, 0, 16, 18, 15, 13, 5, 20, 8, 5, 21, 19]\n","[' ', 'f', 'r', 'a', 'n', 'k', 'e', 'n', 's', 't', 'e', 'i', 'n', ' ', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'r', 'n', ' ', 'p', 'r', 'o', 'm', 'e', 't', 'h', 'e', 'u', 's']\n"]}]},{"cell_type":"markdown","source":["* For a pre-defined batch size $n$ and a pre-defined number of steps $T$, the sequence of indices will be partitioned into subsequences of length $T$, which will be grouped into batches of size $n$.\n","\n","* The following code implements sequential partitioning using a Python generator.\n"],"metadata":{"id":"Ivi4IF9TVRCL"}},{"cell_type":"code","source":["def sequential_partitioning(sequence, batch_size, num_steps, offset=None, batch_first=False):\n","    if offset is None:\n","        offset = int(torch.randint(num_steps, (1,))) # A random number between 0 and `num_steps` (excluding `num_steps`)\n","\n","    sequence = sequence[offset:] # Discards the first `offset` elements of the sequence\n","\n","    len_macro_subseq = (len(sequence) - 1) // batch_size # The length of each macro-subsequence. There is one macro-subsequence for each unit of batch size\n","\n","    num_elements = len_macro_subseq * batch_size # The number of elements that fit into the macro-subsequences\n","\n","    Xs = torch.tensor(sequence[: num_elements]) # Selects the elements that fit into the macro-subsequences\n","    Ys = torch.tensor(sequence[1: num_elements + 1]) # Selects the elements that fit into the macro-subsequences (shifted by one index)\n","\n","    Xs = Xs.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence\n","    Ys = Ys.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence (shifted by one index)\n","\n","    num_subseqs = Xs.shape[1] // num_steps # The number of subsequences that fit into each macro-subsequence\n","\n","    for i in range(0, num_subseqs * num_steps, num_steps):\n","        X = Xs[:, i: i + num_steps] # Each row of `X` contains a subsequence from a distinct macro-subsequence\n","        Y = Ys[:, i: i + num_steps] # Each row of `Y` contains a subsequence from a distinct macro-subsequence (shifted by one index)\n","\n","        if batch_first:\n","            yield X, Y # Yields a pair of `batch_size` x `num_steps` matrices\n","        else:\n","            yield X.T, Y.T # Yields a pair of `num_steps` x `batch_size` matrices"],"metadata":{"id":"rrpYHDGXTA9c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* The following code displays the first three batches of input subsequences and target subsequences (converted into batches of tokens)."],"metadata":{"id":"RkfQRcF8XqBx"}},{"cell_type":"code","source":["import numpy as np\n","\n","print('Partitioning and batching the sequence of token indices (first three batches, converted into batches of tokens):')\n","for i, (X, Y) in enumerate(sequential_partitioning(indices, batch_size=4, num_steps=16, offset=0)):\n","    X_tokens = np.array([vocab.to_tokens(list(X[j])) for j in range(X.shape[0])])\n","    Y_tokens = np.array([vocab.to_tokens(list(Y[j])) for j in range(Y.shape[0])])\n","    print(f'Inputs: \\n{X_tokens}.')\n","    print(f'Targets: \\n{Y_tokens}.\\n')\n","    print()\n","\n","    if i >= 2:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxmmvDOrTgqh","executionInfo":{"status":"ok","timestamp":1711043379287,"user_tz":0,"elapsed":178,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"f62aafa5-2b59-4754-8e6c-491f5c51605d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Partitioning and batching the sequence of token indices (first three batches, converted into batches of tokens):\n","Inputs: \n","[[' ' 'i' ' ' ' ']\n"," ['f' 's' 'a' 'o']\n"," ['r' ' ' 't' 'n']\n"," ['a' 'm' ' ' 'l']\n"," ['n' 'a' 'p' 'y']\n"," ['k' 'n' 'r' ' ']\n"," ['e' 'y' 'e' 't']\n"," ['n' ' ' 's' 'h']\n"," ['s' 'l' 'e' 'e']\n"," ['t' 'e' 'n' ' ']\n"," ['e' 't' 't' 's']\n"," ['i' 't' ' ' 'o']\n"," ['n' 'e' 'e' 'u']\n"," [' ' 'r' 'x' 'n']\n"," ['o' 's' 'i' 'd']\n"," ['r' ' ' 's' ' ']].\n","Targets: \n","[['f' 's' 'a' 'o']\n"," ['r' ' ' 't' 'n']\n"," ['a' 'm' ' ' 'l']\n"," ['n' 'a' 'p' 'y']\n"," ['k' 'n' 'r' ' ']\n"," ['e' 'y' 'e' 't']\n"," ['n' ' ' 's' 'h']\n"," ['s' 'l' 'e' 'e']\n"," ['t' 'e' 'n' ' ']\n"," ['e' 't' 't' 's']\n"," ['i' 't' ' ' 'o']\n"," ['n' 'e' 'e' 'u']\n"," [' ' 'r' 'x' 'n']\n"," ['o' 's' 'i' 'd']\n"," ['r' ' ' 's' ' ']\n"," [' ' 'w' 't' 'o']].\n","\n","\n","Inputs: \n","[[' ' 'w' 't' 'o']\n"," ['t' 'e' 'i' 'f']\n"," ['h' ' ' 'n' ' ']\n"," ['e' 'a' 'g' 't']\n"," [' ' 'r' ' ' 'h']\n"," ['m' 'e' 'i' 'e']\n"," ['o' ' ' 'n' ' ']\n"," ['d' 's' ' ' 'b']\n"," ['e' 'i' 't' 'o']\n"," ['r' 'n' 'h' 'a']\n"," ['n' 'c' 'e' 't']\n"," [' ' 'e' ' ' ' ']\n"," ['p' 'r' 'w' 'a']\n"," ['r' 'e' 'o' 's']\n"," ['o' 'l' 'r' ' ']\n"," ['m' 'y' 'l' 'i']].\n","Targets: \n","[['t' 'e' 'i' 'f']\n"," ['h' ' ' 'n' ' ']\n"," ['e' 'a' 'g' 't']\n"," [' ' 'r' ' ' 'h']\n"," ['m' 'e' 'i' 'e']\n"," ['o' ' ' 'n' ' ']\n"," ['d' 's' ' ' 'b']\n"," ['e' 'i' 't' 'o']\n"," ['r' 'n' 'h' 'a']\n"," ['n' 'c' 'e' 't']\n"," [' ' 'e' ' ' ' ']\n"," ['p' 'r' 'w' 'a']\n"," ['r' 'e' 'o' 's']\n"," ['o' 'l' 'r' ' ']\n"," ['m' 'y' 'l' 'i']\n"," ['e' ' ' 'd' 't']].\n","\n","\n","Inputs: \n","[['e' ' ' 'd' 't']\n"," ['t' 'g' ' ' 's']\n"," ['h' 'r' 'i' ' ']\n"," ['e' 'a' 't' 'k']\n"," ['u' 't' ' ' 'e']\n"," ['s' 'e' 'g' 'e']\n"," [' ' 'f' 'a' 'l']\n"," ['b' 'u' 'v' ' ']\n"," ['y' 'l' 'e' 'c']\n"," [' ' ' ' ' ' 'u']\n"," ['m' 'a' 'm' 't']\n"," ['a' 'd' 'e' ' ']\n"," ['r' 'i' ' ' 't']\n"," ['y' 'e' 'a' 'h']\n"," [' ' 'u' 'n' 'r']\n"," ['w' ' ' ' ' 'o']].\n","Targets: \n","[['t' 'g' ' ' 's']\n"," ['h' 'r' 'i' ' ']\n"," ['e' 'a' 't' 'k']\n"," ['u' 't' ' ' 'e']\n"," ['s' 'e' 'g' 'e']\n"," [' ' 'f' 'a' 'l']\n"," ['b' 'u' 'v' ' ']\n"," ['y' 'l' 'e' 'c']\n"," [' ' ' ' ' ' 'u']\n"," ['m' 'a' 'm' 't']\n"," ['a' 'd' 'e' ' ']\n"," ['r' 'i' ' ' 't']\n"," ['y' 'e' 'a' 'h']\n"," [' ' 'u' 'n' 'r']\n"," ['w' ' ' ' ' 'o']\n"," ['o' 'm' 'i' 'u']].\n","\n","\n"]}]},{"cell_type":"markdown","source":["# Task 1: Gated Recurrent Unit Layer"],"metadata":{"id":"tyg8ijRSYHZY"}},{"cell_type":"markdown","source":["* **Your task is to implement a gated recurrent unit as a PyTorch module called `GRU`**. PyTorch implements an analogous module called [`torch.nn.GRU`](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n","\n","* The method `forward` should receive a $T \\times n \\times d$ tensor called `input`, where $T$ is the number of time steps, $n$ is the batch size, and $d$ is the size of the vocabulary (number of distinct tokens). It should also receive a $1 \\times n \\times h$ tensor called `h_0`, where $h$ is the number of units in the gated recurrent unit layer.\n","    * If the tensor `input` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each matrix has one row for each unit of batch size, so that each row contains a (transposed) input vector. These input vectors are the result of one-hot encoding a token into a vector with $d$ elements.\n","    * If the tensor `h_0` is interpreted as a list with a single matrix, this matrix corresponds to the initial hidden state matrix.\n","\n","* The method `forward` should output a $T \\times n \\times h$ tensor called `outputs`. It should also output a $1 \\times n \\times h$ tensor called `state`.\n","    * If the tensor `outputs` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each matrix has one row for each unit of batch size, so that each row contains a (transposed) hidden state vector.\n","\n","    * If the tensor `state` is interpreted as a list with a single matrix, this matrix corresponds to the last hiddden state matrix."],"metadata":{"id":"u3wB7_-QzCZq"}},{"cell_type":"code","source":["class GRU(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, sigma=0.01):\n","        super(GRU, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # TODO: Create parameters to compute the reset gate matrix, update gate matrix, and candidate hidden state matrix\n","\n","    def forward(self, input, h_0):\n","        \"\"\" Keyword arguments:\n","        `input` -- a `num_steps` x `batch_size` x `input_size` tensor\n","        `h_0`   -- a 1 x `batch_size` x `hidden_size` tensor (PyTorch convention)\n","        \"\"\"\n","        # TODO: Implement the forward pass of this gated recurrent unit layer\n","        return outputs, state # A `num_steps` x `batch_size` x `hidden_size` tensor and a 1 x `batch_size` x `hidden_size` tensor"],"metadata":{"id":"ognM67PJYG1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gated Recurrent Unit Language Model\n","\n","* The following code implements an autoregressive language model based on a gated recurrent unit network in a PyTorch module called `GRULanguageModel`.\n","\n","* The method `forward` receives a $T \\times n$ tensor called `X` and a $1 \\times n \\times h$ tensor called `state`, where $T$ is the number of time steps, $n$ is the batch size, and $h$ is the number of gated recurrent units.\n","    * The elements of the tensor `X` are one-hot encoded, resulting in a $T \\times n \\times d$ tensor called `input` that is provided to the gated recurrent unit layer together with the `state`, where $d$ is the length of the vocabulary (number of distinct tokens).\n","\n","* The method `forward` outputs a $ T \\times n \\times d$ tensor called `outputs` and a $1 \\times n \\times h$ tensor called `state`.\n","    * If the tensor `outputs` is interpreted as a list of $T$ matrices, each of these matrices corresponds to a single time step. Each (logits) matrix has one row for each unit of batch size, so that each row contains a (transposed) logits vector.\n","\n","    * If the tensor `state` is interpreted as a list with a single matrix, this matrix corresponds to the last hiddden state matrix.\n","\n","* The method `generate` receives a string `start_text` and an integer `n_tokens` and generates a string with `len(start_text) + n_tokens` tokens by using the gated recurrent unit network as an autoregressive language model.\n","\n","* The `temperature` allows control over the sampling process. Lowering the temperature causes less probable tokens to be sampled even less frequently, and raising the temperature causes less probable tokens to be sampled more frequently.\n","\n","* Note that you can choose between our implementation of a gated recurrent unit layer (`GRU`) and the PyTorch implementation of a gated recurrent unit layer (`torch.nn.GRU`)."],"metadata":{"id":"xnR0BOABqQ0b"}},{"cell_type":"code","source":["class GRULanguageModel(torch.nn.Module):\n","    def __init__(self, vocab, num_hidden):\n","        super(GRULanguageModel, self).__init__()\n","\n","        self.vocab = vocab\n","        self.num_hidden = num_hidden\n","\n","        # Uncomment one of the following lines to choose between `GRU` (our implementation) and `torch.nn.GRU` (PyTorch implementation)\n","        self.gru = GRU(len(self.vocab), num_hidden) # A gated recurrent layer with as many input units as there are distinct tokens and `num_hidden` units\n","        # self.gru = torch.nn.GRU(len(self.vocab), num_hidden) # A gated recurrent layer with as many input units as there are distinct tokens and `num_hidden` units\n","\n","        self.linear = torch.nn.Linear(num_hidden, len(self.vocab)) # An output layer with as many output units as there are distinct tokens\n","        self.softmax = torch.nn.Softmax(dim=1) # The softmax activation function, which will be used to sample tokens\n","\n","    def forward(self, X, state=None):\n","        \"\"\" Keyword arguments:\n","        `X`     -- a `num_steps` x `batch_size` tensor (batch of token indices)\n","        `state` -- a 1 x `batch_size` x `num_hidden` tensor\n","        \"\"\"\n","        input = torch.nn.functional.one_hot(X, num_classes=len(self.vocab)).float() # One-hot encodes the elements of `X`, which results in a `num_steps` x `batch_size` x `len(vocab)` tensor\n","\n","        if state is None: # If the initial state is not provided, use a tensor filled with zeros\n","            state = self.initial_state(X.shape[1])\n","\n","        outputs, state = self.gru(input, state) # `outputs` is a `num_steps` x `batch_size` x `num_hidden` tensor\n","\n","        # Computes the logits tensor (list of logits matrices)\n","        outputs = self.linear(outputs) # `outputs` is a `num_steps` x `batch_size` x `len(vocab)` tensor obtained by transforming each of the hidden state vectors by a linear layer\n","\n","        return outputs, state # Returns a `num_steps` x `batch_size` x `len(vocab)` tensor and a 1 x `batch_size` x `num_hidden` tensor\n","\n","    def initial_state(self, batch_size):\n","        return torch.zeros(1, batch_size, self.num_hidden) # A 1 x `batch_size` x `num_hidden` tensor filled with zeros\n","\n","    def generate(self, start_text, n_tokens, temperature=1.0):\n","        \"\"\" Keyword arguments:\n","        `start_text`  -- a string with the initial text\n","        `n_tokens`    -- the number of tokens to be generated\n","        `temperature` -- lowering this value causes less probable tokens to be sampled even less frequently\n","        \"\"\"\n","        tokens = tokenize(start_text, use_chars=True) # Converts the initial text into a list of characters\n","        indices = self.vocab[tokens] # Obtains the indices corresponding to the characters\n","\n","        state = self.initial_state(1) # Creates an initial state for a batch composed of a single sequence of indices\n","        X = torch.tensor(indices).reshape(-1, 1) # Organizes the indices into a `len(indices)` x 1 tensor\n","\n","        with torch.no_grad(): # Backpropagation will not be required\n","            for _ in range(n_tokens): # For each token to be generated\n","                outputs, state = self(X, state) # Obtains the outputs and state of the gated recurrent unit network for the batch of indices `X` given the previous `state`\n","                logits = outputs[-1] # The 1 x `len(vocab)` logits matrix for the last time step\n","\n","                p = self.softmax(logits / temperature).reshape(-1).numpy() # Divides the logits by the temperature, applies the softmax activation function, an reshapes the result into a probability vector `p`\n","\n","                index = np.random.choice(len(self.vocab), p=p) # Samples a token index according to the probabilities in `p`\n","                indices.append(index) # Appends the token index to the list of indices\n","\n","                X = torch.tensor([[index]]) # Organizes the last index into a 1 x 1 tensor, which may be given to the gated recurrent unit network together with the current `state`\n","\n","        tokens = self.vocab.to_tokens(indices) # Converts the list of indices into a list of characters\n","\n","        return ''.join(tokens) # Returns the result of concatenating the list of characters into a string\n"],"metadata":{"id":"WLYD2TK5ovPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","\n","* The following code defines the hyperparameters, model, loss, and optimizer. It also implements the training loop.\n","\n","* Note that a hidden state matrix is carried across batches within an epoch. This enables predicting the first token index of a given subsequence based on the corresponding final hidden state vector of the previous batch, which justifies sequential partitioning.\n","\n","* This effectively implements *truncated backpropagation through time*."],"metadata":{"id":"5TGNNvv0ejPp"}},{"cell_type":"code","source":["# Hyperparameters\n","num_hidden = 256 # The number of units in the gated recurrent unit network\n","lr = 0.001 # The learning rate\n","num_epochs = 200 # The number of epochs for training\n","num_steps = 64 # The length of the training sequences\n","batch_size = 32 # The number of training sequences per batch\n","\n","# Model\n","grulm = GRULanguageModel(vocab, num_hidden=num_hidden)\n","\n","# Loss and optimizer\n","loss = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(grulm.parameters(), lr=lr)\n","\n","# Training loop\n","epoch_losses = []\n","for epoch in range(num_epochs):\n","    train_iter = sequential_partitioning(indices, batch_size=batch_size, num_steps=num_steps)\n","\n","    state = grulm.initial_state(batch_size) # A 1 x `batch_size` x `num_hidden` tensor filled with zeros\n","\n","    losses = [] # Stores the loss for each batch\n","\n","    for X, Y in train_iter:\n","        logits, state = grulm(X, state) # Computes the `num_steps` x `batch_size` x `len(vocab)` logits tensor for the `num_steps` x `batch_size` tensor of indices `X`, based on the current `state`\n","\n","        Y_flat = Y.reshape(-1) # Flattens the `num_steps` x `batch_size` tensor of (target) indices `Y` into a vector with `num_steps` * `batch_size` elements\n","        logits_flat = logits.reshape(-1, logits.shape[2]) # Reshapes the `num_steps` x `batch_size` x `len(vocab)` logits tensor into a `num_steps` * `batch_size` x `len(vocab)` logits matrix\n","\n","        l = loss(logits_flat, Y_flat) # Computes the loss given the logits matrix `logits_flat` and the target vector `Y_flat`\n","\n","        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n","        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n","        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n","\n","        losses.append(float(l)) # Stores the loss for this batch\n","\n","        state = state.clone().detach() # A 1 x `batch_size` x `num_hidden` tensor that carries the current state across batches\n","\n","    epoch_losses.append(np.mean(losses))\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}. Loss: {epoch_losses[-1]:.5f}.')\n","\n","plt.plot(epoch_losses) # Plots the average loss across batches for each epoch\n","plt.xlabel('Epoch')\n","plt.ylabel('Average cross entropy loss')\n","plt.show()"],"metadata":{"id":"vaELK53DdX78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text generation\n","\n","* The following code generates text using the trained gated recurrent unit language model.\n","\n","* You may obtain different results by running the cell multiple times (and changing the temperature)"],"metadata":{"id":"C64gC7k1emOY"}},{"cell_type":"code","source":["grulm.generate('franken', n_tokens=512, temperature=0.1)"],"metadata":{"id":"nKHNx5ha5NFZ"},"execution_count":null,"outputs":[]}]}