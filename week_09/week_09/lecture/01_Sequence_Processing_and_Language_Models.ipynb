{"cells":[{"cell_type":"code","source":["import numpy as np\n","import torch"],"metadata":{"id":"5S4eHuXcjevz","executionInfo":{"status":"ok","timestamp":1708180189668,"user_tz":0,"elapsed":1628,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Sequential data\n","\n","* In the following lectures, we will study neural networks that are able to make predictions based on an entire sequence of input vectors (or images) rather than a single vector (or image).\n","\n","* We will focus on textual data, but the core ideas will generalize to other types of sequential data (such as time-series, video, and audio).\n","\n","* A text can be represented by a sequence of characters (such as letters, numbers, and symbols). A text can also be represented by a sequence of pre-defined *tokens*, each of which is a sequence of characters.\n","\n","* Neural networks currently achieve state-of-the-art performance in text classification, summarization, generation, and translation.\n"],"metadata":{"id":"8vC1zzFTe244"}},{"cell_type":"markdown","source":["# Text processing\n","\n","* We will use the book [Frankenstein; or, The Modern Prometheus](https://www.gutenberg.org/cache/epub/84/pg84.txt) by Mary Wollstonecraft Shelley to present a basic text processing pipeline.\n","\n","* This pipeline will convert a text into a sequence of numbers, each of which corresponds to a token (sequence of characters).\n","\n","* The following code downloads the book and stores it into a Python string. In order to simplify the implementation, sequences of non-letters are removed and uppercase letters are converted to lowercase letters.\n"],"metadata":{"id":"b7XFbJ0Hl0HY"}},{"cell_type":"code","source":["import requests\n","import re\n","\n","def preprocess(text):\n","    text = re.sub('[^A-Za-z]+', ' ', text) # Substitutes any sequence of non-letters by a whitespace\n","    text = text.lower() # Converts uppercase letters to lowercase letters\n","\n","    return text\n","\n","# Project Gutenberg has many books stored as text files (https://www.gutenberg.org/)\n","raw_text = requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt').text # Downloads and stores the text into a string\n","raw_text = raw_text.partition('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[2] # Removes foreword\n","raw_text = raw_text.partition('*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***')[0] # Removes afterword\n","\n","text = preprocess(raw_text)"],"metadata":{"id":"wGC_Egelx6zF","executionInfo":{"status":"ok","timestamp":1708180190429,"user_tz":0,"elapsed":766,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(text[:150]) # Prints the first 150 characters"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEswmYx1zBHE","executionInfo":{"status":"ok","timestamp":1708180190430,"user_tz":0,"elapsed":32,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"31ea55f7-422c-48cb-89bf-1bc231c96e3e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[" frankenstein or the modern prometheus by mary wollstonecraft godwin shelley contents letter letter letter letter chapter chapter chapter chapter chap\n"]}]},{"cell_type":"markdown","source":["* A text can be represented by a sequence of pre-defined *tokens*, each of which is a sequence of characters. Different sets of tokens can lead to different text representations.\n","\n","* For example, if each token is a word from a comprehensive list, \"the modern prometheus\" would be composed of 3 tokens (implicitly separated by whitespaces). If each token is a character, \"the modern prometheus\" would be composed of 21 tokens (including whitespaces).\n","\n","* In this context, a vocabulary assigns an index (unique numerical identifier) to each token found in a text. Such indices can be one-hot encoded and provided to neural networks.\n","\n","* In the code presented below, the function `tokenize` converts a text into a list of tokens (in this case, characters or sequences of letters) and the class `Vocabulary` implements a vocabulary.\n","\n"],"metadata":{"id":"p0t0RX8yz91K"}},{"cell_type":"code","source":["import collections\n","\n","def tokenize(text, use_chars):\n","    if use_chars: # One token for each character\n","        return list(text)\n","    else: # One token for each sequence of letters\n","        return text.split()\n","\n","class Vocabulary:\n","    def __init__(self, tokens):\n","        counter = collections.Counter(tokens)\n","        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # List of pairs where each pair is composed of a token and its frequency. Sorted according to decreasing frequency.\n","\n","        self.unknown = '?' # Represents an unknown token\n","\n","        self.id_to_token = sorted([token for token, freq in self.token_freqs]) + [self.unknown] # Maps an index to a token\n","        self.token_to_id = {token: id for id, token in enumerate(self.id_to_token)} # Maps a token to an index\n","\n","    def __len__(self):\n","        return len(self.id_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)): # If called with a single token\n","            return self.token_to_id.get(tokens, self.unknown)\n","        else: # If called with a list of tokens\n","            return [self.token_to_id.get(token, self.unknown) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)): # If called with a single index\n","            return self.id_to_token[indices]\n","        else: # If called with a list of indices\n","            return [self.id_to_token[index] for index in indices]"],"metadata":{"id":"7J0tVQqm39p2","executionInfo":{"status":"ok","timestamp":1708180190430,"user_tz":0,"elapsed":28,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["use_chars = False # Choose between assigning one token for each character or one token for each sequence of letters\n","\n","tokens = tokenize(text, use_chars=use_chars)\n","print(tokens[:10])\n","\n","vocab = Vocabulary(tokens)\n","print(vocab.id_to_token[:10])\n","\n","indices = vocab[tokens]\n","print(indices[:10])\n","\n","tokens_from_indices = vocab.to_tokens(indices)\n","print(tokens_from_indices[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpmPIM5BBFh7","executionInfo":{"status":"ok","timestamp":1708180190431,"user_tz":0,"elapsed":27,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"86917364-8527-4351-b054-eaa00b7b4af3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['frankenstein', 'or', 'the', 'modern', 'prometheus', 'by', 'mary', 'wollstonecraft', 'godwin', 'shelley']\n","['a', 'abandon', 'abandoned', 'abbey', 'abhor', 'abhorred', 'abhorrence', 'abhorrent', 'ability', 'abject']\n","[2608, 4298, 6169, 3991, 4811, 837, 3846, 6884, 2767, 5555]\n","['frankenstein', 'or', 'the', 'modern', 'prometheus', 'by', 'mary', 'wollstonecraft', 'godwin', 'shelley']\n"]}]},{"cell_type":"code","source":["print(vocab.token_freqs[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6D2bVtlEWemN","executionInfo":{"status":"ok","timestamp":1708180190432,"user_tz":0,"elapsed":21,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"b6f1e759-64b4-4d7d-f37c-e1bdd72fdd5e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 4195), ('and', 2976), ('i', 2850), ('of', 2642), ('to', 2094), ('my', 1776), ('a', 1391), ('in', 1129), ('was', 1021), ('that', 1018)]\n"]}]},{"cell_type":"markdown","source":["* Sophisticated tokenization is implemented in widely available natural language processing libraries (such as [spaCy](https://spacy.io/)). Such implementations employ language-specific tokenization rules and have pre-defined tokens extracted from large corpora (set of texts).\n","\n","* The following code tokenizes the same raw text using spaCy."],"metadata":{"id":"f2lkATyTI-Gy"}},{"cell_type":"code","source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(raw_text)"],"metadata":{"id":"p4wi3H7PHjlx","executionInfo":{"status":"ok","timestamp":1708180208171,"user_tz":0,"elapsed":17752,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["spacy_tokens = [str(token) for token in doc]\n","print(spacy_tokens[:128])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ps6eM9FCI2HE","executionInfo":{"status":"ok","timestamp":1708180208173,"user_tz":0,"elapsed":30,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"bfb9f396-128a-4a91-eb79-1d89114e0df2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', 'Frankenstein', ';', '\\r\\n\\r\\n', 'or', ',', 'the', 'Modern', 'Prometheus', '\\r\\n\\r\\n', 'by', 'Mary', 'Wollstonecraft', '(', 'Godwin', ')', 'Shelley', '\\r\\n\\r\\n\\r\\n ', 'CONTENTS', '\\r\\n\\r\\n ', 'Letter', '1', '\\r\\n ', 'Letter', '2', '\\r\\n ', 'Letter', '3', '\\r\\n ', 'Letter', '4', '\\r\\n ', 'Chapter', '1', '\\r\\n ', 'Chapter', '2', '\\r\\n ', 'Chapter', '3', '\\r\\n ', 'Chapter', '4', '\\r\\n ', 'Chapter', '5', '\\r\\n ', 'Chapter', '6', '\\r\\n ', 'Chapter', '7', '\\r\\n ', 'Chapter', '8', '\\r\\n ', 'Chapter', '9', '\\r\\n ', 'Chapter', '10', '\\r\\n ', 'Chapter', '11', '\\r\\n ', 'Chapter', '12', '\\r\\n ', 'Chapter', '13', '\\r\\n ', 'Chapter', '14', '\\r\\n ', 'Chapter', '15', '\\r\\n ', 'Chapter', '16', '\\r\\n ', 'Chapter', '17', '\\r\\n ', 'Chapter', '18', '\\r\\n ', 'Chapter', '19', '\\r\\n ', 'Chapter', '20', '\\r\\n ', 'Chapter', '21', '\\r\\n ', 'Chapter', '22', '\\r\\n ', 'Chapter', '23', '\\r\\n ', 'Chapter', '24', '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', 'Letter', '1', '\\r\\n\\r\\n', '_', 'To', 'Mrs.', 'Saville', ',', 'England', '.', '_', '\\r\\n\\r\\n\\r\\n', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17â€”.', '\\r\\n\\r\\n\\r\\n', 'You', 'will', 'rejoice', 'to']\n"]}]},{"cell_type":"markdown","source":["# Language model\n","\n","* A language model assigns a probability to each possible sequence of tokens (text).\n","\n","* For example, consider the three following texts:\n","    1. \"We are studying neural networks.\"\n","    2. \"Neural networks are studying us.\"\n","    3. \"Qwhuep zdmhxa qwerlj qwerz dsfa.\"\n","\n","* A good language model trained on a large corpora (such as the set of all webpages in English) would likely assign a relatively high probability to the first text, lower probability to the second text, and very low probability to the third text.\n","\n","* Language models can be used to generate text, which in turn can be used to solve tasks defined by so-called prompts.\n","\n","* For example, by generating text that starts with \"I have a delicious recipe for a quick and healthy dinner: \", a language model can be used to suggest such a recipe.\n","\n","* Language models also have other uses. For example, by comparing the probabilities of the texts \"to recognize speech\" and \"to wreck a nice beach\", a language model can help a speech recognition system decide between these two interpretations of a speech recording."],"metadata":{"id":"A1AQmLR9k0IL"}},{"cell_type":"markdown","source":["# Chain rule of probability\n","\n","* Consider a sequence of discrete random variables $X_1, X_2, \\ldots, X_T$ and a sequence of values $x_1, x_2, \\ldots, x_T$ such that $\\mathbb{P}(X_1 = x_1, X_2 = x_2, \\ldots, X_T = x_T) > 0$.\n","\n","* The chain rule of probability states that\n","\n","$$ \\mathbb{P}(X_1 = x_1, X_2 = x_2, \\ldots, X_T = x_T) = \\mathbb{P}(X_1 = x_1) \\prod_{t = 2}^{T} \\mathbb{P}(X_{t} = x_{t} \\mid X_{1} = x_1, \\ldots, X_{t-1} = x_{t-1} ), $$\n","\n","where the conditional probability $\\mathbb{P}(X_{t} = x_{t} \\mid X_{1} = x_1, \\ldots, X_{t-1} = x_{t-1} )$ is given by\n","\n","$$ \\mathbb{P}(X_{t} = x_{t} \\mid X_{1} = x_1, \\ldots, X_{t-1} = x_{t-1} ) = \\frac{\\mathbb{P}(X_{1} = x_1, \\ldots, X_{t} = x_{t})}{\\mathbb{P}(X_{1} = x_1, \\ldots, X_{t-1} = x_{t-1})}. $$\n","\n","* For example, if $T = 3$,\n","\n","$$ \\mathbb{P}(X_1 = x_1, X_2 = x_2, X_3 = x_3) = \\mathbb{P}(X_1 = x_1) \\mathbb{P}(X_2 = x_2 \\mid X_1 = x_1) \\mathbb{P}(X_3 = x_3 \\mid X_1 = x_1, X_2 = x_2). $$\n","\n","* In words, in order to know the probability of observing a sequence, it suffices to know the probability of observing its first element and the probability of observing each of its following elements given the previous elements.\n"],"metadata":{"id":"HP8f6GUEYas_"}},{"cell_type":"markdown","source":["# Autoregressive language model\n","\n","* Recall that a language model assigns a probability to each possible sequence of tokens (text).\n","\n","* By the chain rule of probability, for every sequence of tokens $x_1, \\ldots, x_T$, a language model only needs to assign a probability $\\mathbb{P}(X_1 = x_1)$ to $x_1$ being the first token and, for every $t > 1$, a probability $\\mathbb{P}(X_{t} = x_{t} \\mid X_1 = x_1, \\ldots, X_{t-1} = x_{t-1})$ to $x_t$ being the $t$-th token if the previous tokens are $x_{1}, \\ldots, x_{t-1}$.\n","\n","* An autoregressive language model can be used to generate text by sampling the first token $x_1$ from the distribution for $X_1$ and, for every $t > 1$, sampling the token $x_t$ from the distribution for $X_t$ given $X_1 = x_1, \\ldots, X_{t-1} = x_{t-1}$.\n","\n","* Recall that any model that receives its own predictions in order to make additional predictions is considered autoregressive.\n","\n"],"metadata":{"id":"MVzOHv6gc6xQ"}},{"cell_type":"markdown","source":["# Markov chain language models\n","\n","* A first order Markov chain can represent one of the simplest language models.\n","\n","* For every $T > 1$ and sequence of tokens $x_1, x_2, \\ldots, x_T$, this model supposes that $\\mathbb{P}(X_1 = x_1, X_2 = x_2, \\ldots, X_T = x_t) > 0$ and, for every $t > 1$,\n","\n","$$ \\mathbb{P}(X_t = x_t \\mid X_{1} = x_{1}, \\ldots, X_{t-1} = x_{t-1}) = \\mathbb{P}(X_t = x_t \\mid X_{t-1} = x_{t-1}). $$\n","\n","* In words, this model supposes that the current token allows predicting the next token without considering the previous tokens.\n","\n","* For example, if $T = 3$, a first order Markov chain language model supposes that\n","\n","$$ \\mathbb{P}(X_1 = x_1, X_2 = x_2, X_3 = x_3) = \\mathbb{P}(X_1 = x_1) \\mathbb{P}(X_2 = X_2 \\mid X_1 = x_1) \\mathbb{P}(X_3 = x_3 \\mid X_2 = x_2). $$\n","\n","* A time-homogeneous first order Markov chain language model supposes that, for every $t > 1$ and pair of tokens $x$ and $x'$,\n","\n","$$ \\mathbb{P}(X_t = x' \\mid X_{t-1} = x) = p_{x' \\mid x},$$\n","\n","where $p_{x \\mid x'}$ denotes a parameter associated to the pair of tokens $x$ and $x'$.\n","\n","* In words, this model supposes that the current token allows predicting the next token without considering how many tokens came before it.\n","\n","* For example, a time-homogeneous first order Markov chain language model supposes that\n","\n","$$\\mathbb{P}(X_3 = x' \\mid X_{2} = x) = p_{x' \\mid x} = \\mathbb{P}(X_2 = x' \\mid X_1 = x). $$\n","\n","* There are different approaches to obtain the parameters of a time-homogeneous first order Markov chain language model based on a given text. The following approach is one of the simplest.\n","\n","* For a given text, let $n_{x, x'}$ denote the number of times that the token $x$ is followed by the token $x'$, and let $n_{x} = \\sum_{x'} n_{x, x'}$ denote the number of times that token $x$ is followed by some token. Furthermore, let $p_{x' \\mid x}$ be given by\n","$$ p_{x' \\mid x} = \\frac{n_{x, x'} + \\alpha}{ n_{x} + v\\alpha}, $$\n","\n","where $v$ is the length of the vocabulary (number of distinct tokens) and $\\alpha > 0$ is a small constant that avoids division by zero.\n","\n","* A time-homogeneous Markov chain language model can be used to generate text by receiving the first token $x_1$ and, for every $t > 1$, sampling the token $x_t$ from the distribution for $X_t$ given $X_{t-1} = x_{t-1}$.\n","\n","* An $n$-gram is a sequence of $n$ tokens. An $n$-th order Markov chain language model supposes that the current $n$-gram allows predicting the next token without considering the tokens that preceded it.\n","\n","* Note that there are $v^n$ possible n-grams, where $v$ is the length of the vocabulary (number of distinct tokens). Therefore, most possible $n$-grams will not appear in a text if $n$ is relatively large, even if $v$ is small.\n"],"metadata":{"id":"WA1-67-6kVu-"}},{"cell_type":"code","source":["n = 2\n","\n","distinct_tokens = len(vocab)\n","print(f'Distinct tokens in the text: {distinct_tokens}.')\n","\n","possible_n_grams = distinct_tokens**n\n","print(f'Possible {n}-grams: {possible_n_grams}.')\n","\n","existing_n_grams = len(tokens) - n + 1\n","print(f'Non-distinct {n}-grams in the text: {existing_n_grams}.')\n","\n","if possible_n_grams > existing_n_grams:\n","    fraction = 1 - existing_n_grams / possible_n_grams\n","    print(f'At least {fraction * 100}% of the possible {n}-grams are not in the text.') # Certainly more if an `n`-gram appears more than once."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAacVuyIrRFV","executionInfo":{"status":"ok","timestamp":1708180208174,"user_tz":0,"elapsed":21,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"78c4d14b-dd6a-4097-e038-7751c8578ae8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Distinct tokens in the text: 6978.\n","Possible 2-grams: 48692484.\n","Non-distinct 2-grams in the text: 75327.\n","At least 99.84530056014394% of the possible 2-grams are not in the text.\n"]}]},{"cell_type":"markdown","source":["## Implementation\n","\n","* The following code implements an $n$-th order time-homogeneous Markov chain language model based on the processed text.\n","\n","* A hyperparameter called temperature is used to control the sampling process. Lowering the temperature causes less probable tokens to be sampled even less frequently, and raising the temperature causes less probable tokens to be sampled more frequently."],"metadata":{"id":"3xWCOHlWYge6"}},{"cell_type":"code","source":["class MarkovModel:\n","    def __init__(self, text, order, use_chars, alpha=None):\n","        self.text = text\n","        self.order = order\n","        self.use_chars = use_chars\n","\n","        self.tokens = tokenize(text, self.use_chars)\n","        self.vocab = Vocabulary(self.tokens)\n","        self.indices = np.array(self.vocab[self.tokens])\n","\n","        self.alpha = alpha\n","        if self.alpha is None:\n","            self.alpha = 1. / len(self.vocab)\n","\n","        self.counts = {} # Stores the number of times that each ngram is followed by each token\n","        for i in range(len(self.indices) - order):\n","            ngram = tuple(self.indices[i: i + order])\n","            index = self.indices[i + order]\n","\n","            self.counts[(ngram, index)] = self.counts.get((ngram, index), 0) + 1\n","\n","        self.ngram_token_freqs = sorted(self.counts.items(), key=lambda x: x[1], reverse=True)\n","\n","    def pseudocounts(self, ngram):\n","        return np.array([self.counts.get((ngram, index), 0) + self.alpha for index in range(len(self.vocab))]) # An array with the number of times that `ngram` was followed by each token, plus `self.alpha`\n","\n","    def generate(self, start_text, n_tokens, temperature):\n","        tokens = tokenize(start_text, self.use_chars)\n","\n","        if len(tokens) < self.order:\n","            raise Exception(f'You must provide at least {self.order} tokens to generate text')\n","\n","        indices = self.vocab[tokens]\n","        for _ in range(n_tokens):\n","            ngram = tuple(indices[-self.order:])\n","\n","            pseudocounts = self.pseudocounts(ngram)\n","\n","            # The following two lines change the probability of sampling tokens based on the `temperature`\n","            pseudocounts = pseudocounts / np.max(pseudocounts)\n","            pseudocounts = np.exp(pseudocounts / temperature)\n","\n","            p = pseudocounts / np.sum(pseudocounts)\n","\n","            token = np.random.choice(len(self.vocab), p=p)\n","\n","            indices.append(token)\n","\n","        tokens = self.vocab.to_tokens(indices)\n","\n","        return ''.join(tokens) if self.use_chars else ' '.join(tokens)\n","\n","mm = MarkovModel(text=text, order=4, use_chars=False) # Choose the order and whether to assign one token for each character or one token for each sequence of letters\n","\n","print(f'Most frequent pairs of {mm.order}-gram and token:')\n","for (ngram, token), freq in mm.ngram_token_freqs[:10]:\n","    print(f'{tuple(mm.vocab.to_tokens(ngram))} -> {mm.vocab.to_tokens(token)} ({freq}).')\n","\n","print('\\nGenerated text: ')\n","mm.generate('i cannot describe to', n_tokens=150, temperature=0.01)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"IQCx9YXD0rPS","executionInfo":{"status":"ok","timestamp":1708180208900,"user_tz":0,"elapsed":738,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"24df7169-5b56-4c2c-9308-d67103f42aa5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Most frequent pairs of 4-gram and token:\n","('chapter', 'chapter', 'chapter', 'chapter') -> chapter (20).\n","('letter', 'to', 'mrs', 'saville') -> england (4).\n","('be', 'with', 'you', 'on') -> your (4).\n","('with', 'you', 'on', 'your') -> wedding (4).\n","('you', 'on', 'your', 'wedding') -> night (4).\n","('i', 'cannot', 'describe', 'to') -> you (3).\n","('for', 'my', 'own', 'part') -> i (3).\n","('for', 'a', 'long', 'time') -> i (3).\n","('was', 'to', 'convey', 'me') -> away (3).\n","('monster', 'whom', 'i', 'had') -> created (3).\n","\n","Generated text: \n"]},{"output_type":"execute_result","data":{"text/plain":["'i cannot describe to you my sensations on the near prospect of my undertaking it is impossible to communicate to you a conception of the trembling sensation half pleasurable and half fearful with which i am preparing to depart i am going to unexplored regions to the land of mist and snow but i shall kill no albatross therefore do not be alarmed for my safety or if i should come back to you as worn and woeful as the ancient mariner you will smile at my allusion but i will disclose a secret i have often attributed my attachment to my passionate enthusiasm for the dangerous mysteries of ocean to that production of the most imaginative of modern poets there is something at work in my soul which i do not understand i am practically industrious painstaking a workman to execute with perseverance and labour but besides this there is a love for'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# Neural language models\n","\n","* Consider the following two texts:\n","    1. \"The banana is ripe.\"\n","    2. \"The strawberry is ripe.\"\n","\n","* A good language model trained on a large corpora should be able infer that interchanging \"bananas\" for \"strawberries\" often results in a meaningful sentence, since both words refer to edible fruits.\n","\n","* Therefore, a good language model could assign a high probability to the second text even if it was not part of the large training corpora.\n","\n","* Markov chain language models do not enable such generalization.\n","\n","* More powerful language models, such as language models based on neural networks, may be able to address this issue.\n","\n"],"metadata":{"id":"cO84imOUbBD3"}},{"cell_type":"markdown","source":["# Classification for next token prediction\n","\n","* Let $T > 1$ and consider a sequence $\\mathbf{s}_{T} = \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_{T}$, where each $\\mathbf{x}_{i} \\in \\mathbb{R}^v$ is a vector that corresponds to one-hot encoding a token $x_i$ from a vocabulary with $v$ tokens.\n","\n","* Let $\\mathbf{s}_{t} = \\mathbf{x}_1, \\ldots, \\mathbf{x}_{t}$ denote a sequence that contains the first $t < T$ elements of the sequence $\\mathbf{s}_{T}$.\n","\n","* If each $\\mathbf{s}_{t}$ is interpreted as an observation, the task of predicting $x_{t+1}$ given $\\mathbf{s}_{t}$ is a classification task.\n","\n","* In the next lectures, we will see how a neural network can make a prediction $\\hat{\\mathbf{x}}_{t+1}$ based on an entire sequence of vectors $\\mathbf{s}_t = \\mathbf{x}_1, \\ldots, \\mathbf{x}_{t}$.\n","\n","* Assuming that is possible, an appropriate loss for a single sequence $\n","\\mathbf{s}_{T}$ could be given by\n","\n","$$ - \\frac{1}{T - 1} \\sum_{t = 2}^T \\sum_{k = 1}^v x_{t, k} \\log \\hat{x}_{t, k}, $$\n","\n","which corresponds to averaging the cross entropy loss for each of the $T-1$ predictions.\n"],"metadata":{"id":"3i1guc6YcXAB"}},{"cell_type":"markdown","source":["# Partitioning and batching\n","\n","* If the purpose of a language model is to generate entire books, each sequence $\\mathbf{s}_T$ in a training dataset may correspond to an entire book.\n","\n","* If the purpose of a language model is to generate passages from books, each sequence $\\mathbf{s}_T$ in the training dataset may correspond to a passage from a book.\n","\n","* Therefore, there are different ways to organize a corpora into a training dataset for the purpose of training a language model.\n","\n","* In what follows, we will assume that every text in a given corpora has been concatenated into a single text, which was then processed into a single sequence of token indices $a = a_1, a_2, \\ldots, a_L$.\n","\n","* We will cover two methods for organizing the sequence $a$ into a training dataset: **random partitioning** and **sequential partitioning**.\n","\n","* For a pre-defined batch size $B$ and a pre-defined number of steps $T$, the sequence $a$ will be partitioned into subsequences of length $T$, which will be grouped into batches of size $B$.\n","\n","* More concretely, each epoch of these methods will produce a sequence of pairs $(\\mathbf{X}_1, \\mathbf{Y}_1), (\\mathbf{X}_2, \\mathbf{Y}_2), \\ldots, (\\mathbf{X}_{n}, \\mathbf{Y}_n)$, where $\\mathbf{X}_j$ and $\\mathbf{Y}_j$ are $B \\times T$ matrices.\n","\n","* The $i$-th row of the matrix $\\mathbf{X}_j$ will contain a subsequence $a_{k + 1}, a_{k + 2}, \\ldots, a_{k + T}$ with $T$ elements from the sequence $a$, where $k$ is an index that depends on $i$, $j$, and the partitioning method. In this case, the $i$-th row of the matrix $\\mathbf{Y}_j$ would contain the subsequence $a_{k+2}, a_{k + 3}, \\ldots, a_{k + T + 1}$.\n","\n","* In words, for the purpose of training a language model, the first $t$ elements of each row of the matrix $\\mathbf{X}_j$ will be used to predict the $t$-th element of the corresponding row of the matrix $\\mathbf{Y}_j$.\n","\n","* There are two reasons for partitioning and batching the original sequence $a$ in this way:\n","    * Training the neural networks that we will study in the following lectures using long sequences will be expensive in comparison with computing predictions using long sequences.\n","    * For the same reasons that previous implementations made predictions for batches of vectors (rather than single vectors), the vectorized implementations of these neural networks make predictions for entire batches of sequences (rather than single sequences)."],"metadata":{"id":"nDLSpa6YrNch"}},{"cell_type":"markdown","source":["## Random partitioning\n","\n","* The following code implements random partitioning using a Python generator."],"metadata":{"id":"cTYc1LX9jYVB"}},{"cell_type":"code","source":["def random_partitioning(sequence, batch_size, num_steps, offset=None):\n","    if offset is None:\n","        offset = np.random.randint(num_steps) # A random number between 0 and `num_steps` (excluding `num_steps`)\n","\n","    sequence = sequence[offset:] # Discards the first `offset` elements of the sequence\n","\n","    num_subseqs = (len(sequence) - 1) // num_steps # The number of subsequences of length `num_steps` that fit into the `sequence`\n","\n","    subseq_indices = list(range(0, num_subseqs * num_steps, num_steps)) # A list with the index where each subsequence starts: [0, `num_steps`, 2 * `num_steps`, ...]\n","\n","    np.random.shuffle(subseq_indices) # Randomizes the indices to enable creating batches composed of randomly chosen subsequences\n","\n","    num_batches = num_subseqs // batch_size # The number of batches required to fit every subsequences\n","\n","    for i in range(0, num_batches * batch_size, batch_size):\n","        batch_indices = subseq_indices[i: i + batch_size] # The index where each subsequence that should be part of this batch starts\n","\n","        X = [sequence[j: j + num_steps] for j in batch_indices] # Organizes each subsequence into a row\n","        Y = [sequence[j + 1: j + 1 + num_steps] for j in batch_indices] # Organizes each subsequence (shifted by one index) into a row\n","\n","        yield torch.tensor(X), torch.tensor(Y) # Yields a pair of matrices\n","\n","\n","sequence = list(range(31))\n","print(f'Partitioning and batching the sequence {sequence}:')\n","for i, (X, Y) in enumerate(random_partitioning(sequence, batch_size=3, num_steps=5, offset=0)):\n","    print(f'X_{i + 1}: \\n{X}.')\n","    print(f'Y_{i + 1}: \\n{Y}.\\n')\n","    print()\n","\n","\n","print('Partitioning and batching the sequence of token indices (first two batches, converted into batches of tokens):')\n","for i, (X, Y) in enumerate(random_partitioning(indices, batch_size=3, num_steps=5)):\n","    X_tokens = np.array([vocab.to_tokens(list(X[j])) for j in range(X.shape[0])])\n","    Y_tokens = np.array([vocab.to_tokens(list(Y[j])) for j in range(Y.shape[0])])\n","    print(f'X_{1}: \\n{X_tokens}.')\n","    print(f'Y_{1}: \\n{Y_tokens}.\\n')\n","    print()\n","\n","    if i >= 1:\n","        break"],"metadata":{"id":"tvBlCYdmQefr","executionInfo":{"status":"ok","timestamp":1708180208901,"user_tz":0,"elapsed":36,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95d13e34-f1e6-4a63-bf84-8016d02cfbd9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Partitioning and batching the sequence [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]:\n","X_1: \n","tensor([[25, 26, 27, 28, 29],\n","        [ 5,  6,  7,  8,  9],\n","        [20, 21, 22, 23, 24]]).\n","Y_1: \n","tensor([[26, 27, 28, 29, 30],\n","        [ 6,  7,  8,  9, 10],\n","        [21, 22, 23, 24, 25]]).\n","\n","\n","X_2: \n","tensor([[ 0,  1,  2,  3,  4],\n","        [15, 16, 17, 18, 19],\n","        [10, 11, 12, 13, 14]]).\n","Y_2: \n","tensor([[ 1,  2,  3,  4,  5],\n","        [16, 17, 18, 19, 20],\n","        [11, 12, 13, 14, 15]]).\n","\n","\n","Partitioning and batching the sequence of token indices (first two batches, converted into batches of tokens):\n","X_1: \n","[['but' 'for' 'one' 'event' 'and']\n"," ['best' 'assistants' 'to' 'my' 'plan']\n"," ['pentland' 'hills' 'compensated' 'him' 'for']].\n","Y_1: \n","[['for' 'one' 'event' 'and' 'then']\n"," ['assistants' 'to' 'my' 'plan' 'may']\n"," ['hills' 'compensated' 'him' 'for' 'the']].\n","\n","\n","X_1: \n","[['her' 'face' 'were' 'hard' 'and']\n"," ['he' 'ah' 'i' 'wish' 'you']\n"," ['occurred' 'to' 'me' 'which' 'led']].\n","Y_1: \n","[['face' 'were' 'hard' 'and' 'rude']\n"," ['ah' 'i' 'wish' 'you' 'had']\n"," ['to' 'me' 'which' 'led' 'me']].\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ufUjM7LndwZF"},"source":["## Sequential Partitioning\n","\n","* The following code implements sequential partitioning using a Python generator.\n","\n","* Most notably, if the $i$-th row of the matrix $X_{j}$ contains a subsequence $a_{k + 1}, a_{k + 2}, \\ldots, a_{k + T}$ with $T$ elements from the sequence $a$, the $i$-th row of the matrix $X_{j+1}$ will contain the adjacent subsequence  $a_{k + 1 + T}, a_{k + 2 + T}, \\ldots, a_{k + 2T}$."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"jBt6X5rAdwZG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708180208902,"user_tz":0,"elapsed":24,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"7618aa16-b249-4e48-90e9-70184ee39369"},"outputs":[{"output_type":"stream","name":"stdout","text":["Partitioning and batching the sequence [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]:\n","X_1: \n","tensor([[ 0,  1,  2,  3,  4],\n","        [10, 11, 12, 13, 14],\n","        [20, 21, 22, 23, 24]]).\n","Y_1: \n","tensor([[ 1,  2,  3,  4,  5],\n","        [11, 12, 13, 14, 15],\n","        [21, 22, 23, 24, 25]]).\n","\n","\n","X_2: \n","tensor([[ 5,  6,  7,  8,  9],\n","        [15, 16, 17, 18, 19],\n","        [25, 26, 27, 28, 29]]).\n","Y_2: \n","tensor([[ 6,  7,  8,  9, 10],\n","        [16, 17, 18, 19, 20],\n","        [26, 27, 28, 29, 30]]).\n","\n","\n","Partitioning and batching the sequence of token indices (first two batches, converted into batches of tokens):\n","X_1: \n","[['the' 'modern' 'prometheus' 'by' 'mary']\n"," ['i' 'had' 'before' 'experienced' 'sensations']\n"," ['arrived' 'at' 'mainz' 'the' 'course']].\n","Y_1: \n","[['modern' 'prometheus' 'by' 'mary' 'wollstonecraft']\n"," ['had' 'before' 'experienced' 'sensations' 'of']\n"," ['at' 'mainz' 'the' 'course' 'of']].\n","\n","\n","X_2: \n","[['wollstonecraft' 'godwin' 'shelley' 'contents' 'letter']\n"," ['of' 'horror' 'and' 'i' 'have']\n"," ['of' 'the' 'rhine' 'below' 'mainz']].\n","Y_2: \n","[['godwin' 'shelley' 'contents' 'letter' 'letter']\n"," ['horror' 'and' 'i' 'have' 'endeavoured']\n"," ['the' 'rhine' 'below' 'mainz' 'becomes']].\n","\n","\n"]}],"source":["def sequential_partitioning(sequence, batch_size, num_steps, offset=None):\n","    if offset is None:\n","        offset = np.random.randint(num_steps) # A random number between 0 and `num_steps` (excluding `num_steps`)\n","\n","    sequence = sequence[offset:] # Discards the first `offset` elements of the sequence\n","\n","    len_macro_subseq = (len(sequence) - 1) // batch_size # The length of each macro-subsequence. There is one macro-subsequence for each unit of batch size\n","\n","    num_elements = len_macro_subseq * batch_size # The number of elements that fit into the macro-subsequences\n","\n","    Xs = torch.tensor(sequence[: num_elements]) # Selects the elements that fit into the macro-subsequences\n","    Ys = torch.tensor(sequence[1: num_elements + 1]) # Select the elements that fit into the macro-subsequences (shifted by one index)\n","\n","    Xs = Xs.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence\n","    Ys = Ys.reshape(batch_size, -1) # Each row of this matrix corresponds to a macro-subsequence (shifted by one index)\n","\n","    num_subseqs = Xs.shape[1] // num_steps # The number of subsequences that fit into each macro-subsequence\n","\n","    for i in range(0, num_subseqs * num_steps, num_steps):\n","        X = Xs[:, i: i + num_steps] # Each row of `X` contains a subsequence from a distinct macro-subsequence\n","        Y = Ys[:, i: i + num_steps] # Each row of `Y` contains a subsequence from a distinct macro-subsequence (shifted by one index)\n","\n","        yield X, Y # Yields a pair of matrices\n","\n","\n","sequence = list(range(31))\n","print(f'Partitioning and batching the sequence {sequence}:')\n","for i, (X, Y) in enumerate(sequential_partitioning(sequence, batch_size=3, num_steps=5, offset=0)):\n","    print(f'X_{i + 1}: \\n{X}.')\n","    print(f'Y_{i + 1}: \\n{Y}.\\n')\n","    print()\n","\n","\n","print('Partitioning and batching the sequence of token indices (first two batches, converted into batches of tokens):')\n","for i, (X, Y) in enumerate(sequential_partitioning(indices, batch_size=3, num_steps=5)):\n","    X_tokens = np.array([vocab.to_tokens(list(X[j])) for j in range(X.shape[0])])\n","    Y_tokens = np.array([vocab.to_tokens(list(Y[j])) for j in range(Y.shape[0])])\n","    print(f'X_{i + 1}: \\n{X_tokens}.')\n","    print(f'Y_{i + 1}: \\n{Y_tokens}.\\n')\n","    print()\n","\n","    if i >= 1:\n","        break"]},{"cell_type":"markdown","source":["# Recommended reading\n","\n","* [Dive Into Deep Learning](https://d2l.ai/index.html): Chapters 9.1, 9.2, and 9.3."],"metadata":{"id":"NLQ6FaIYsM1E"}},{"cell_type":"markdown","source":["# [Storing this notebook as a `pdf`]"],"metadata":{"id":"4Jny9SyhsPQl"}},{"cell_type":"code","source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n","\n","# Set the path to this notebook below (add \\ before spaces). The output `pdf` will be stored in the corresponding folder.\n","!jupyter nbconvert --to pdf /content/gdrive/My\\ Drive/Colab\\ Notebooks/nndl/week_09/lecture/01_Sequence_Processing_and_Language_Models.ipynb\n","\n","# If having issues, save this notebook (File > Save) and restart the session (Runtime > Restart session) before running this cell. To debug, remove the first line (`%%capture`)."],"metadata":{"id":"jEGgbKRXsP_W","executionInfo":{"status":"ok","timestamp":1708180217692,"user_tz":0,"elapsed":8804,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":13,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}