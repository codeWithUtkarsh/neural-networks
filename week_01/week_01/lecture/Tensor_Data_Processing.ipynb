{"cells":[{"cell_type":"markdown","metadata":{"id":"u528q8RCbQh6"},"source":["# Data Manipulation\n","\n","* The basic data structure used in deep learning is the $n$-dimensional array, which is also called a *tensor*.\n","    * A rank 0 tensor corresponds to a *number*.\n","    * A rank 1 tensor corresponds to a *vector*.\n","    * A rank 2 tensor corresponds to a *matrix*.\n","    * Tensors of higher rank do not have special names.\n","* The class `Tensor` in PyTorch is similar to the class `ndarray` in Numpy. However, it also enables GPU acceleration and automatic differentiation.\n","* Together, these properties make the `Tensor` class very useful for deep learning.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"LKSF3PUXbQh7","scrolled":true,"executionInfo":{"status":"ok","timestamp":1705395865776,"user_tz":0,"elapsed":5362,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[],"source":["# First, we import `torch` (the library is called PyTorch, but it is imported as `torch`)\n","import torch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0dRE9OyLbQh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865777,"user_tz":0,"elapsed":32,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"8a5f92d4-8981-4e1d-f20e-c38b14031800"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu121\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"Oy1peA3MbQh9"},"source":["# Vector\n","\n","* We will denote vectors by boldface lower-case letters (such as $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z})$.\n","* Operations between matrices and vectors will behave as if the vector were a column matrix:\n","\n","$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n","where $x_1, \\ldots, x_n$ are elements of the vector."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kxAdxaKcbQh9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865777,"user_tz":0,"elapsed":29,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"a1d9b212-059b-4e9f-e6a3-d245c9cb0af9"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","tensor([0, 1, 2, 3])\n"]}],"source":["# A vector with 4 integers in the range [0,3]\n","# Unless otherwise specified, a new tensor is stored in main memory, enabling CPU-based computation\n","x = torch.arange(4)\n","print(type(x))\n","print(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JeQa40RabQh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865778,"user_tz":0,"elapsed":25,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"dbb207c9-7d8e-4b1f-cc02-7b0a0403ec62"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(3)\n"]}],"source":["# Acessing the i-th element: x[i]. Indices start at zero.\n","print(x[3])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZI2FuIqebQh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865778,"user_tz":0,"elapsed":20,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"da972e64-fc2b-4fc3-9c37-4c8d4776b6c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["4\n","torch.Size([4])\n","torch.Size([4])\n","<class 'torch.Size'>\n"]}],"source":["# Vector shape (dimensionality)\n","print(len(x))\n","print(x.size())\n","print(x.shape)\n","print(type(x.size()))"]},{"cell_type":"markdown","metadata":{"id":"TUb8DVCUbQh_"},"source":["# Matrices\n","\n","* We will denote matrices by boldface capital letters (such as $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$).\n","\n","* We will let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ denote that matrix $\\mathbf{A}$ is composed of $m$ rows and $n$ columns of real numbers.\n","\n","* We can represent a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ by a table:\n","\n","$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix},$$\n","\n","where the element in the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column is given by $a_{ij}$.\n","\n","* For any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the *shape* of $\\mathbf{A}$ is $(m, n)$.\n","\n","* A matrix is called a *square matrix* if the number of rows is the same as the number of columns.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8U9keq5sbQh_","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865778,"user_tz":0,"elapsed":17,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"60be1a8e-1daa-4c44-eb84-63fd3479e100"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19])\n","tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11],\n","        [12, 13, 14, 15],\n","        [16, 17, 18, 19]])\n","tensor(11)\n","tensor(11)\n"]}],"source":["# Reshape function: changes the shape of a tensor without changing the number of elements or their values\n","A = torch.arange(20)\n","print(A)\n","B = A.reshape(5, 4)\n","print(B)\n","print(B[2, 3])\n","print(B[2][3])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"olCGMQPf4Nae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865778,"user_tz":0,"elapsed":15,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"bcbedead-0f87-4bf5-b678-a2917fc4df21"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19])\n","tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11],\n","        [12, 13, 14, 15],\n","        [16, 17, 18, 19]])\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19])\n"]}],"source":["# Reshaping convention\n","A = torch.arange(20)\n","print(A)\n","\n","# From vector to matrix\n","c = 0\n","B = torch.zeros((5, 4), dtype=int)\n","for i in range(5):\n","    for j in range(4):\n","        B[i, j] = A[c]\n","        c += 1\n","print(B)\n","\n","# From matrix to vector\n","c = 0\n","C = torch.zeros(20, dtype=int)\n","for i in range(5):\n","    for j in range(4):\n","        C[c] = B[i, j]\n","        c += 1\n","print(C)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HMIwWi0ZbQh_","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395865779,"user_tz":0,"elapsed":12,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"e829f04f-6d22-4fbf-bdbf-9298726a12e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","torch.Size([5, 4])\n","torch.Size([5, 4])\n"]}],"source":["# Matrix shape\n","print(len(B))\n","print(B.size())\n","print(B.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"DfnqT2DJbQiA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395866059,"user_tz":0,"elapsed":288,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"8da4746e-24e9-46b8-dc9f-791811d5e8b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True]])\n"]}],"source":["# Use -1 for a dimension that can be automatically inferred\n","A1 = torch.arange(20).reshape(5, -1);\n","A2 = torch.arange(20).reshape(-1, 4);\n","print(A1 == A2)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"gq8STO4VbQiA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395866059,"user_tz":0,"elapsed":34,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"db099b22-60dc-4151-d09c-5e398dcc8668"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  4,  8, 12, 16],\n","        [ 1,  5,  9, 13, 17],\n","        [ 2,  6, 10, 14, 18],\n","        [ 3,  7, 11, 15, 19]])\n","tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11],\n","        [12, 13, 14, 15],\n","        [16, 17, 18, 19]])\n"]}],"source":["# Transpose\n","B1 = B.T\n","print(B1)\n","B2 = B1.transpose(1, 0)\n","print(B2)"]},{"cell_type":"markdown","metadata":{"id":"1S2vzvhybQiB"},"source":["# Tensors"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IBVTYkptbQiB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395866060,"user_tz":0,"elapsed":26,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"45b18ca6-07c1-4dac-fbfd-a46a4955cc42"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15],\n","         [16, 17, 18, 19],\n","         [20, 21, 22, 23]]])\n"]}],"source":["# A rank 3 tensor\n","X = torch.arange(24).reshape(2, 3, -1)\n","print(X)"]},{"cell_type":"markdown","metadata":{"id":"yGz3gmpXbQiB"},"source":["# Commonly-used Tensor Constructors\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"_dhFpD5sbQiB","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395866060,"user_tz":0,"elapsed":16,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"dce03d05-feae-4622-9710-818aa33ffcd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]],\n","\n","        [[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]])\n"]}],"source":["print(torch.ones((2, 3, 4))) # Initialize with ones"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"EEi7T-_qbQiC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395866622,"user_tz":0,"elapsed":572,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"0d8513fb-6456-4e51-faab-c0fc3e3fa683"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["print(torch.zeros(2, 3)) # Initialize with zeros"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tof2tVtGbQiC","scrolled":true,"outputId":"9433ae5f-5f5d-4040-d756-ac9bb5d90e88","executionInfo":{"status":"ok","timestamp":1705395866622,"user_tz":0,"elapsed":52,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.5730,  0.0458, -1.0057, -0.2085],\n","        [ 0.3169, -0.1660, -0.0406,  0.0657],\n","        [ 0.0972, -0.1748, -1.0358, -0.7907]])\n"]}],"source":["print(torch.randn(3, 4)) # Initialize with samples from a Gaussian distribution with mean 0 and standard deviation 1"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"kUy5p6A6bQiC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705395866622,"user_tz":0,"elapsed":48,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"30d290d7-f3c0-4517-a121-0719241e60e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2, 1, 4, 3],\n","        [1, 2, 3, 4]])\n"]}],"source":["print(torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4]])) # Initialize from Python lists"]},{"cell_type":"markdown","metadata":{"id":"LFZreKf2bQiD"},"source":["# Common Tensor Operators"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp_a4tGObQiD","scrolled":true,"outputId":"b6729703-5f83-4e18-e0e0-494cb78e07e3","executionInfo":{"status":"ok","timestamp":1705395866623,"user_tz":0,"elapsed":41,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([[ 0.,  2.,  4.,  6.],\n","        [ 8., 10., 12., 14.],\n","        [16., 18., 20., 22.],\n","        [24., 26., 28., 30.],\n","        [32., 34., 36., 38.]])\n","tensor([[  0.,   1.,   4.,   9.],\n","        [ 16.,  25.,  36.,  49.],\n","        [ 64.,  81., 100., 121.],\n","        [144., 169., 196., 225.],\n","        [256., 289., 324., 361.]])\n","tensor([[ 2.,  3.,  4.,  5.],\n","        [ 6.,  7.,  8.,  9.],\n","        [10., 11., 12., 13.],\n","        [14., 15., 16., 17.],\n","        [18., 19., 20., 21.]])\n"]}],"source":["A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n","print(A)\n","print(A + B)\n","print(A * B)\n","print(A + 2)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_8TN5WKbQiD","outputId":"865fd53a-0786-4e91-d630-36a61deea0e4","executionInfo":{"status":"ok","timestamp":1705395866623,"user_tz":0,"elapsed":33,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(190.)\n","tensor([40., 45., 50., 55.])\n","tensor([ 6., 22., 38., 54., 70.])\n"]}],"source":["# Summations (analogously for A.mean())\n","print(A.sum())\n","print(A.sum(dim=0))\n","print(A.sum(dim=1))"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9aakDknbQiE","scrolled":true,"outputId":"a522170b-276d-4ee5-c3fd-b252ec66894d","executionInfo":{"status":"ok","timestamp":1705395866623,"user_tz":0,"elapsed":26,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01],\n","        [5.4598e+01, 1.4841e+02, 4.0343e+02, 1.0966e+03],\n","        [2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04],\n","        [1.6275e+05, 4.4241e+05, 1.2026e+06, 3.2690e+06],\n","        [8.8861e+06, 2.4155e+07, 6.5660e+07, 1.7848e+08]])\n","tensor([[  0.,   1.,   4.,   9.],\n","        [ 16.,  25.,  36.,  49.],\n","        [ 64.,  81., 100., 121.],\n","        [144., 169., 196., 225.],\n","        [256., 289., 324., 361.]])\n","tensor([[  0.,   1.,   4.,   9.],\n","        [ 16.,  25.,  36.,  49.],\n","        [ 64.,  81., 100., 121.],\n","        [144., 169., 196., 225.],\n","        [256., 289., 324., 361.]])\n","tensor([[ 1.0000,  0.5403, -0.4161, -0.9900],\n","        [-0.6536,  0.2837,  0.9602,  0.7539],\n","        [-0.1455, -0.9111, -0.8391,  0.0044],\n","        [ 0.8439,  0.9074,  0.1367, -0.7597],\n","        [-0.9577, -0.2752,  0.6603,  0.9887]])\n"]}],"source":["# Functions are applied elementwise\n","print(torch.exp(A))\n","print(A**2)\n","print(torch.pow(A, 2))\n","print(torch.cos(A))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AFwrFy6YbQiE","scrolled":true,"outputId":"f4503d3a-4903-4715-e01e-c0c3e1d81969","executionInfo":{"status":"ok","timestamp":1705395866623,"user_tz":0,"elapsed":21,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.],\n","        [ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([[ 0.,  1.,  2.,  3.,  0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.,  4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.,  8.,  9., 10., 11.],\n","        [12., 13., 14., 15., 12., 13., 14., 15.],\n","        [16., 17., 18., 19., 16., 17., 18., 19.]])\n"]}],"source":["# Concatenation\n","print(torch.cat((A, B), dim=0))\n","print(torch.cat((A, B), dim=1))"]},{"cell_type":"markdown","metadata":{"id":"MQvDz7TqbQiE"},"source":["# Dot Product\n","\n","* Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, we will let $\\mathbf{x}^T \\mathbf{y}$ denote their *dot product*. The dot product is the sum of the products of corresponding elements: $$\\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i = x_1 y_1 + x_2 y_2 + \\ldots + x_d y_d$$."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VMPW6RmbQiE","outputId":"6f3724c9-e09d-4873-daa2-0b3c14c9bec6","executionInfo":{"status":"ok","timestamp":1705395866623,"user_tz":0,"elapsed":18,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 1., 2., 3.])\n","tensor([1., 1., 1., 1.])\n","tensor(6.)\n"]}],"source":["x = torch.arange(4, dtype=torch.float32)\n","y = torch.ones(4, dtype=torch.float32)\n","print(x)\n","print(y)\n","print(x.dot(y))"]},{"cell_type":"markdown","metadata":{"id":"qlrjbcofbQiF"},"source":["* Dot products are useful in many different ways.\n","* Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, if the elements of $\\mathbf{y}$ are non-negative and sum to one (that is, $\\sum_{i = 1}^d y_i = y_1 + y_2 + \\ldots + y_d =  1$), then the dot product $\\mathbf{x}^T \\mathbf{y}$ is the weighted average of the elements in $\\mathbf{x}$ by the elements in $\\mathbf{y}$\n","* Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ of *length* one (that is, $\\| \\mathbf{x} \\|_2 = \\| \\mathbf{y} \\|_2 = 1$, as detailed below), the dot product $\\mathbf{x}^T \\mathbf{y}$ is the cosine of the angle between them.\n"]},{"cell_type":"markdown","metadata":{"id":"WWkx1OVQbQiF"},"source":["# Matrix-Vector Multiplication\n","\n","* Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{x} \\in \\mathbb{R}^n$. Let us write $\\mathbf{A}$ in terms of its *row vectors*:\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^T_{1} \\\\\n","\\mathbf{a}^T_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^T_m \\\\\n","\\end{bmatrix},$$\n","where each $\\mathbf{a}^T_{i} \\in \\mathbb{R}^n$ is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$.\n","\n","* $\\mathbf{A}\\mathbf{x}$ is the column vector of length $m$ whose $i^\\mathrm{th}$ element is $\\mathbf{a}^T_i \\mathbf{x}$:\n","\n","$$\n","\\mathbf{A}\\mathbf{x}\n","= \\begin{bmatrix}\n","\\mathbf{a}^T_{1} \\\\\n","\\mathbf{a}^T_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^T_m \\\\\n","\\end{bmatrix}\\mathbf{x}\n","= \\begin{bmatrix}\n"," \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n"," \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n","\\vdots\\\\\n"," \\mathbf{a}^T_{m} \\mathbf{x}\\\\\n","\\end{bmatrix}.\n","$$\n","\n","* Multiplication by $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ maps vectors from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlHXImeIbQiF","outputId":"4add446f-fa80-4782-9237-28c8ab2945b8","executionInfo":{"status":"ok","timestamp":1705395866624,"user_tz":0,"elapsed":16,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([0., 1., 2., 3.])\n","tensor([ 14.,  38.,  62.,  86., 110.])\n"]}],"source":["# Matrix-vector multiplication\n","print(A)\n","print(x)\n","print(torch.mv(A, x))"]},{"cell_type":"markdown","metadata":{"id":"I4WilX3FbQiF"},"source":["# Matrix Multiplication\n","\n","\n","* Consider two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n","\\end{bmatrix}.$$\n","\n","\n","* Denote by $\\mathbf{a}^T_{i} \\in \\mathbb{R}^k$ the *row vector* corresponding to the $i^\\mathrm{th}$ row of $\\mathbf{A}$, and by $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ the *column vector* corresponding to the $j^\\mathrm{th}$ column of $\\mathbf{B}$, so that\n","\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^T_{1} \\\\\n","\\mathbf{a}^T_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^T_n \\\\\n","\\end{bmatrix},\n","\\quad \\mathbf{B}=\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}.\n","$$\n","\n","\n","* $\\mathbf{AB} \\in \\mathbb{R}^{n \\times m}$ is a matrix given by:\n","\n","$$\\mathbf{AB} = \\begin{bmatrix}\n","\\mathbf{a}^T_{1} \\\\\n","\\mathbf{a}^T_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^T_n \\\\\n","\\end{bmatrix}\n","\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}\n","= \\begin{bmatrix}\n","\\mathbf{a}^T_{1} \\mathbf{b}_1 & \\mathbf{a}^T_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^T_{1} \\mathbf{b}_m \\\\\n"," \\mathbf{a}^T_{2}\\mathbf{b}_1 & \\mathbf{a}^T_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\mathbf{b}_m \\\\\n"," \\vdots & \\vdots & \\ddots &\\vdots\\\\\n","\\mathbf{a}^T_{n} \\mathbf{b}_1 & \\mathbf{a}^T_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\mathbf{b}_m\n","\\end{bmatrix}.\n","$$\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yft9jH64bQiG","outputId":"5fcf2f68-28f9-4d43-ad93-e10db822a093","executionInfo":{"status":"ok","timestamp":1705395866624,"user_tz":0,"elapsed":12,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.]])\n","tensor([[ 42.,  48.,  54.],\n","        [114., 136., 158.],\n","        [186., 224., 262.],\n","        [258., 312., 366.],\n","        [330., 400., 470.]])\n"]}],"source":["# Matrix(-matrix) multiplication\n","print(A)\n","B = torch.arange(12, dtype=torch.float32).reshape(4,3)\n","print(B)\n","C = torch.mm(A, B)\n","print(C)"]},{"cell_type":"markdown","metadata":{"id":"B1ewx1oBbQiH"},"source":["# Norms\n","\n","* Let $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ be vectors and $\\alpha \\in \\mathbb{R}$ be a scalar. A *norm* is a function $f$ that maps a vector to a scalar. A *norm* must satisfy the following properties:\n","    1. $f(\\mathbf{x}) \\geq 0.$\n","\n","    2. $f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$\n","\n","    3. $f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$\n","\n","    4. $\\mathbf{x} = \\mathbf{0} \\Leftrightarrow f(\\mathbf{x})=0,$ where $\\mathbf{0}$ denotes the zero vector."]},{"cell_type":"markdown","metadata":{"id":"sLvYSJkRbQiH"},"source":["* The $L_2$ *norm* $\\| \\cdot \\|_2$ gives the square root of the sum of the squares of the elements of a vector:\n","$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^d x_i^2} = \\sqrt{ x_1^2 + x_2^2 + \\ldots + x_d^2 },$$\n","which is the length of the vector. Because of the importance of this norm, the subscript $2$ is often omitted, so that $\\|\\mathbf{x}\\| = \\|\\mathbf{x}\\|_2$.\n","\n","* The $L_1$ *norm*  $\\| \\cdot \\|_1$ gives the sum of the absolute values of the elements of a vector:\n","\n","$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^d \\left|x_i \\right| = |x_1| + |x_2| + \\ldots + |x_d|.$$\n","\n","* The $L_i$ distance between vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ is defined through the $L_i$ norm as $\\| \\mathbf{x} - \\mathbf{y} \\|_i$.\n"]},{"cell_type":"markdown","metadata":{"id":"AUfy0xr5bQiH"},"source":["# Broadcasting Mechanism\n","\n","* Mathematically, elementwise operations between tensors require them to have the same shape.\n","* Conveniently, under certain conditions, PyTorch can interpret and perform elementwise operations between tensors of different shapes using the *broadcasting mechanism*:\n","    * First, one or both tensors are expanded by copying elements in order to create two tensors with the same shape.\n","    * Second, the elementwise operation is carried on the resulting tensors.\n","\n","* We typically broadcast across a dimension where an array has length 1, such as in the following example.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MI5nPGrnbQiH","outputId":"a314ee11-e730-4ae1-993c-720fb9e6b2c1","executionInfo":{"status":"ok","timestamp":1705395866907,"user_tz":0,"elapsed":293,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensors:\n","tensor([[0],\n","        [1],\n","        [2]])\n","tensor([[0, 1]])\n","Sum with broadcasting:\n","tensor([[0, 1],\n","        [1, 2],\n","        [2, 3]])\n","Expanded tensors:\n","tensor([[0, 0],\n","        [1, 1],\n","        [2, 2]])\n","tensor([[0, 1],\n","        [0, 1],\n","        [0, 1]])\n","Sum with expansion:\n","tensor([[0, 1],\n","        [1, 2],\n","        [2, 3]])\n"]}],"source":["print('Tensors:')\n","a = torch.arange(3).reshape((3, 1))\n","b = torch.arange(2).reshape((1, 2))\n","print(a)\n","print(b)\n","\n","print('Sum with broadcasting:')\n","print(a + b)\n","\n","print('Expanded tensors:')\n","a_expanded = torch.cat((a, a), dim=1)\n","b_expanded = torch.cat((b, b, b), dim=0)\n","print(a_expanded)\n","print(b_expanded)\n","\n","print('Sum with expansion:')\n","print(a_expanded + b_expanded)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67G-yuhxbQiI","outputId":"2815ec3b-5e80-4912-d35d-65a5e4aa5795","executionInfo":{"status":"ok","timestamp":1705395866907,"user_tz":0,"elapsed":26,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([ 6., 22., 38., 54., 70.])\n","tensor([[ 6.],\n","        [22.],\n","        [38.],\n","        [54.],\n","        [70.]])\n","torch.Size([5])\n","torch.Size([5, 1])\n"]}],"source":["# Another example\n","A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","print(A)\n","B1 = A.sum(dim=1);\n","B2 = A.sum(dim=1, keepdims=True);\n","print(B1)\n","print(B2)\n","print(B1.size())\n","print(B2.size())"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BobOi9e3bQiK","outputId":"eb1805a6-808c-448b-8365-db4dd0ea0db0","executionInfo":{"status":"ok","timestamp":1705395866908,"user_tz":0,"elapsed":22,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n","        [0.1818, 0.2273, 0.2727, 0.3182],\n","        [0.2105, 0.2368, 0.2632, 0.2895],\n","        [0.2222, 0.2407, 0.2593, 0.2778],\n","        [0.2286, 0.2429, 0.2571, 0.2714]])"]},"metadata":{},"execution_count":25}],"source":["A/B2 # A/B1 does not work"]},{"cell_type":"markdown","metadata":{"id":"KFP1LNMDbQiK"},"source":["# Tensor Indexing and Slicing\n","\n","* Elements in a tensor can be accessed by indices that start at zero    \n","* The range `i:j` includes all the elements from index $i$ up to index $j$, including the element at $i$ but excluding the element at $j$ (just like the function `range`)\n","* As in Python lists, a negative index can be used to access elements starting from the last element along a dimension (for example, -1 denotes the index of last element along a dimension)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gvD7dGIrbQiL","outputId":"5af707f2-d8d8-4543-f9c6-90a90ecd883e","executionInfo":{"status":"ok","timestamp":1705395866908,"user_tz":0,"elapsed":15,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4])\n","tensor([2, 3, 4])\n","tensor(3)\n","tensor([0, 1, 2])\n","tensor([2, 3, 4])\n","tensor([0, 1, 2, 3, 4])\n"]}],"source":["# Indexing and slicing an array\n","A = torch.arange(5)\n","print(A)\n","print(A[2:5])\n","print(A[-2])\n","print(A[:3]) # A[0:3]\n","print(A[2:]) # A[2:5]\n","print(A[:]) # A[0:5]"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sg0CIK9L9OSI","outputId":"61c0439c-5b32-4b09-941a-7c569fbab0ca","executionInfo":{"status":"ok","timestamp":1705395867163,"user_tz":0,"elapsed":265,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([[ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.]])\n","tensor([[4., 5.],\n","        [8., 9.]])\n","tensor([16., 17., 18., 19.])\n","tensor([[ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.]])\n"]}],"source":["# Indexing and slicing a matrix\n","X = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","print(X)\n","print(X[1:3, :])\n","print(X[1:3, :2])\n","print(X[-1, :])\n","print(X[-3:-1, :])"]},{"cell_type":"markdown","metadata":{"id":"G1ua9S-KbQiL"},"source":["# Saving Memory\n","\n","* Running operations can cause new memory to be allocated to store the results.\n","* We do not want to allocate memory unnecessarily all the time.\n","    * In machine learning, we might have hundreds of megabytes of parameters (or more!)\n","* Where possible, we want to perform operations *in-place*."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5HAGjd9bQiL","outputId":"b65f5082-2345-474b-a074-cdf1eb23ce6d","executionInfo":{"status":"ok","timestamp":1705395867164,"user_tz":0,"elapsed":29,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["136511199190432 136511199184272\n","136511199190432 136511199190992\n","136511199190432 136511199190992\n","136511199190432 136511199190992\n"]}],"source":["# In-place example\n","X = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","Y = 10*X\n","print(id(X), id(Y))\n","Y = Y + X # not in-place\n","print(id(X), id(Y))\n","Y += X # in-place\n","print(id(X), id(Y))\n","Y[:] = Y + X # in-place, overrides the content of the tensor Y with the tensor `Y + X`\n","print(id(X), id(Y))"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6G_nt7V_umA","outputId":"b93c151c-607e-4efc-e127-81a894ab8f2b","executionInfo":{"status":"ok","timestamp":1705395867164,"user_tz":0,"elapsed":19,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1., -1., -1., -1.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n","tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])\n"]}],"source":["# Warning: the assignment operator `=` does not copy data, it just assigns names\n","A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","B = A\n","B[0, :] = -1\n","print(A)\n","\n","# The method `clone` creates a copy of a tensor\n","A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","B = A.clone()\n","B[0, :] = -1\n","print(A)"]},{"cell_type":"markdown","metadata":{"id":"IPZ7SUmybQiM"},"source":["# Converting Tensors to Other Objects"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGMYuFTsbQiM","outputId":"11e338fe-9a50-4098-a45b-88e10428794b","executionInfo":{"status":"ok","timestamp":1705395867165,"user_tz":0,"elapsed":16,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","<class 'torch.Tensor'>\n"]}],"source":["# Converting to a NumPy array, or vice-versa\n","A = X.numpy()\n","print(type(A))\n","B = torch.tensor(A)\n","print(type(B))"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdBl_j_AbQiM","outputId":"bd16ebb7-ccaa-40e4-b1ba-8f717bc3679b","executionInfo":{"status":"ok","timestamp":1705395867410,"user_tz":0,"elapsed":255,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3.5000])\n","3.5\n","3.5\n","3\n"]}],"source":["# Converting a size-1 tensor to a Python scalar\n","a = torch.tensor([3.5])\n","print(a)\n","print(a.item())\n","print(float(a))\n","print(int(a))"]},{"cell_type":"markdown","metadata":{"id":"I0Q22PjezQPO"},"source":["# Recommended reading\n","\n","* [Dive into Deep Learning](https://d2l.ai): Chapters 1, 2.1, 2.2, and 2.3."]},{"cell_type":"markdown","metadata":{"id":"ryVvpIDlKMoc"},"source":["# [Storing this notebook as a `pdf`]"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"M4ArF0iRgBMJ","executionInfo":{"status":"ok","timestamp":1705396040383,"user_tz":0,"elapsed":18393,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n","\n","# Set the path to this notebook below (add \\ before spaces). The output `pdf` will be stored in the corresponding folder.\n","!jupyter nbconvert --to pdf /content/gdrive/My\\ Drive/Colab\\ Notebooks/nndl/week_01/lecture/Tensor_Data_Processing.ipynb\n","\n","# If having issues, save this notebook (File > Save) and restart the session (Runtime > Restart session) before running this cell. To debug, remove the first line (`%%capture`)."]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}