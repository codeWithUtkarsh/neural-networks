{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PsoK5jBsbNpF"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision"]},{"cell_type":"markdown","metadata":{"id":"S2izhX3CbNpF"},"source":["# Implementing a Multilayer Perceptron\n","\n","* In this lab session, you will use high-level PyTorch functionalities to implement multilayer perceptrons for classification.\n","\n","* You will also experiment with different hyperparameters and observe the results.\n","\n","* You will use the [fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which is composed of labeled images of clothes and accessories.\n","\n","* The following function is used to load the dataset. You don't need to understand it for now."]},{"cell_type":"code","source":["# You don't need to understand this function for now.\n","def load_data_fashion_mnist(batch_size, resize=None):\n","    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n","    trans = [torchvision.transforms.ToTensor()]\n","    if resize:\n","        trans.insert(0, torchvision.transforms.Resize(resize))\n","    trans = torchvision.transforms.Compose(trans)\n","    mnist_train = torchvision.datasets.FashionMNIST(\n","        root=\"../data\", train=True, transform=trans, download=True)\n","    mnist_test = torchvision.datasets.FashionMNIST(\n","        root=\"../data\", train=False, transform=trans, download=True)\n","    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n","                            num_workers=2),\n","            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n","                            num_workers=2))"],"metadata":{"id":"o4DPyHvnJpLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU244TLybNpG"},"outputs":[],"source":["batch_size = 256 # Defines the batch size\n","train_iter, test_iter = load_data_fashion_mnist(batch_size) # Loads the fashion MNIST dataset. `train_iter` and `test_iter` are `DataLoader` objects."]},{"cell_type":"code","source":["X, y = next(iter(train_iter)) # Requests the first training batch\n","print(X.size()) # 256 images per batch. Each image is represented by a 1 x 28 x 28 tensor (number of channels x height x width). The images are grayscale, so there is a single channel.\n","print(y.size()) # 256 targets. Each target is a number between 0 and 9. The classification problem has 10 clases."],"metadata":{"id":"SFZWXO3bKhUO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* The following code displays some images from the first training batch."],"metadata":{"id":"I-K0Cb1HKkkF"}},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","\n","class_labels = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'boot'] # Pre-defined class labels\n","\n","for i in range(3):\n","    print(f'\\nImage {i} ({class_labels[int(y[i])]}):\\n') # Prints the index `i` and the label associated to the `i`-th image.\n","    cv2_imshow(X[i].numpy().transpose(1, 2, 0) * 255) # Converts and displays the `i`-th image in the batch."],"metadata":{"id":"bxMUQ0lcKljq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Softmax Regression Model\n","\n","* We have already implemented softmax regression for this classification task. We will use that implementation as a starting point for our multilayer perceptron implementation.\n","\n","* The following steps are involved in our previous implementation of a softmax regression model, which is presented below.\n","\n","* Each image is a rank 3 tensor with $1 \\cdot 28 \\cdot 28 = 784$ elements. We *flatten* each image into a $784$-dimensional vector (observation).\n","\n","* Because the fashion MNIST dataset has $10$ classes, the softmax regression model outputs a $10$-dimensional vector.\n","\n","* Therefore, we need a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{784 \\times 10}$ and a bias vector $\\mathbf{b} \\in \\mathbb{R}^{10 \\times 1}$.\n","\n","* We initialize the weight matrix using samples from a normal distribution and the bias vector to zero.\n","\n","* We compute the logit matrix $\\mathbf{O}$ inside a subclass of `torch.nn.Module`. This is how neural networks are typically implemented in PyTorch.\n","\n","* The class `torch.nn.Module` requires implementing the method `forward`, which should define the forward pass for a batch of observations."],"metadata":{"id":"Bc2Cwc0BLWkt"}},{"cell_type":"code","source":["class SoftmaxRegression(torch.nn.Module):\n","    def __init__(self, num_inputs, num_outputs):\n","        super(SoftmaxRegression, self).__init__() # Initializes superclass\n","\n","        self.num_inputs = num_inputs\n","        self.num_outputs = num_outputs\n","\n","        self.Linear1 = torch.nn.Linear(num_inputs, num_outputs) # Creates a linear layer\n","\n","        torch.nn.init.normal_(self.Linear1.weight, std=0.01) # Initializes the weight matrix\n","        torch.nn.init.zeros_(self.Linear1.bias) # Initializes the bias vector\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.num_inputs) # Reshapes the (`batch_size`, 1, 28, 28) batch of images `x` into a (`batch_size`, 784) batch of observations `x`\n","        out = self.Linear1(x) # A linear layer multiplies `x` by a weight matrix and adds a bias vector (to each row, using broadcasting)\n","        return out # Returns a (`batch_size`, 10) logits matrix\n","\n","num_inputs = 784 # Number of features (inputs)\n","num_outputs = 10 # Number of classes (outputs)\n","sr = SoftmaxRegression(num_inputs, num_outputs)\n","print(sr)"],"metadata":{"id":"rbXMtuUONFIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SAmdYIP7bNpG","origin_pos":4},"source":["## Task 1\n","\n","* Use the code presented below as a starting point for implementing a multilayer perceptron. You should use the softmax regression implementation as a reference in order to fill the lines labeled with `TODO`.\n","\n","* The multilayer perceptron should have one hidden layer composed of `num_hidden` units. The first layer should have a $\\text{ReLU}$ activation function. The second layer should not have an activation function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIweuroNbNpH"},"outputs":[],"source":["class Net(torch.nn.Module):\n","    def __init__(self, num_inputs, num_hidden, num_outputs):\n","        super(Net, self).__init__() # Initializes superclass\n","\n","        self.num_inputs = num_inputs\n","        self.num_hidden = num_hidden\n","        self.num_outputs = num_outputs\n","\n","        self.Linear1 = None # TODO: Create the first layer, with `num_inputs` inputs and `num_hidden` outputs\n","        self.relu = torch.nn.ReLU() # Creates the activation function\n","        self.Linear2 = None # TODO: Create the second layer, with `num_hidden` inputs and `num_outputs` outputs\n","\n","        pass #TODO: Initialize the weight matrix for the first layer\n","        pass # TODO: Initialize the bias vector for the first layer\n","\n","        pass #TODO: Initialize the weight matrix for the second layer\n","        pass # TODO: Initialize the bias vector for the second layer\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.num_inputs) # Reshapes the (`batch_size`, 1, 28, 28) batch of images `x` into a (`batch_size`, 784) batch of observations `x`\n","\n","        x = None # TODO: Apply the first linear layer, which multiplies `x` by a weight matrix and add a bias vector (to each row, using broadcasting)\n","        x = None # TODO: Apply the activation function `self.relu` to `x`\n","        x = None # TODO: Apply the second linear layer, which multiplies `x` by a weight matrix and add a bias vector (to each row, using broadcasting)\n","\n","        return x # Returns a (`batch_size`, 10) logits matrix\n","\n","num_inputs = 784  # Number of features (inputs)\n","num_hidden = 256  # Number of hidden units\n","num_outputs = 10  # Number of classes (outputs)\n","net = Net(num_inputs, num_hidden, num_outputs)\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"vJHLwt8vbNpH","origin_pos":8},"source":["## Loss function\n","\n","* The *neural network* defined above computes the logits matrix $\\mathbf{O}$, not the prediction matrix $\\mathbf{\\hat{Y}} = \\text{softmax}(\\mathbf{O})$.\n","\n","* This is because PyTorch provides a class called `CrossEntropyLoss` that implements the desired cross entropy loss but requires a logits matrix $\\mathbf{O}$ instead of the prediction matrix $\\mathbf{\\hat{Y}}$.\n","\n","* The class `CrossEntropyLoss` implements the cross entropy loss in a way that avoids numerical instabilities that would result from a naive implementation."]},{"cell_type":"code","source":["loss = torch.nn.CrossEntropyLoss()"],"metadata":{"id":"9mG5nuTMVDYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimization Algorithm\n","\n","* We will employ minibatch stochastic gradient descent with a learning rate of $0.1$ as the optimization algorithm.\n","\n","* Because we implemented a subclass of `torch.nn.Module`, the model parameters can be accessed through the method `parameters`."],"metadata":{"id":"IoWuV9rMVFbk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNgzCdWObNpI","origin_pos":10,"tab":["pytorch"]},"outputs":[],"source":["lr = 0.1\n","optimizer = torch.optim.SGD(net.parameters(), lr=lr)"]},{"cell_type":"markdown","source":["## Evaluation\n","\n","* Recall that the highest element of a logits vector determines which class will be predicted.\n","\n","* We can use this to compute the number of correct predictions per batch."],"metadata":{"id":"ukjgb9tdVqib"}},{"cell_type":"code","source":["def correct(logits, y):\n","    y_hat = logits.argmax(axis=1) # Finds the column with the highest value for each row of `logits`.\n","    return (y_hat == y).float().sum() # Computes the number of times that `y_hat` and `y` match.\n","\n","# Example: 1 correct classification,\n","y = torch.tensor([2, 1])\n","logits = torch.tensor([[0.1, 0.3, 0.6], [0.5, 0.2, 0.3]])\n","print(correct(logits, y))"],"metadata":{"id":"K5Ss_9Q6VtY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* We can use the previous function to compute the accuracy of our model in a given dataset by accumulating the number of correct predictions across batches and then dividing that number by the number of examples in the dataset."],"metadata":{"id":"GUzPnfBsVwfO"}},{"cell_type":"code","source":["def evaluate_metric(net, data_iter, metric):\n","    \"\"\"Compute the average `metric` of the model on a dataset.\"\"\"\n","    c = torch.tensor(0.)\n","    n = torch.tensor(0.)\n","    for X, y in data_iter:\n","        logits = net(X)\n","        c += metric(logits, y)\n","        n += len(y)\n","\n","    return c / n"],"metadata":{"id":"57Q-RD6UVyVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Training accuracy: {evaluate_metric(net, train_iter, correct)}. Testing accuracy: {evaluate_metric(net, test_iter, correct)}.')"],"metadata":{"id":"CYRjJSyxV1Wm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiBOrs2pbNpI"},"source":["## Training\n","\n","* The following code implements the training loop for the multilayer perceptron.\n","\n","* The training/testing dataset accuracy is displayed after each epoch and stored for plotting.\n","\n","* **Important:** it is a methodological mistake to compute performance metrics on the *testing* dataset for the purposes of hyperparameter tuning. A *validation* dataset should be used for that purpose, even if it requires splitting the original training dataset into a training dataset and a validation dataset. The *test* dataset should only be used to evaluate the performance of the final set of hyperparameters, in order to assess generalization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik7KovzgbNpI","origin_pos":12,"tab":["pytorch"]},"outputs":[],"source":["losses = [] # Stores the loss for each training batch\n","train_accs = [] # Stores the training accuracy after each epoch\n","test_accs = [] # Stores the testing accuracy after each epoch\n","\n","num_epochs = 60\n","for epoch in range(num_epochs):\n","    print(f'\\nEpoch {epoch + 1}/{num_epochs}.')\n","    for X, y in train_iter:\n","        logits = net(X) # Computes the logits for the batch of images `X`\n","\n","        l = loss(logits, y) # Computes the loss given the `logits` and the class vector `y`\n","        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n","        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n","\n","        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n","\n","        losses.append(float(l)) # Stores the loss for this batch\n","\n","    with torch.no_grad(): # Computing performance metrics does not require gradients\n","        train_accs.append(evaluate_metric(net, train_iter, correct))\n","        test_accs.append(evaluate_metric(net, test_iter, correct))\n","        print(f'Training accuracy: {train_accs[-1]}. Testing accuracy: {test_accs[-1]}.') # Computes and displays training/testing dataset accuracy.\n","\n","plt.plot(losses) # Plots the loss for each training batch\n","plt.xlabel('Training batch')\n","plt.ylabel('Cross entropy loss')\n","plt.show()\n","\n","plt.plot(train_accs, label='Training accuracy')\n","plt.plot(test_accs, label='Testing accuracy')\n","plt.legend(loc='best')\n","plt.xlabel('Epoch')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Qaak5FgZbNpJ"},"source":["## Task 2\n","\n","* You will now experiment with different hyperparameters and observe the results.\n","\n","* Test a few alternatives for the following hyperparameters and observe the impact on model performance:\n","    * Number of epochs: 20, 60.\n","    * Number of hidden units: 128, 256, 512.\n","    * Learning rate: 0.5, 0.9, 0.01.\n","    * Activation function: $\\text{ReLU}$, $\\text{sigmoid}$.\n","    * Standard deviation for weight initialization (`std`): 0.0, 0.1, 1.0.\n","\n","* Note that the performance of the model depends on **combinations** of hyperparameters. Therefore, whether a hyperparameter achieves high performance depends on the remaining hyperparameters.\n","\n","* Finding good hyperparameters generally requires trying all possible combinations of hyperparameters (where each hyperparameter is only allowed to be one of a few alternatives). Using the alternatives listed above, this would require testing $2 \\cdot 3 \\cdot 3 \\cdot 2 \\cdot 3 = 108$ combinations."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}