{"cells":[{"cell_type":"markdown","metadata":{"id":"Od_NYpeKGnAD"},"source":["# Classification task\n","\n","* In a regression task, a model must be able to predict the target (output) associated to a new observation (input). Targets may belong to an infinite set of possible values, and not even have known minimum or maximum values.\n","\n","* A regression model is appropriate to predict a quantity associated to an observation (for example, the price of a house, the position of a particle, or the rating of a movie).\n","\n","* In a classification task, a model must also be able to predict the target (output) associated to a new observation (input). However, the target belongs to a known finite set of possible values that may not be ordered.\n","\n","* A classification model is appropriate to predict a class associated to an observation (for example, whether an image depicts a dog or a cat, whether an e-mail is spam, or whether a movie will be liked).\n","\n","* As we will see, the fact that the targets may not be ordered requires a classification task to be treated differently from a regression task."]},{"cell_type":"markdown","metadata":{"id":"X4QG148gGnAL"},"source":["## Example\n","\n","* Suppose that each observation corresponds to an $1024 \\times 1024$ color image encoded into a vector with $d = 1024 \\cdot 1024 \\cdot 3$ elements (each pixel has three components: red, green, and blue).\n","\n","* Suppose that our task is to identify the animal depicted in the image, which is certainly a dog, bird, or cat.\n","\n","* We could choose a number to represent each class (for example: $1$ for dog, $2$ for bird, and $3$ for cat) and treat this task as a regression task.\n","\n","* However, our choice of numbers implicitly introduces an order (in our example, we have implicitly biased the model to behave as if dogs were more similar to birds than to cats). In order to see this, recall that a regression loss penalizes a prediction based on its squared distance to the target."]},{"cell_type":"markdown","metadata":{"id":"Twk76jOrGnAM"},"source":["# One-hot encoding\n","\n","* One-hot encoding allows dealing with targets (and features) that do not admit an order.\n","\n","* In an one-hot encoding, each possible value is assigned to a distinct vector where all elements are zero except for one, which is one.\n","\n","* In the previous example, we could assign $[1, 0, 0]^T$ for dog, $[0, 1, 0]^T$ for bird, and $[0, 0, 1]^T$ for cat. Note that each of these vectors has as many elements as there are classes in the problem.\n","\n","* Note that the (Euclidean) distance between any pair of these vectors is the same."]},{"cell_type":"markdown","metadata":{"id":"2z1FLqf6GnAM"},"source":["# Softmax regression\n","\n","* Softmax regression is one of the simplest approaches for classification, just as linear regression is one of the simplest approaches for regression.\n","\n","* The name softmax regression is somewhat unfortunate, since the softmax regression model is a classification model.\n","\n","* A softmax regression model receives an observation (input) and predicts a target vector (output).\n","\n","* A softmax regression model is trained based on a dataset composed of pairs of inputs and outputs, where each output is a vector that results from one-hot encoding a class.\n","\n","* In our example, a softmax regression model would receive a vector $\\mathbf{x}$ with $d = 1024 \\cdot 1024 \\cdot 3$ elements and predict a vector $\\hat{\\mathbf{y}}$ with $3$ elements.\n","\n","* The predicted class for a new observation $\\mathbf{x}$ would be based on the highest value in the corresponding prediction vector $\\hat{\\mathbf{y}}$\n","\n","* In our example, if $\\hat{\\mathbf{y}} = [0.1, 0.7, 0.2]^T$ is the prediction for observation $\\mathbf{x}$, we would say that the model predicts the class \"bird\"."]},{"cell_type":"markdown","source":["## Model\n","\n","* The softmax regression prediction can be computed in two steps, which define the *forward pass*.\n","\n","### First step\n","\n","* Let $q$ denote the number of classes in the regresion task (in our example, $q = 3$). Suppose that the classes are numbered from $1$ to $q$. How the classes are ordered will not affect the performance of this model.\n","\n","* The first step computes the so-called logits $\\mathbf{o} = [o_1, o_2, \\ldots, o_q]^T$ for an observation $\\mathbf{x} = [x_1, x_2, \\ldots, x_d]^T$ as\n","\n","$$ \\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}, $$\n","\n","where $\\mathbf{W} \\in \\mathbb{R}^{q \\times d}$ is a weight matrix and $\\mathbf{b} = [b_1, b_2, \\ldots, b_q]^T$ is a bias vector.\n","\n","* The $i$-th element $o_i$ of the logits $\\mathbf{o}$ can be interpreted as a *score* for the $i$-th class, in the sense that increasing $o_i$ relative to the other elements of $\\mathbf{o}$ makes predicting class $i$ more likely.\n","\n","* The $i$-th row of $\\mathbf{W}$ and the $i$-th element of $\\mathbf{b}$ can be interpreted as parameters associated to the $i$-th class.\n","\n","### Second step\n","\n","* The elements of the logits $\\mathbf{o}$ are real numbers. However, we know that the elements of the target vector $\\mathbf{y}$ are always between zero and one.\n","\n","* The elements of the target vector $\\mathbf{y}$ also sum to one, so that they can be interpreted as probabilities.\n","\n","* For example, if $\\mathbf{y} = [0, 1, 0]^T$ is the target vector for observation $\\mathbf{x}$, we may say that the probability of $\\mathbf{x}$ depicting a bird is one.\n","\n","* In order to enable a similar interpretation, the prediction vector $\\hat{\\mathbf{y}}$ for a new observation $\\mathbf{x}$ will be constrained to have elements between zero and one that sum to one.\n","\n","* One of the simplest (differentiable) functions that can transform the logits $\\mathbf{o}$ into a prediction vector $\\hat{\\mathbf{y}}$ with these properties is the $\\text{softmax}$ function.\n","\n","* The prediction vector $\\hat{\\mathbf{y}}$ for a new observation $\\mathbf{x}$ can be computed based on the logits $\\mathbf{o}$ as\n","\n","$$ \\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{o}), $$\n","\n","where, for every $j \\in \\{1, 2, \\ldots, q\\}$,\n","\n","$$ \\hat{y}_j = \\text{softmax}(\\mathbf{o})_j = \\frac{e^{o_j}}{ e^{o_1} + e^{o_2} + \\cdots + e^{o_q}}. $$\n","\n","* Intuitively, $e^{o_j}$ is positive and increases with $o_j$. By dividing $e^{o_j}$ by $e^{o_1} + e^{o_2} + \\cdots + e^{o_q}$, the sum $\\sum_{j} \\hat{y}_j$ is guaranteed to be $1$.\n","\n","* The predicted class for observation $\\mathbf{x}$ is a class $j$ such that $\\hat{y}_j = \\max_k \\hat{y}_k$.\n"],"metadata":{"id":"wpN0G2Zlsgs-"}},{"cell_type":"markdown","metadata":{"id":"WcmCjpXBGnAP"},"source":["# Vectorized softmax regression\n","\n","* Vectorization is the process of writing an algorithm using efficient operations on vectors, matrices, or higher-order tensors.\n","\n","* Let $(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, \\mathbf{y}^{(n)})$ denote a dataset with $n$ examples, where each observation $\\mathbf{x}^{(i)}$ corresponds to a one-hot code $\\mathbf{y}^{(i)}$. Let $q$ denote the number of classes in the problem, so that $\\mathbf{y}^{(i)}$ has $q$ elements for every $i$.\n","\n","* Consider an observation matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ that contains one row for each observation and one column for each feature, so that\n","$$ \\mathbf{X}=\n","\\begin{bmatrix}\n","{\\mathbf{x}^{(1)}}^T \\\\\n","{\\mathbf{x}^{(2)}}^T \\\\\n","\\vdots \\\\\n","{\\mathbf{x}^{(n)}}^T \\\\\n","\\end{bmatrix} =\n","\\begin{bmatrix}\n","x_1^{(1)} & x_2^{(1)} & \\ldots & x_d^{(1)} \\\\\n","x_1^{(2)} & x_2^{(2)} & \\ldots & x_d^{(2)} \\\\\n","\\vdots & \\vdots & \\ddots \\\\\n","x_1^{(n)} & x_2^{(n)} & \\ldots & x_d^{(n)}\n","\\end{bmatrix}.$$\n","\n","* Consider a target matrix $\\mathbf{Y} \\in [0, 1]^{n \\times q}$ that contains one row for each one-hot code, so that\n","$$ \\mathbf{Y}=\n","\\begin{bmatrix}\n","{\\mathbf{y}^{(1)}}^T \\\\\n","{\\mathbf{y}^{(2)}}^T \\\\\n","\\vdots \\\\\n","{\\mathbf{y}^{(n)}}^T \\\\\n","\\end{bmatrix} =\n","\\begin{bmatrix}\n","y_1^{(1)} & y_2^{(1)} & \\ldots & y_q^{(1)} \\\\\n","y_1^{(2)} & y_2^{(2)} & \\ldots & y_q^{(2)} \\\\\n","\\vdots & \\vdots & \\ddots \\\\\n","y_1^{(n)} & y_2^{(n)} & \\ldots & y_q^{(n)}\n","\\end{bmatrix}.$$\n","\n","* In that case, the logit matrix $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$ may be computed as\n","$$ \\mathbf{O} = \\mathbf{X} \\mathbf{W} + \\mathbf{B}, $$\n","where $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$ is a weight matrix and $\\mathbf{B} \\in \\mathbb{R}^{n \\times q}$ is a bias matrix given by replicating a row vector $\\mathbf{b}^T = [b_1, b_2, \\ldots, b_q]$ across $n$ rows.\n","\n","* Note that we have redefined the weight matrix $\\mathbf{W}$ to facilitate vectorization. In this version, the $i$-the column of $\\mathbf{W}$ contains parameters associated to the $i$-th class.\n","\n","* In practice, the bias vector $\\mathbf{b}$ does not need to be replicated to generate $\\mathbf{B}$ if broadcasting is used.\n","\n","* Finally, the prediction matrix $\\mathbf{\\hat{Y}}$ that contains one row for each prediction vector is given by\n","\n","$$ \\mathbf{\\hat{Y}} = \\text{softmax}(\\mathbf{O}), $$\n","\n","where the $\\text{softmax}$ function is applied individually to each **row** of the logit matrix $\\mathbf{O}$. This involves computing $e^{\\mathbf{O}}$ (elementwise exponentiation) and then dividing each of its elements by a corresponding sum across rows.\n","\n","* The $i$-th row of the prediction matrix $\\mathbf{\\hat{Y}}$ contains the (transposed) prediction vector $\\mathbf{\\hat{y}}^{(i)}$ for the $i$-th observation $\\mathbf{x}^{(i)}$, which can be compared against the target vector $\\mathbf{y}^{(i)}$ for the $i$-th observation.\n","\n","* Therefore, we seek parameters (weight matrix $\\mathbf{W}$ and bias vector $\\mathbf{b}$) that make $\\mathbf{Y}$ and $\\mathbf{\\hat{Y}}$ as similar as possible."]},{"cell_type":"markdown","metadata":{"id":"Lo_9fbqlGnAP"},"source":["# Negative log-likelihood loss function\n","\n","* In order to train a model for a given dataset, we once again need to be able to measure how well a given model performs on this dataset.\n","\n","* Because the outputs of the softmax regression model can be intepreted as (conditional) probabilities of classes given an observation, it is natural to seek parameters that would make the dataset as likely as possible. This is called maximum likelihood estimation.\n","\n","* Using the mean squared error for *linear regression* also has a probabilistic interpretation: if we assume that each target is sampled from a normal distribution whose mean is given by a linear model applied to the corresponding observation, the parameters that minimize the mean squared error make the dataset as likely as possible.\n","\n","\n","* Let $\\mathbf{\\hat{y}}$ denote the prediction vector computed by a softmax regression model for an observation $\\mathbf{x}$ with target vector $\\mathbf{y}$. The probability assigned to the correct class by the model is given by\n","$$ \\hat{y}_{1}^{y_{1}} \\hat{y}_{2}^{y_{2}} \\cdots \\hat{y}_{q}^{y_{q}} = \\prod_{k=1}^q  \\hat{y}_{k}^{y_{k}}, $$\n","since $y_{k} = 1$ if and only if the correct class is $k$, and $y_k = 0$ otherwise.\n","\n","* Maximum likelihood estimation suggests maximizing the probability that each observation $\\mathbf{x}$ in the dataset is assigned to the correct class by the model.\n","\n","* Maximizing the previous expression is equivalent to maximizing the so-called log-likelihood given by\n","$$ \\log \\left[ \\prod_{k=1}^q  \\hat{y}_{k}^{y_{k}} \\right] = \\sum_{k = 1}^q y_k \\log \\hat{y}_k, $$\n","since $\\hat{y}_k > 0$ for every $k$ and $\\log$ (implicitly base $e$) is an increasing function.\n","\n","* For the $i$-th example in the dataset, maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood function $l^{(i)}$ given by\n","\n","$$ l^{(i)}(\\mathbf{W}, \\mathbf{b}) = -\\sum_{k = 1}^q y_k^{(i)} \\log \\hat{y}_k^{(i)}.$$\n","\n","* Note that the model (weight matrix $\\mathbf{W}$ and bias vector $\\mathbf{b}$) affects the negative log-likelihood for the $i$-th example $l^{(i)}(\\mathbf{W}, \\mathbf{b})$ through the prediction vector $\\mathbf{\\hat{y}^{(i)}} = [\\hat{y}^{(i)}_1, \\hat{y}^{(i)}_2, \\ldots, \\hat{y}^{(i)}_q]^T$.\n","\n","* Note that if the $i$-th observation $\\mathbf{x}^{(i)}$ belongs to class $k$, then $l^{(i)}(\\mathbf{W}, \\mathbf{b}) = - y_k \\log \\hat{y}_k = -\\log \\hat{y}_k$, since all other terms in the summation above will be zero. Therefore, the loss $l^{(i)}(\\mathbf{W}, \\mathbf{b})$ decreases as $\\hat{y}_k$ increases."]},{"cell_type":"markdown","metadata":{"id":"OtqDP1a_GnAS"},"source":["# Cross-Entropy Loss\n","\n","* Under additional assumptions, the cross-entropy loss is an appropriate way to combine the negative log-likelihood loss for each example into a single measure of how well a model performs in the training dataset.\n","\n","* The cross-entropy loss $C$ for a softmax regression model defined by a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$ is given by\n","\n","$$ C = \\sum_{i = 1}^n l^{(i)}(\\mathbf{W}, \\mathbf{b}) = - \\sum_{i = 1}^n \\sum_{k = 1}^q y_k^{(i)} \\log \\hat{y}_k^{(i)}. $$\n","\n","* In order to employ gradient descent, we must compute the gradient $\\nabla C$ of the loss $C$ with respect to the model parameters $\\mathbf{W}$ and $\\mathbf{b}$.\n","\n","* We will leave that task to our automatic differentiation tools, although it is not that difficult to find an analytic expression for this gradient.\n","\n","* In order to imagine the gradient $\\nabla C$ as a vector, you can imagine that the weight matrix $\\mathbf{W}$ is *flattened* into a vector and concatenated with the bias vector $\\mathbf{b}$ into a long parameter vector with $d \\cdot q  + q$ elements."]},{"cell_type":"markdown","metadata":{"id":"yA5kAdU4GnAT"},"source":["# Prediction and Evaluation\n","\n","* After the model is trained, we can predict the probability of each class given a new observation.\n","\n","* We can assign the class with the highest predicted probability to a new observation.\n","\n","* A prediction is correct if it matches the actual class.\n","\n","* The *accuracy* of a model is defined as the fraction of examples in a given dataset for which the model makes correct predictions (100\\% accuracy means always correct, 0% accuracy means never correct).\n","\n","* The accuracy of the model on a *validation dataset* (data not used for training) can be used to assess generalization.\n"]},{"cell_type":"markdown","source":["# Recommended reading\n","\n","* [Dive into Deep Learning](https://d2l.ai): Chapters 3.4, 3.5, 3.6, and 3.7."],"metadata":{"id":"VnYigF4fd-0R"}},{"cell_type":"markdown","source":["# [Storing this notebook as a `pdf`]"],"metadata":{"id":"DhzIO-4keH4Q"}},{"cell_type":"code","source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n","\n","# Set the path to this notebook below (add \\ before spaces). The output `pdf` will be stored in the corresponding folder.\n","!jupyter nbconvert --to pdf /content/gdrive/My\\ Drive/Colab\\ Notebooks/nndl/week_04/lecture/01_Softmax_Regression.ipynb\n","\n","# If having issues, save this notebook (File > Save) and restart the session (Runtime > Restart session) before running this cell. To debug, remove the first line (`%%capture`)."],"metadata":{"id":"TtztBH_7eJAm","executionInfo":{"status":"ok","timestamp":1703672846215,"user_tz":0,"elapsed":92018,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":1,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}