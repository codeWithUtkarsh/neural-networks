{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Dz7XBNODEa-H","executionInfo":{"status":"ok","timestamp":1703348948594,"user_tz":0,"elapsed":16,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"source":["import torch"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"b129ZxUYEa-O"},"source":["# Concise Implementation of Linear Regression\n","\n","* This notebook shows how linear regression can be implemented using higher-level functionalities provided by PyTorch."]},{"cell_type":"markdown","metadata":{"id":"ypLcYVK4Ea-O"},"source":["# Creating the Dataset"]},{"cell_type":"code","metadata":{"origin_pos":4,"tab":["pytorch"],"id":"yIqTr3LqEa-P","executionInfo":{"status":"ok","timestamp":1703348948595,"user_tz":0,"elapsed":13,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"source":["# Generating the Dataset\n","def synthetic_data(w, b, num_examples):\n","    \"\"\"Generate y = Xw + b + noise.\"\"\"\n","    X = torch.normal(0, 1, (num_examples, len(w)))\n","    y = torch.mm(X, w) + b\n","    y += torch.normal(0, 0.01, y.size())\n","    return X, y\n","\n","true_w = torch.tensor([[2], [-3.4]])\n","true_b = torch.tensor(4.2)\n","features, labels = synthetic_data(true_w, true_b, 1000)"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":5,"id":"rekqORZsEa-P"},"source":["# Generating batches\n","\n","* We will generate batches using high-level functionalities provided by PyTorch classes.\n","\n","* The abstract class `Dataset` requires implementing a function called `__len__` (called by Python's `len` function) and a function called `__getitem__`, which enables indexing, iterating, and slicing.\n","\n","* The class `TensorDataset` implements the `Dataset` interface. The constructor of this class accepts an arbitrary number of tensors whose first dimension have the same length. When indexed, a `TensorDataset` returns a tuple\n","with the corresponding elements from each of these tensors.\n","\n","* The class `DataLoader` enables iterating through minibatches of a `Dataset`."]},{"cell_type":"code","metadata":{"origin_pos":7,"scrolled":true,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"FFfiz-8xEa-Q","executionInfo":{"status":"ok","timestamp":1703348948595,"user_tz":0,"elapsed":12,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"34c699c5-af93-44c9-82c7-58a4937ab70b"},"source":["dataset = torch.utils.data.TensorDataset(features, labels) # Creates a `TensorDataset`\n","print(dataset[0]) # The first example in our dataset\n","\n","print()\n","\n","batch_size = 10\n","data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True) # Creates a `DataLoader`\n","next(iter(data_iter)) # Creates an iterator from the `DataLoader` and requests the first element"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([-0.9580,  0.5034]), tensor([0.5655]))\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[tensor([[-1.6006e+00, -1.4821e+00],\n","         [ 9.5151e-01,  9.2238e-01],\n","         [-3.8018e-02, -1.6698e-03],\n","         [-4.6217e-01,  1.3457e+00],\n","         [-1.4593e-01, -1.1210e-01],\n","         [ 1.7226e+00,  6.5449e-01],\n","         [ 1.0066e+00,  3.5062e-02],\n","         [ 1.3718e+00,  2.1815e-01],\n","         [ 4.0931e-02, -4.0356e-01],\n","         [-6.6187e-01, -9.7512e-01]]),\n"," tensor([[ 6.0499],\n","         [ 2.9787],\n","         [ 4.1381],\n","         [-1.3030],\n","         [ 4.2936],\n","         [ 5.4188],\n","         [ 6.0936],\n","         [ 6.2045],\n","         [ 5.6280],\n","         [ 6.1918]])]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"XEiSjVwVEa-S"},"source":["# Defining the Model\n","\n","* Recall that a linear model can be interpreted as a very simple neural network.\n","\n","* We will use the neural network functionalities provided by PyTorch to define our model, effectively creating and training a neural network!\n","\n","* In neural network terms, we need to use a fully-connected linear layer, which is implemented by the `Linear` class in PyTorch.\n"]},{"cell_type":"code","metadata":{"origin_pos":17,"tab":["pytorch"],"id":"5cSU9gBAEa-S","executionInfo":{"status":"ok","timestamp":1703348948595,"user_tz":0,"elapsed":10,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"source":["num_of_inp = 2 # Number of inputs to the layer\n","num_of_out = 1 # Number of outputs from the layer\n","net = torch.nn.Linear(num_of_inp, num_of_out) # Creates our model (a neural network with a fully-connected linear layer and a single output)"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":19,"id":"THI1NMumEa-S"},"source":["# Initializing Model Parameters\n","\n","* We will initialize the parameters of the neural network as in the previous notebook.\n","* We can modify the parameters by accessing the weights (`net.weight.data`) and the bias (`bias.data`).\n","* Accessing the `data` of the tensors eliminates the need for `torch.no_grad`.\n","* The in-place methods `normal_` and `fill_` can be used to overwrite parameter values."]},{"cell_type":"code","metadata":{"origin_pos":24,"tab":["pytorch"],"id":"n4kdhcFbEa-T","executionInfo":{"status":"ok","timestamp":1703348948596,"user_tz":0,"elapsed":11,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"source":["net.weight.data.normal_(0, 0.01); # Each weight is sampled from a normal distribution with mean 0 and standard deviation 0.01.\n","net.bias.data.fill_(0); # The bias is initialized to 0."],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":29,"id":"uVx3xZgvEa-U"},"source":["# Defining the Loss Function\n","* The `MSELoss` class computes the mean squared error."]},{"cell_type":"code","metadata":{"origin_pos":34,"tab":["pytorch"],"id":"Hb6RDaH-Ea-U","executionInfo":{"status":"ok","timestamp":1703348948596,"user_tz":0,"elapsed":10,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"source":["loss = torch.nn.MSELoss()"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"oY5C13wpEa-V"},"source":["# Defining the Optimization Algorithm\n","\n","* Stochastic gradient descent is implemented by the `SGD` class.\n","\n","* The constructor of this class requires a list of parameters to be optimized (which can be obtained through `net.parameters()`) and accepts some hyperparameters (such as the learning rate `lr`)."]},{"cell_type":"code","metadata":{"origin_pos":41,"tab":["pytorch"],"id":"zrgVW6MhEa-V","executionInfo":{"status":"ok","timestamp":1703348948596,"user_tz":0,"elapsed":10,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"source":["optimizer = torch.optim.SGD(net.parameters(), lr=0.5)"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":43,"id":"VWrjKTnSEa-V"},"source":["# Training Loop\n","* During each **epoch**:\n","    * Execute one iteration per minibatch.\n","    * During each iteration:\n","        * Obtain the minibatch.\n","        * Compute predictions and loss using the current model (**forward pass**).\n","        * Compute the gradients of the loss with respect to model parameters (**backward pass**).\n","        * Update the model parameters."]},{"cell_type":"code","metadata":{"origin_pos":45,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"wDd9wQBBEa-W","executionInfo":{"status":"ok","timestamp":1703348948895,"user_tz":0,"elapsed":308,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}},"outputId":"00ef65b4-5551-4500-8e88-341ea99befc1"},"source":["print('\\nInitial parameters:')\n","print(net.weight)\n","print(net.bias)\n","\n","print()\n","\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in data_iter: # Minibatch: `X` and `y`\n","        y_hat = net(X) # Prediction for the minibatch\n","        l = loss(y_hat, y) # Loss for the minibatch\n","        optimizer.zero_grad() # Zeroes the gradient stored inside each parameter\n","        l.backward() # Computes gradient of `l` with respect to parameters\n","        optimizer.step() # Updates each parameter based on the gradient stored inside it.\n","\n","    # After each epoch, computes the loss for the entire training dataset\n","    l = loss(net(features), labels)\n","    print(f'Epoch {epoch + 1}. Loss: {l:f}.')\n","\n","print('\\nLearned parameters:')\n","print(net.weight)\n","print(net.bias)\n","\n","print('\\nTrue parameters:')\n","print(true_w)\n","print(true_b)"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Initial parameters:\n","Parameter containing:\n","tensor([[-0.0146, -0.0055]], requires_grad=True)\n","Parameter containing:\n","tensor([0.], requires_grad=True)\n","\n","Epoch 1. Loss: 0.000171.\n","Epoch 2. Loss: 0.000111.\n","Epoch 3. Loss: 0.000266.\n","\n","Learned parameters:\n","Parameter containing:\n","tensor([[ 2.0045, -3.4012]], requires_grad=True)\n","Parameter containing:\n","tensor([4.2120], requires_grad=True)\n","\n","True parameters:\n","tensor([[ 2.0000],\n","        [-3.4000]])\n","tensor(4.2000)\n"]}]},{"cell_type":"markdown","metadata":{"origin_pos":47,"id":"WQlzvMCREa-W"},"source":["# Evaluation\n","\n","* Because we created the dataset, we can evaluate our success by comparing the true parameters with the learned parameters.\n"]},{"cell_type":"markdown","source":["# [Storing this notebook as a `pdf`]"],"metadata":{"id":"zBtyERw0d-JW"}},{"cell_type":"code","source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n","\n","# Set the path to this notebook below (add \\ before spaces). The output `pdf` will be stored in the corresponding folder.\n","!jupyter nbconvert --to pdf /content/gdrive/My\\ Drive/Colab\\ Notebooks/nndl/week_03/lecture/03_Linear_Regression_Concise.ipynb\n","\n","# If having issues, save this notebook (File > Save) and restart the session (Runtime > Restart session) before running this cell. To debug, remove the first line (`%%capture`)."],"metadata":{"id":"rYckB5gweAuA","executionInfo":{"status":"ok","timestamp":1703348948594,"user_tz":0,"elapsed":17684,"user":{"displayName":"Paulo Rauber","userId":"08697591328641962840"}}},"execution_count":48,"outputs":[]}]}